from typing import Dict, Any, Set, Tuple, List
import pandas as pd
import numpy as np
import io

from sklearn.ensemble import RandomForestClassifier
from sqlalchemy.engine import Inspector
from openai import OpenAI
import logging
import json
import matplotlib.pyplot as plt
import seaborn as sns
import config
from config import API_KEY
import urllib.parse
from scipy.stats import ttest_ind, chi2_contingency
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans

client = OpenAI(api_key=API_KEY)


def analyze_single_table(table_name: str, df: pd.DataFrame) -> Dict[str, Any]:
    numeric_df = df.select_dtypes(include=np.number)
    corr = numeric_df.corr().replace({np.nan: None}).to_dict() if not numeric_df.empty else {}
    stats = df.describe(include='all').replace({np.nan: None}).to_json()

    prompt = (
        f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'. "
        f"–í–æ—Ç –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}. "
        f"–í–æ—Ç –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π: {json.dumps(corr)}. "
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã—è–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã, —Å–∫—Ä—ã—Ç—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã. "
        "–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –ø–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è Markdown –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è (`**—Ç–µ—Ä–º–∏–Ω**`)."
    )

    response = client.chat.completions.create(
        model="gpt-4.1-nano",
        messages=[{"role": "user", "content": prompt}],
        temperature=0.4
    )
    insight = response.choices[0].message.content
    return {"insight": insight, "correlations": corr}


def analyze_joins(inspector: Inspector, dataframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
    joint_insights = {}
    analyzed_pairs: Set[Tuple[str, str]] = set()
    all_tables = list(dataframes.keys())

    for table_name in all_tables:
        try:
            foreign_keys = inspector.get_foreign_keys(table_name)
        except Exception as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
            continue

        for fk in foreign_keys:
            left_table = table_name
            right_table = fk['referred_table']

            pair = tuple(sorted((left_table, right_table)))
            if pair in analyzed_pairs:
                continue

            analyzed_pairs.add(pair)

            df_left = dataframes[left_table]
            df_right = dataframes.get(right_table)

            if df_right is None:
                continue

            left_on = fk['constrained_columns']
            right_on = fk['referred_columns']

            try:
                merged_df = pd.merge(
                    df_left, df_right,
                    left_on=left_on,
                    right_on=right_on,
                    suffixes=(f'_{left_table}', f'_{right_table}')
                )

                if merged_df.empty:
                    continue

                join_key = f"{left_table} üîó {right_table}"
                analysis_result = analyze_single_table(join_key, merged_df)
                stats = merged_df.describe(include='all').replace({np.nan: None}).to_json()
                corr = analysis_result["correlations"]

                prompt = (
                    f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –°–í–Ø–ó–¨ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ '{left_table}' –∏ '{right_table}', –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã "
                    f"–ø–æ –∫–ª—é—á–∞–º "
                    f"({left_table}.{','.join(left_on)} = {right_table}.{','.join(right_on)}). "
                    f"–í–æ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {stats}. "
                    f"–í–æ—Ç –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {json.dumps(corr)}. "
                    "–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø–æ–∏—Å–∫–µ –∏–Ω—Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∏–º–µ–Ω–Ω–æ –∏–∑-–∑–∞ —Å–≤—è–∑–∏ –¥–≤—É—Ö —Ç–∞–±–ª–∏—Ü. "
                    "–û—Ç–≤–µ—Ç –¥–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É, –∏—Å–ø–æ–ª—å–∑—É—è Markdown –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è (`**—Ç–µ—Ä–º–∏–Ω**`)."
                )

                response = client.chat.completions.create(
                    model="gpt-4.1-nano",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0.5
                )

                analysis_result["insight"] = response.choices[0].message.content
                joint_insights[join_key] = analysis_result

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–∞–±–ª–∏—Ü {left_table} –∏ {right_table}: {e}")

    return joint_insights


def generate_visualizations(
        dataframes: dict[str, pd.DataFrame], report_id: int
) -> dict[str, list[str]]:
    visualizations = {}
    sns.set_theme(style="whitegrid")

    for name, df in dataframes.items():
        if df.empty:
            logging.warning(f"DataFrame –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}' –ø—É—Å—Ç, –≥—Ä–∞—Ñ–∏–∫–∏ –Ω–µ –±—É–¥—É—Ç —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã.")
            continue

        # Prepare column info for GPT more explicitly, including types for better suggestions
        column_info = [
            {"name": col, "dtype": str(df[col].dtype), "nunique": df[col].nunique(),
             "is_numeric": pd.api.types.is_numeric_dtype(df[col])}
            for col in df.columns
        ]

        # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—Ç–æ—á–Ω–µ–Ω–∏–µ–º –¥–ª—è bar-–≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏ —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º —Ç–∏–ø–æ–≤ —Å—Ç–æ–ª–±—Ü–æ–≤
        prompt = (
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π DataFrame —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º '{name}' —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏ –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: "
            f"{json.dumps(column_info)}. "
            "–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ 2 –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. "
            "–û—Ç–≤–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –≤ –≤–∏–¥–µ JSON-–æ–±—ä–µ–∫—Ç–∞ —Å –∫–ª—é—á–æ–º 'charts', –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. "
            "–ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –≤ –º–∞—Å—Å–∏–≤–µ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: 'chart_type' ('hist', 'bar', 'scatter', 'pie'), "
            "'columns' (–°–ü–ò–°–û–ö –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –°–¢–û–õ–ë–¶–û–í, –í–´–ë–†–ê–ù–ù–´–• –ò–ó –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –°–ü–ò–°–ö–ê). –î–ª—è 'bar' –≥—Ä–∞—Ñ–∏–∫–∞: –µ—Å–ª–∏ "
            "—ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—á–∞—Å—Ç–æ—Ç–∞), –∏—Å–ø–æ–ª—å–∑—É–π 1 —Å—Ç–æ–ª–±–µ—Ü; –µ—Å–ª–∏ —ç—Ç–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ "
            "–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—É–º–º–∞), –∏—Å–ø–æ–ª—å–∑—É–π 2 —Å—Ç–æ–ª–±—Ü–∞ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —á–∏—Å–ª–æ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å). –ò 'title' (–Ω–∞–∑–≤–∞–Ω–∏–µ "
            "–≥—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º). "
            "–í—ã–±–∏—Ä–∞–π —Å—Ç–æ–ª–±—Ü—ã —Å —É–º–æ–º. –ù–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π scatter –µ—Å–ª–∏ –Ω–µ—Ç –¥–≤—É—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏–ª–∏ pie –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å >10 "
            "—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. "
            "–ü—Ä–∏–º–µ—Ä: {{\"charts\": [{{\"chart_type\": \"bar\", \"columns\": [\"col1\"], \"title\": \"–ü—Ä–∏–º–µ—Ä "
            "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\"}}, {{\"chart_type\": \"bar\", \"columns\": [\"col1\", \"col2\"], \"title\": \"–ü—Ä–∏–º–µ—Ä "
            "—Å—É–º–º—ã\"}}]}} "
        )

        try:
            response = client.chat.completions.create(
                model="gpt-4.1-nano",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.2,
                response_format={"type": "json_object"}
            )
            response_data = json.loads(response.choices[0].message.content)
            chart_ideas = response_data.get("charts", [])

            if not chart_ideas:
                logging.warning(f"GPT –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∏–¥–µ–π –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è '{name}'.")
                continue

            chart_urls = []
            for i, idea in enumerate(chart_ideas):
                plt.figure(figsize=(10, 6))
                chart_type = idea.get("chart_type")
                columns = idea.get("columns", [])
                title = idea.get("title", "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫")

                # Ensure all proposed columns exist in the DataFrame
                if not all(col in df.columns for col in columns):
                    logging.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö "
                        f"—Å—Ç–æ–ª–±—Ü–æ–≤ ({', '.join(columns)}) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ DataFrame.")
                    plt.close()  # Important to close the figure
                    continue

                # Create a copy for plotting to avoid modifying the original dataframe in case of type conversion
                plot_df = df.copy()

                try:
                    if chart_type == 'hist' and len(columns) == 1:
                        col = columns[0]
                        # Attempt to convert to numeric, coerce errors to NaN, then drop NaNs for plotting
                        plot_df[col] = pd.to_numeric(plot_df[col], errors='coerce')
                        if pd.api.types.is_numeric_dtype(plot_df[col]):
                            sns.histplot(plot_df.dropna(subset=[col]), x=col, kde=True)
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –¥–ª—è '{col}': –°—Ç–æ–ª–±–µ—Ü –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º –ø–æ—Å–ª–µ "
                                f"–ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                            plt.close()
                            continue
                    elif chart_type == 'bar':  # Bar chart can take 1 or 2 columns
                        if len(columns) == 1:
                            col = columns[0]
                            # For single-column bar chart (frequency distribution)
                            value_counts = plot_df[col].value_counts().nlargest(15)  # Top 15 categories
                            if not value_counts.empty:
                                sns.barplot(x=value_counts.index, y=value_counts.values)
                                plt.xticks(rotation=45, ha='right')
                                plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')  # Add Y-axis label for frequency
                            else:
                                logging.warning(
                                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è '{col}'.")
                                plt.close()
                                continue
                        elif len(columns) == 2:
                            x_col, y_col = columns[0], columns[1]
                            # Attempt to convert y_col to numeric
                            plot_df[y_col] = pd.to_numeric(plot_df[y_col], errors='coerce')

                            if pd.api.types.is_numeric_dtype(plot_df[y_col]):
                                # Filter out NaN in relevant columns for grouping/summing
                                temp_df = plot_df.dropna(subset=[x_col, y_col])
                                if not temp_df.empty:
                                    top_15 = temp_df.groupby(x_col)[y_col].sum().nlargest(15)
                                    sns.barplot(x=top_15.index, y=top_15.values)
                                    plt.xticks(rotation=45, ha='right')
                                else:
                                    logging.warning(
                                        f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è "
                                        f"'{x_col}' –∏ '{y_col}' –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN.")
                                    plt.close()
                                    continue
                            else:
                                logging.warning(
                                    f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è '{y_col}': –°—Ç–æ–ª–±–µ—Ü "
                                    f"–∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                                plt.close()
                                continue
                        else:  # If column count is not 1 or 2 for bar chart
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –ù–µ–≤–µ—Ä–Ω–æ–µ "
                                f"–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ ({len(columns)}). –û–∂–∏–¥–∞–µ—Ç—Å—è 1 –∏–ª–∏ 2.")
                            plt.close()
                            continue
                    elif chart_type == 'scatter' and len(columns) == 2:
                        x_col, y_col = columns[0], columns[1]
                        # Attempt to convert both columns to numeric
                        plot_df[x_col] = pd.to_numeric(plot_df[x_col], errors='coerce')
                        plot_df[y_col] = pd.to_numeric(plot_df[y_col], errors='coerce')

                        if pd.api.types.is_numeric_dtype(plot_df[x_col]) and pd.api.types.is_numeric_dtype(
                                plot_df[y_col]):
                            # Drop NaNs for scatter plot
                            sns.scatterplot(plot_df.dropna(subset=[x_col, y_col]), x=x_col, y=y_col)
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–æ—á–µ—á–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è '{x_col}' –∏ '{y_col}': –û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ "
                                f"—Å—Ç–æ–ª–±—Ü–∞ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º–∏ –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                            plt.close()
                            continue
                    elif chart_type == 'pie' and len(columns) == 1:
                        col = columns[0]
                        # For pie, ensure the column is suitable (e.g., categorical or low unique values)
                        if plot_df[col].nunique() <= 10 and not plot_df[col].empty:
                            plot_df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False)
                            plt.ylabel('')  # Remove y-label for pie chart
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è '{col}': –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö "
                                f"–∑–Ω–∞—á–µ–Ω–∏–π (>10) –∏–ª–∏ —Å—Ç–æ–ª–±–µ—Ü –ø—É—Å—Ç.")
                            plt.close()
                            continue
                    else:  # Fallback for unsupported chart types or invalid column counts
                        logging.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞"
                            f" ({chart_type}) –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ ({len(columns)}).")
                        plt.close()
                        continue

                    plt.title(title)
                    plt.tight_layout()

                    buffer = io.BytesIO()
                    plt.savefig(buffer, format='png', bbox_inches='tight')
                    buffer.seek(0)

                    file_name_for_s3 = f"{urllib.parse.quote(name)}_{i}.png"
                    s3_key = f"charts/{report_id}/{file_name_for_s3}"

                    blob = config.gcs_bucket.blob(s3_key)
                    blob.upload_from_string(buffer.getvalue(), content_type='image/png')

                    api_url = f"{config.BASE_API_URL}/chart/{report_id}/{file_name_for_s3}"
                    chart_urls.append(api_url)
                except Exception as e:
                    logging.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' (—Ç–∞–±–ª–∏—Ü–∞: '{name}', —Ç–∏–ø: '{chart_type}', —Å—Ç–æ–ª–±—Ü—ã: {columns}): {e}",
                        exc_info=True)
                finally:
                    plt.close()  # Always close the plot to free memory

            if chart_urls:
                visualizations[name] = chart_urls

        except json.JSONDecodeError as jde:
            logging.error(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç OpenAI –¥–ª—è '{name}'. –û—Ç–≤–µ—Ç: {response.choices[0].message.content}. "
                f"–û—à–∏–±–∫–∞: {jde}")
        except Exception as e:
            logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–¥–µ–π –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è '{name}': {e}", exc_info=True)

    return visualizations


def cluster_data(df: pd.DataFrame, table_name: str, n_clusters: int = 3) -> dict:
    numeric_df = df.select_dtypes(include=np.number).dropna()
    if numeric_df.shape[0] < 10 or numeric_df.shape[1] < 2:
        return {"message": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

    try:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        scaled = StandardScaler().fit_transform(numeric_df)

        # KMeans –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=n_clusters, n_init="auto", random_state=42)
        kmeans.fit(scaled)

        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        sizes = pd.Series(kmeans.labels_).value_counts().sort_index().to_dict()

        # –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_profiles_df = numeric_df.copy()
        cluster_profiles_df["cluster"] = kmeans.labels_
        profiles = (
            cluster_profiles_df.groupby("cluster").mean().round(3).to_dict()
        )

        rf = RandomForestClassifier(random_state=42)
        rf.fit(scaled, kmeans.labels_)
        importances = rf.feature_importances_
        feature_importance = sorted(
            zip(numeric_df.columns, importances),
            key=lambda x: x[1],
            reverse=True
        )
        top_features = [f[0] for f in feature_importance[:5]]

        # GPT-–∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
        prompt = (
            f"–ú—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü—ã '{table_name}' –Ω–∞ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞(–æ–≤). "
            f"–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {sizes}. "
            f"–í–æ—Ç —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {json.dumps(profiles, ensure_ascii=False)}. "
            f"–í–∞–∂–Ω–µ–π—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {top_features}. "
            "–î–∞–π –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, —á—Ç–æ —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞ –≥—Ä—É–ø–ø—ã. "
            "–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ, –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –≤ Markdown."
        )

        try:
            gpt_response = client.chat.completions.create(
                model="gpt-4.1-nano",
                messages=[{"role": "user", "content": prompt}],
                temperature=0.4
            )
            gpt_summary = gpt_response.choices[0].message.content
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–∏ –æ–ø–∏—Å–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            gpt_summary = None

        return {
            "clusters_count": n_clusters,
            "sample_count": numeric_df.shape[0],
            "feature_count": numeric_df.shape[1],
            "sizes": sizes,
            "cluster_profiles": profiles,
            "important_features": top_features,
            "gpt_summary": gpt_summary
        }

    except Exception as e:
        return {"error": str(e)}


def generate_and_test_hypotheses(df: pd.DataFrame, table_name: str) -> List[Dict[str, Any]]:
    hypotheses_results = []

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    if df.shape[1] < 2:
        logging.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name}: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑.")
        return []

    stats = df.describe(include='all').replace({pd.NA: None}).to_json()
    columns_info = [{"name": col, "dtype": str(df[col].dtype)} for col in df.columns]

    prompt = (
        f"–í–æ—Ç DataFrame –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}', –≤–æ—Ç –µ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}. "
        f"–í–æ—Ç —Å—Ç–æ–ª–±—Ü—ã: {columns_info}. "
        "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 2 –≥–∏–ø–æ—Ç–µ–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏, "
        "–∏ —É–∫–∞–∂–∏, –ø–æ –∫–∞–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º –∏—Ö –ø—Ä–æ–≤–µ—Ä—è—Ç—å. "
        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞: "
        "[{\"hypothesis\": \"...\", \"test\": \"t-test\" –∏–ª–∏ \"chi2\", \"columns\": [\"col1\", \"col2\"]}]."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )

        raw_output = response.choices[0].message.content.strip()
        logging.debug(f"GPT hypothesis raw output for {table_name}: {raw_output}")

        try:
            parsed = json.loads(raw_output)
        except json.JSONDecodeError as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è {table_name}: {e}")
            return []

        if not isinstance(parsed, list):
            logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ GPT –¥–ª—è {table_name}, –æ–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫: {parsed}")
            return []

        for item in parsed:
            if not isinstance(item, dict):
                logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ —Å–ø–∏—Å–∫–µ –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è {table_name}: {item}")
                continue

            hypothesis = item.get("hypothesis", "").strip()
            test = item.get("test", "").strip().lower()
            cols = item.get("columns", [])

            if not hypothesis or not test or not isinstance(cols, list) or len(cols) < 2:
                logging.warning(f"–ù–µ–ø–æ–ª–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –¥–ª—è {table_name}: {item}")
                continue

            explanation = ""
            p_value = None
            result = "–Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å"

            try:
                if test == "t-test":
                    group_col, value_col = cols[0], cols[1]
                    if group_col in df.columns and value_col in df.columns:
                        groups = df[group_col].dropna().unique()
                        if len(groups) == 2:
                            a = df[df[group_col] == groups[0]][value_col].dropna()
                            b = df[df[group_col] == groups[1]][value_col].dropna()
                            if len(a) > 1 and len(b) > 1:
                                stat, p = ttest_ind(a, b)
                                result = "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞" if p < 0.05 else "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–∞"
                                p_value = round(float(p), 5)
                                explanation = f"t-test –º–µ–∂–¥—É {groups[0]} –∏ {groups[1]} –ø–æ {value_col}"
                elif test == "chi2":
                    col_a, col_b = cols[0], cols[1]
                    if col_a in df.columns and col_b in df.columns:
                        contingency = pd.crosstab(df[col_a], df[col_b])
                        if contingency.shape[0] > 1 and contingency.shape[1] > 1:
                            stat, p, *_ = chi2_contingency(contingency)
                            result = "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞" if p < 0.05 else "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–∞"
                            p_value = round(float(p), 5)
                            explanation = f"chi2 –º–µ–∂–¥—É {col_a} –∏ {col_b}"
            except Exception as e:
                explanation = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}"
                logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≥–∏–ø–æ—Ç–µ–∑—ã {hypothesis} –≤ {table_name}: {e}")

            hypotheses_results.append({
                "hypothesis": hypothesis,
                "test": test,
                "columns": cols,
                "p_value": p_value,
                "result": result,
                "explanation": explanation
            })

    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è {table_name}: {e}")

    return hypotheses_results


def perform_full_analysis(
        inspector: Inspector, dataframes: Dict[str, pd.DataFrame], report_id: int
) -> Dict[str, Any]:
    single_table_analysis = {}
    for table, df in dataframes.items():
        single_table_analysis[table] = analyze_single_table(table, df)

    joint_table_analysis = analyze_joins(inspector, dataframes)
    visualizations = generate_visualizations(dataframes.copy(), report_id)

    def generate_overall_summary(dataframes: Dict[str, pd.DataFrame], insights: Dict[str, Any],
                                 joins: Dict[str, Any]) -> str:
        prompt = (
            f"–¢—ã –ø–æ–ª—É—á–∏–ª –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏: {list(dataframes.keys())}. "
            f"–í–æ—Ç –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç–∞–±–ª–∏—Ü–∞–º: {json.dumps(insights, ensure_ascii=False)[:5000]}. "
            f"–í–æ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏: {json.dumps(joins, ensure_ascii=False)[:3000]}. "
            "–°–¥–µ–ª–∞–π –æ–±—â–∏–π –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –æ–±–∑–æ—Ä –≤—Å–µ–π –±–∞–∑—ã: –æ–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ —Ç—Ä–µ–Ω–¥—ã, —Å–≤—è–∑–∏, –∞–Ω–æ–º–∞–ª–∏–∏, –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è "
            "–ø–∞—Ç—Ç–µ—Ä–Ω—ã. "
            "–û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –≤ –≤–∏–¥–µ Markdown —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ –ø–æ–¥–∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏."
        )
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content

    def clean_nan(obj):
        if isinstance(obj, dict):
            return {k: clean_nan(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [clean_nan(item) for item in obj]
        if isinstance(obj, float) and np.isnan(obj):
            return None
        return obj

    hypotheses_by_table = {}
    for table, df in dataframes.items():
        hypotheses_by_table[table] = generate_and_test_hypotheses(df, table)

    clusters_by_table = {}
    for table, df in dataframes.items():
        clusters_by_table[table] = cluster_data(df, table)

    return clean_nan({
        "single_table_insights": single_table_analysis,
        "joint_table_insights": joint_table_analysis,
        "visualizations": visualizations,
        "overall_summary": generate_overall_summary(dataframes, single_table_analysis, joint_table_analysis),
        "hypotheses": hypotheses_by_table,
        "clusters": clusters_by_table
    })
