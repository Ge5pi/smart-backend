from typing import Dict, Any, Set, Tuple, List
import pandas as pd
import numpy as np
import io
from sklearn.ensemble import RandomForestClassifier
from sqlalchemy.engine import Inspector
from openai import OpenAI
import logging
import json
import matplotlib.pyplot as plt
import seaborn as sns
import config
from config import API_KEY
import urllib.parse
from scipy.stats import ttest_ind, chi2_contingency
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import redis
import pickle
import hashlib
import time
from concurrent.futures import ThreadPoolExecutor

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞
client = OpenAI(api_key=API_KEY)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Redis –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è
try:
    from config import REDIS_URL

    redis_client = redis.from_url(REDIS_URL, decode_responses=False)
except:
    redis_client = None
    logging.warning("Redis –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–æ")

# –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
try:
    from config import (
        MAX_DATAFRAME_ROWS,
        MAX_TABLES_DETAILED_ANALYSIS,
        MAX_JOINS_TO_ANALYZE,
        MAX_CHARTS_PER_TABLE,
        MAX_TABLES_TO_VISUALIZE,
        CACHE_TTL_ANALYSIS,
        CACHE_TTL_VISUALIZATIONS
    )
except ImportError:
    MAX_DATAFRAME_ROWS = 100000
    MAX_TABLES_DETAILED_ANALYSIS = 10
    MAX_JOINS_TO_ANALYZE = 10
    MAX_CHARTS_PER_TABLE = 2
    MAX_TABLES_TO_VISUALIZE = 5
    CACHE_TTL_ANALYSIS = 3600
    CACHE_TTL_VISUALIZATIONS = 7200


def get_df_hash(df: pd.DataFrame) -> str:
    """–°–æ–∑–¥–∞–µ—Ç —Ö–µ—à DataFrame –¥–ª—è –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
    df_string = f"{df.shape}:{df.columns.tolist()}:{df.head().to_json()}"
    return hashlib.md5(df_string.encode()).hexdigest()


def analyze_single_table(table_name: str, df: pd.DataFrame) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–¥–Ω—É —Ç–∞–±–ª–∏—Ü—É —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT –∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤.
    –í–∫–ª—é—á–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤.
    """
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à
    if redis_client:
        df_hash = get_df_hash(df)
        cache_key = f"analysis:{table_name}:{df_hash}"
        try:
            cached = redis_client.get(cache_key)
            if cached:
                logging.info(f"–ù–∞–π–¥–µ–Ω –∫–µ—à –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}")
                return pickle.loads(cached)
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if len(df) > 100000:
        df_sample = df.sample(n=100000, random_state=42)
        logging.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name}: –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—ã–±–æ—Ä–∫—É –∏–∑ 100000 —Å—Ç—Ä–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞")
    else:
        df_sample = df

    numeric_df = df_sample.select_dtypes(include=np.number)

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ - —Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ–ø –∫–æ–ª–æ–Ω–æ–∫
    if not numeric_df.empty and numeric_df.shape[1] > 10:
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ 10 —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –ø–æ –≤–∞—Ä–∏–∞—Ü–∏–∏
        variances = numeric_df.var().sort_values(ascending=False)
        top_cols = variances.head(10).index.tolist()
        numeric_df = numeric_df[top_cols]

    corr = numeric_df.corr().replace({np.nan: None}).to_dict() if not numeric_df.empty else {}

    # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ - —Ç–æ–ª—å–∫–æ –±–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    stats_dict = {}
    for col in df_sample.columns[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 –∫–æ–ª–æ–Ω–∫–∞–º–∏
        col_stats = {
            'count': int(df_sample[col].count()),
            'unique': int(df_sample[col].nunique())
        }
        if pd.api.types.is_numeric_dtype(df_sample[col]):
            col_stats.update({
                'mean': float(df_sample[col].mean()) if not pd.isna(df_sample[col].mean()) else None,
                'std': float(df_sample[col].std()) if not pd.isna(df_sample[col].std()) else None,
                'min': float(df_sample[col].min()) if not pd.isna(df_sample[col].min()) else None,
                'max': float(df_sample[col].max()) if not pd.isna(df_sample[col].max()) else None
            })
        stats_dict[col] = col_stats

    stats = json.dumps(stats_dict, ensure_ascii=False)

    prompt = (
        f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'. "
        f"–°—Ç—Ä–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ: {len(df)}. "
        f"–í–æ—Ç –æ–ø–∏—Å–∞—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ (–≤—ã–±–æ—Ä–∫–∞): {stats}. "
        f"–í–æ—Ç –º–∞—Ç—Ä–∏—Ü–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è –∫–ª—é—á–µ–≤—ã—Ö —á–∏—Å–ª–æ–≤—ã—Ö –ø–æ–ª–µ–π: {json.dumps(corr)}. "
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –≤—ã—è–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ –∏–Ω—Å–∞–π—Ç—ã, —Å–∫—Ä—ã—Ç—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏ –∏ –∞–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö —ç—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã. "
        "–ë—É–¥—å –∫—Ä–∞—Ç–∫–∏–º, —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –∏ –ø–∏—à–∏ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è Markdown –¥–ª—è –≤—ã–¥–µ–ª–µ–Ω–∏—è (`**—Ç–µ—Ä–º–∏–Ω**`)."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        insight = response.choices[0].message.content
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ GPT –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
        insight = "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ –æ—Ç GPT"

    result = {"insight": insight, "correlations": corr, "row_count": len(df)}

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
    if redis_client:
        try:
            redis_client.setex(cache_key, CACHE_TTL_ANALYSIS, pickle.dumps(result))
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ –∫–µ—à: {e}")

    return result


def analyze_joins(inspector: Inspector, dataframes: Dict[str, pd.DataFrame]) -> Dict[str, Any]:
    """
    –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ —á–µ—Ä–µ–∑ foreign keys.
    –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π.
    """
    joint_insights = {}
    analyzed_pairs: Set[Tuple[str, str]] = set()
    all_tables = list(dataframes.keys())
    join_count = 0

    for table_name in all_tables:
        if join_count >= MAX_JOINS_TO_ANALYZE:
            logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π: {MAX_JOINS_TO_ANALYZE}")
            break

        try:
            foreign_keys = inspector.get_foreign_keys(table_name)
        except Exception as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {e}")
            continue

        for fk in foreign_keys:
            if join_count >= MAX_JOINS_TO_ANALYZE:
                break

            left_table = table_name
            right_table = fk['referred_table']
            pair = tuple(sorted((left_table, right_table)))

            if pair in analyzed_pairs:
                continue

            analyzed_pairs.add(pair)

            df_left = dataframes[left_table]
            df_right = dataframes.get(right_table)

            if df_right is None:
                continue

            left_on = fk['constrained_columns']
            right_on = fk['referred_columns']

            try:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ø–µ—Ä–µ–¥ JOIN
                max_rows_for_join = 5000
                df_left_sample = df_left.sample(n=min(len(df_left), max_rows_for_join), random_state=42)
                df_right_sample = df_right.sample(n=min(len(df_right), max_rows_for_join), random_state=42)

                merged_df = pd.merge(
                    df_left_sample, df_right_sample,
                    left_on=left_on,
                    right_on=right_on,
                    suffixes=(f'_{left_table}', f'_{right_table}'),
                    how='inner'  # –ò—Å–ø–æ–ª—å–∑—É–µ–º inner join –¥–ª—è –º–µ–Ω—å—à–µ–≥–æ —Ä–∞–∑–º–µ—Ä–∞
                )

                if merged_df.empty or len(merged_df) < 10:
                    continue

                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                if len(merged_df.columns) > 20:
                    # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ —Å –Ω–∞–∏–±–æ–ª—å—à–µ–π –≤–∞—Ä–∏–∞—Ü–∏–µ–π
                    numeric_cols = merged_df.select_dtypes(include=np.number).columns
                    if len(numeric_cols) > 10:
                        variances = merged_df[numeric_cols].var().sort_values(ascending=False)
                        top_numeric = variances.head(10).index.tolist()
                        other_cols = [c for c in merged_df.columns if c not in numeric_cols][:10]
                        merged_df = merged_df[top_numeric + other_cols]

                join_key = f"{left_table} üîó {right_table}"

                # –ë—ã—Å—Ç—Ä–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ analyze_single_table
                numeric_df = merged_df.select_dtypes(include=np.number)
                corr = numeric_df.corr().replace({np.nan: None}).to_dict() if not numeric_df.empty else {}

                basic_stats = {
                    'merged_rows': len(merged_df),
                    'columns': len(merged_df.columns),
                    'left_rows': len(df_left),
                    'right_rows': len(df_right)
                }

                prompt = (
                    f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –°–í–Ø–ó–¨ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ '{left_table}' ({basic_stats['left_rows']} —Å—Ç—Ä–æ–∫) "
                    f"–∏ '{right_table}' ({basic_stats['right_rows']} —Å—Ç—Ä–æ–∫), –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã "
                    f"–ø–æ –∫–ª—é—á–∞–º ({left_table}.{','.join(left_on)} = {right_table}.{','.join(right_on)}). "
                    f"–ü–æ—Å–ª–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –ø–æ–ª—É—á–∏–ª–æ—Å—å {basic_stats['merged_rows']} —Å—Ç—Ä–æ–∫. "
                    f"–ö–ª—é—á–µ–≤—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏: {json.dumps(corr)[:500]}. "
                    "–°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –ø–æ–∏—Å–∫–µ –∏–Ω—Å–∞–π—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ –≤–æ–∑–Ω–∏–∫–∞—é—Ç –∏–º–µ–Ω–Ω–æ –∏–∑-–∑–∞ —Å–≤—è–∑–∏ –¥–≤—É—Ö —Ç–∞–±–ª–∏—Ü. "
                    "–û—Ç–≤–µ—Ç –¥–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è), –∏—Å–ø–æ–ª—å–∑—É—è Markdown."
                )

                try:
                    response = client.chat.completions.create(
                        model="gpt-5-mini",
                        messages=[{"role": "user", "content": prompt}]
                    )

                    joint_insights[join_key] = {
                        "insight": response.choices[0].message.content,
                        "correlations": corr,
                        "stats": basic_stats
                    }
                except Exception as e:
                    logging.error(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ —Å–≤—è–∑–∏ {join_key}: {e}")
                    joint_insights[join_key] = {
                        "insight": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–∏",
                        "correlations": corr,
                        "stats": basic_stats
                    }

                join_count += 1

            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –∏ –∞–Ω–∞–ª–∏–∑–µ —Ç–∞–±–ª–∏—Ü {left_table} –∏ {right_table}: {e}")

    return joint_insights


def generate_visualizations(
        dataframes: dict[str, pd.DataFrame], report_id: int
) -> dict[str, list[str]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è —Ç–∞–±–ª–∏—Ü —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º GPT –∏ matplotlib.
    –í–∫–ª—é—á–∞–µ—Ç –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤.
    """
    visualizations = {}
    sns.set_theme(style="whitegrid")

    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤
    table_count = 0

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–∞–±–ª–∏—Ü—ã –ø–æ —Ä–∞–∑–º–µ—Ä—É
    sorted_tables = sorted(dataframes.items(), key=lambda x: len(x[1]), reverse=True)

    for name, df in sorted_tables:
        if table_count >= MAX_TABLES_TO_VISUALIZE:
            logging.info(f"–î–æ—Å—Ç–∏–≥–Ω—É—Ç –ª–∏–º–∏—Ç –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {MAX_TABLES_TO_VISUALIZE} —Ç–∞–±–ª–∏—Ü")
            break

        if df.empty or len(df) < 10:
            logging.warning(f"DataFrame –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}' —Å–ª–∏—à–∫–æ–º –º–∞–ª, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é.")
            continue

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–µ—à –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π
        if redis_client:
            cache_key = f"viz:{report_id}:{name}"
            try:
                cached_urls = redis_client.get(cache_key)
                if cached_urls:
                    visualizations[name] = pickle.loads(cached_urls)
                    logging.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∑–∞–∫–µ—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–ª—è {name}")
                    table_count += 1
                    continue
            except Exception as e:
                logging.warning(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π: {e}")

        column_info = [
            {"name": col, "dtype": str(df[col].dtype), "nunique": df[col].nunique(),
             "is_numeric": pd.api.types.is_numeric_dtype(df[col])}
            for col in df.columns
        ]

        prompt = (
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π DataFrame —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º '{name}' —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ —Å—Ç–æ–ª–±—Ü–∞–º–∏ –∏ –∏—Ö —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏: "
            f"{json.dumps(column_info)}. "
            f"–ü—Ä–µ–¥–ª–æ–∂–∏ –¥–æ {MAX_CHARTS_PER_TABLE} –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —ç—Ç–∏—Ö –¥–∞–Ω–Ω—ã—Ö. "
            "–û—Ç–≤–µ—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å –≤ –≤–∏–¥–µ JSON-–æ–±—ä–µ–∫—Ç–∞ —Å –∫–ª—é—á–æ–º 'charts', –∫–æ—Ç–æ—Ä—ã–π —Å–æ–¥–µ—Ä–∂–∏—Ç –º–∞—Å—Å–∏–≤ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π. "
            "–ö–∞–∂–¥—ã–π –æ–±—ä–µ–∫—Ç –≤ –º–∞—Å—Å–∏–≤–µ –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å: 'chart_type' ('hist', 'bar', 'scatter', 'pie'), "
            "'columns' (–°–ü–ò–°–û–ö –°–£–©–ï–°–¢–í–£–Æ–©–ò–• –°–¢–û–õ–ë–¶–û–í, –í–´–ë–†–ê–ù–ù–´–• –ò–ó –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –°–ü–ò–°–ö–ê). –î–ª—è 'bar' –≥—Ä–∞—Ñ–∏–∫–∞: –µ—Å–ª–∏ "
            "—ç—Ç–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –æ–¥–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (—á–∞—Å—Ç–æ—Ç–∞), –∏—Å–ø–æ–ª—å–∑—É–π 1 —Å—Ç–æ–ª–±–µ—Ü; –µ—Å–ª–∏ —ç—Ç–æ –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø–æ "
            "–∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, —Å—É–º–º–∞), –∏—Å–ø–æ–ª—å–∑—É–π 2 —Å—Ç–æ–ª–±—Ü–∞ (–∫–∞—Ç–µ–≥–æ—Ä–∏—è, —á–∏—Å–ª–æ–≤–æ–π –ø–æ–∫–∞–∑–∞—Ç–µ–ª—å). –ò 'title' (–Ω–∞–∑–≤–∞–Ω–∏–µ "
            "–≥—Ä–∞—Ñ–∏–∫–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º). "
            "–í—ã–±–∏—Ä–∞–π —Å—Ç–æ–ª–±—Ü—ã —Å —É–º–æ–º. –ù–µ –ø—Ä–µ–¥–ª–∞–≥–∞–π scatter –µ—Å–ª–∏ –Ω–µ—Ç –¥–≤—É—Ö —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏–ª–∏ pie –¥–ª—è –∫–æ–ª–æ–Ω–æ–∫ —Å >10 "
            "—É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π. "
            "–ü—Ä–∏–º–µ—Ä: {\\\"charts\\\": [{\\\"chart_type\\\": \\\"bar\\\", \\\"columns\\\": [\\\"col1\\\"], \\\"title\\\": \\\"–ü—Ä–∏–º–µ—Ä "
            "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è\\\"}, {\\\"chart_type\\\": \\\"bar\\\", \\\"columns\\\": [\\\"col1\\\", \\\"col2\\\"], \\\"title\\\": \\\"–ü—Ä–∏–º–µ—Ä "
            "—Å—É–º–º—ã\\\"}]} "
        )

        try:
            response = client.chat.completions.create(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}
            )
            response_data = json.loads(response.choices[0].message.content)
            chart_ideas = response_data.get("charts", [])

            if not chart_ideas:
                logging.warning(f"GPT –Ω–µ –ø—Ä–µ–¥–ª–æ–∂–∏–ª –∏–¥–µ–π –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è '{name}'.")
                continue

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥—Ä–∞—Ñ–∏–∫–æ–≤ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
            chart_ideas = chart_ideas[:MAX_CHARTS_PER_TABLE]

            chart_urls = []
            for i, idea in enumerate(chart_ideas):
                plt.figure(figsize=(10, 6))
                chart_type = idea.get("chart_type")
                columns = idea.get("columns", [])
                title = idea.get("title", "–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫")

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫
                if not all(col in df.columns for col in columns):
                    logging.warning(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –û–¥–∏–Ω –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö "
                        f"—Å—Ç–æ–ª–±—Ü–æ–≤ ({', '.join(columns)}) –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ DataFrame.")
                    plt.close()
                    continue

                plot_df = df.copy()

                try:
                    if chart_type == 'hist' and len(columns) == 1:
                        col = columns[0]
                        plot_df[col] = pd.to_numeric(plot_df[col], errors='coerce')
                        if pd.api.types.is_numeric_dtype(plot_df[col]):
                            sns.histplot(plot_df.dropna(subset=[col]), x=col, kde=True)
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—É –¥–ª—è '{col}': –°—Ç–æ–ª–±–µ—Ü –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º –ø–æ—Å–ª–µ "
                                f"–ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                            plt.close()
                            continue

                    elif chart_type == 'bar':
                        if len(columns) == 1:
                            col = columns[0]
                            value_counts = plot_df[col].value_counts().nlargest(15)
                            if not value_counts.empty:
                                sns.barplot(x=value_counts.index, y=value_counts.values)
                                plt.xticks(rotation=45, ha='right')
                                plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
                            else:
                                logging.warning(
                                    f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã —á–∞—Å—Ç–æ—Ç—ã –¥–ª—è '{col}'.")
                                plt.close()
                                continue

                        elif len(columns) == 2:
                            x_col, y_col = columns[0], columns[1]
                            plot_df[y_col] = pd.to_numeric(plot_df[y_col], errors='coerce')
                            if pd.api.types.is_numeric_dtype(plot_df[y_col]):
                                temp_df = plot_df.dropna(subset=[x_col, y_col])
                                if not temp_df.empty:
                                    top_15 = temp_df.groupby(x_col)[y_col].sum().nlargest(15)
                                    sns.barplot(x=top_15.index, y=top_15.values)
                                    plt.xticks(rotation=45, ha='right')
                                else:
                                    logging.warning(
                                        f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ—Å—Ç—Ä–æ–µ–Ω–∏—è —Å—Ç–æ–ª–±—á–∞—Ç–æ–π –¥–∏–∞–≥—Ä–∞–º–º—ã –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è "
                                        f"'{x_col}' –∏ '{y_col}' –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏ NaN.")
                                    plt.close()
                                    continue
                            else:
                                logging.warning(
                                    f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –¥–ª—è '{y_col}': –°—Ç–æ–ª–±–µ—Ü "
                                    f"–∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                                plt.close()
                                continue
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Å—Ç–æ–ª–±—á–∞—Ç—É—é –¥–∏–∞–≥—Ä–∞–º–º—É '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –ù–µ–≤–µ—Ä–Ω–æ–µ "
                                f"–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ ({len(columns)}). –û–∂–∏–¥–∞–µ—Ç—Å—è 1 –∏–ª–∏ 2.")
                            plt.close()
                            continue

                    elif chart_type == 'scatter' and len(columns) == 2:
                        x_col, y_col = columns[0], columns[1]
                        plot_df[x_col] = pd.to_numeric(plot_df[x_col], errors='coerce')
                        plot_df[y_col] = pd.to_numeric(plot_df[y_col], errors='coerce')
                        if pd.api.types.is_numeric_dtype(plot_df[x_col]) and pd.api.types.is_numeric_dtype(
                                plot_df[y_col]):
                            sns.scatterplot(plot_df.dropna(subset=[x_col, y_col]), x=x_col, y=y_col)
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ç–æ—á–µ—á–Ω—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è '{x_col}' –∏ '{y_col}': –û–¥–∏–Ω –∏–ª–∏ –æ–±–∞ "
                                f"—Å—Ç–æ–ª–±—Ü–∞ –Ω–µ —è–≤–ª—è—é—Ç—Å—è —á–∏—Å–ª–æ–≤—ã–º–∏ –ø–æ—Å–ª–µ –ø–æ–ø—ã—Ç–∫–∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è.")
                            plt.close()
                            continue

                    elif chart_type == 'pie' and len(columns) == 1:
                        col = columns[0]
                        if plot_df[col].nunique() <= 10 and not plot_df[col].empty:
                            plot_df[col].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, counterclock=False)
                            plt.ylabel('')
                        else:
                            logging.warning(
                                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –∫—Ä—É–≥–æ–≤—É—é –¥–∏–∞–≥—Ä–∞–º–º—É –¥–ª—è '{col}': –°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö "
                                f"–∑–Ω–∞—á–µ–Ω–∏–π (>10) –∏–ª–∏ —Å—Ç–æ–ª–±–µ—Ü –ø—É—Å—Ç.")
                            plt.close()
                            continue

                    else:
                        logging.warning(
                            f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã '{name}': –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø –≥—Ä–∞—Ñ–∏–∫–∞"
                            f" ({chart_type}) –∏–ª–∏ –Ω–µ–≤–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ ({len(columns)}).")
                        plt.close()
                        continue

                    plt.title(title)
                    plt.tight_layout()
                    buffer = io.BytesIO()
                    plt.savefig(buffer, format='png', bbox_inches='tight')
                    buffer.seek(0)

                    file_name_for_s3 = f"{urllib.parse.quote(name)}_{i}.png"
                    s3_key = f"charts/{report_id}/{file_name_for_s3}"
                    blob = config.gcs_bucket.blob(s3_key)
                    blob.upload_from_string(buffer.getvalue(), content_type='image/png')

                    api_url = f"{config.BASE_API_URL}/chart/{report_id}/{file_name_for_s3}"
                    chart_urls.append(api_url)

                except Exception as e:
                    logging.error(
                        f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å –≥—Ä–∞—Ñ–∏–∫ '{title}' (—Ç–∞–±–ª–∏—Ü–∞: '{name}', —Ç–∏–ø: '{chart_type}', —Å—Ç–æ–ª–±—Ü—ã: {columns}): {e}",
                        exc_info=True)
                finally:
                    plt.close()

            if chart_urls:
                visualizations[name] = chart_urls

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫–µ—à
                if redis_client:
                    try:
                        redis_client.setex(f"viz:{report_id}:{name}", CACHE_TTL_VISUALIZATIONS,
                                           pickle.dumps(chart_urls))
                    except Exception as e:
                        logging.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π –≤ –∫–µ—à: {e}")

            table_count += 1

        except json.JSONDecodeError as jde:
            logging.error(
                f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –æ—Ç OpenAI –¥–ª—è '{name}'. –û—Ç–≤–µ—Ç: {response.choices[0].message.content}. "
                f"–û—à–∏–±–∫–∞: {jde}")
        except Exception as e:
            logging.error(f"–û–±—â–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–¥–µ–π –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –¥–ª—è '{name}': {e}", exc_info=True)

    return visualizations


def cluster_data(df: pd.DataFrame, table_name: str, n_clusters: int = 3) -> dict:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º KMeans
    """
    numeric_df = df.select_dtypes(include=np.number).dropna()

    if numeric_df.shape[0] < 10 or numeric_df.shape[1] < 2:
        return {"message": "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"}

    try:
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        scaled = StandardScaler().fit_transform(numeric_df)

        # KMeans –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
        kmeans = KMeans(n_clusters=n_clusters, n_init="auto", random_state=42)
        kmeans.fit(scaled)

        # –†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        sizes = pd.Series(kmeans.labels_).value_counts().sort_index().to_dict()

        # –ü—Ä–æ—Ñ–∏–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_profiles_df = numeric_df.copy()
        cluster_profiles_df["cluster"] = kmeans.labels_
        profiles = cluster_profiles_df.groupby("cluster").mean().round(3).to_dict()

        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        rf = RandomForestClassifier(random_state=42, n_estimators=50)  # –°–Ω–∏–∂–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        rf.fit(scaled, kmeans.labels_)
        importances = rf.feature_importances_
        feature_importance = sorted(
            zip(numeric_df.columns, importances),
            key=lambda x: x[1],
            reverse=True
        )

        top_features = [f[0] for f in feature_importance[:5]]

        prompt = (
            f"–ú—ã –ø—Ä–æ–≤–µ–ª–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü—ã '{table_name}' –Ω–∞ {n_clusters} –∫–ª–∞—Å—Ç–µ—Ä–∞(–æ–≤). "
            f"–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {sizes}. "
            f"–í–æ—Ç —Å—Ä–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º: {json.dumps(profiles, ensure_ascii=False)}. "
            f"–í–∞–∂–Ω–µ–π—à–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {top_features}. "
            "–î–∞–π –ø–æ–Ω—è—Ç–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏ –ø—Ä–µ–¥–ª–æ–∂–∏, —á—Ç–æ —ç—Ç–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∑–∞ –≥—Ä—É–ø–ø—ã. "
            "–û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ, –Ω–∞ —Ä—É—Å—Å–∫–æ–º, –≤ Markdown."
        )

        try:
            gpt_response = client.chat.completions.create(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            gpt_summary = gpt_response.choices[0].message.content
        except Exception as e:
            logging.warning(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–∏ –æ–ø–∏—Å–∞–Ω–∏–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {e}")
            gpt_summary = None

        return {
            "clusters_count": n_clusters,
            "sample_count": numeric_df.shape[0],
            "feature_count": numeric_df.shape[1],
            "sizes": sizes,
            "cluster_profiles": profiles,
            "important_features": top_features,
            "gpt_summary": gpt_summary
        }

    except Exception as e:
        return {"error": str(e)}


def generate_and_test_hypotheses(df: pd.DataFrame, table_name: str) -> List[Dict[str, Any]]:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏ –ø—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
    """
    hypotheses_results = []

    # –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
    if df.shape[1] < 2:
        logging.info(f"–¢–∞–±–ª–∏—Ü–∞ {table_name}: —Å–ª–∏—à–∫–æ–º –º–∞–ª–æ –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≥–∏–ø–æ—Ç–µ–∑.")
        return []

    stats = df.describe(include='all').replace({pd.NA: None}).to_json()
    columns_info = [{"name": col, "dtype": str(df[col].dtype)} for col in df.columns]

    prompt = (
        f"–í–æ—Ç DataFrame –∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}', –≤–æ—Ç –µ–≥–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {stats}. "
        f"–í–æ—Ç —Å—Ç–æ–ª–±—Ü—ã: {columns_info}. "
        "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π 2 –≥–∏–ø–æ—Ç–µ–∑—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏, "
        "–∏ —É–∫–∞–∂–∏, –ø–æ –∫–∞–∫–∏–º —Å—Ç–æ–ª–±—Ü–∞–º –∏—Ö –ø—Ä–æ–≤–µ—Ä—è—Ç—å. "
        "–û—Ç–≤–µ—Ç—å —Å—Ç—Ä–æ–≥–æ –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON –º–∞—Å—Å–∏–≤–∞ –æ–±—ä–µ–∫—Ç–æ–≤, –±–µ–∑ –ª–∏—à–Ω–µ–≥–æ —Ç–µ–∫—Å—Ç–∞: "
        "[{\\\"hypothesis\\\": \\\"...\\\", \\\"test\\\": \\\"t-test\\\" –∏–ª–∏ \\\"chi2\\\", \\\"columns\\\": [\\\"col1\\\", \\\"col2\\\"]}]."
    )

    try:
        response = client.chat.completions.create(
            model="gpt-5-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        raw_output = response.choices[0].message.content.strip()
        logging.debug(f"GPT hypothesis raw output for {table_name}: {raw_output}")

        try:
            parsed = json.loads(raw_output)
        except json.JSONDecodeError as e:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è {table_name}: {e}")
            return []

        if not isinstance(parsed, list):
            logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ GPT –¥–ª—è {table_name}, –æ–∂–∏–¥–∞–ª—Å—è —Å–ø–∏—Å–æ–∫: {parsed}")
            return []

        for item in parsed:
            if not isinstance(item, dict):
                logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –≤ —Å–ø–∏—Å–∫–µ –≥–∏–ø–æ—Ç–µ–∑ –¥–ª—è {table_name}: {item}")
                continue

            hypothesis = item.get("hypothesis", "").strip()
            test = item.get("test", "").strip().lower()
            cols = item.get("columns", [])

            if not hypothesis or not test or not isinstance(cols, list) or len(cols) < 2:
                logging.warning(f"–ù–µ–ø–æ–ª–Ω–∞—è –≥–∏–ø–æ—Ç–µ–∑–∞ –¥–ª—è {table_name}: {item}")
                continue

            explanation = ""
            p_value = None
            result = "–Ω–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å"

            try:
                if test == "t-test":
                    group_col, value_col = cols[0], cols[1]
                    if group_col in df.columns and value_col in df.columns:
                        groups = df[group_col].dropna().unique()
                        if len(groups) == 2:
                            a = df[df[group_col] == groups[0]][value_col].dropna()
                            b = df[df[group_col] == groups[1]][value_col].dropna()
                            if len(a) > 1 and len(b) > 1:
                                stat, p = ttest_ind(a, b)
                                result = "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞" if p < 0.05 else "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–∞"
                                p_value = round(float(p), 5)
                                explanation = f"t-test –º–µ–∂–¥—É {groups[0]} –∏ {groups[1]} –ø–æ {value_col}"

                elif test == "chi2":
                    col_a, col_b = cols[0], cols[1]
                    if col_a in df.columns and col_b in df.columns:
                        contingency = pd.crosstab(df[col_a], df[col_b])
                        if contingency.shape[0] > 1 and contingency.shape[1] > 1:
                            stat, p, *_ = chi2_contingency(contingency)
                            result = "–ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞" if p < 0.05 else "–æ–ø—Ä–æ–≤–µ—Ä–≥–Ω—É—Ç–∞"
                            p_value = round(float(p), 5)
                            explanation = f"chi2 –º–µ–∂–¥—É {col_a} –∏ {col_b}"

            except Exception as e:
                explanation = f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ: {e}"
                logging.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –≥–∏–ø–æ—Ç–µ–∑—ã {hypothesis} –≤ {table_name}: {e}")

            hypotheses_results.append({
                "hypothesis": hypothesis,
                "test": test,
                "columns": cols,
                "p_value": p_value,
                "result": result,
                "explanation": explanation
            })

    except Exception as e:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã –¥–ª—è {table_name}: {e}")

    return hypotheses_results


def perform_full_analysis(
        inspector: Inspector, dataframes: Dict[str, pd.DataFrame], report_id: int
) -> Dict[str, Any]:
    """
    –í—ã–ø–æ–ª–Ω—è–µ—Ç –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.
    –í–∫–ª—é—á–∞–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—é —Ç–∞–±–ª–∏—Ü.
    """
    start_time = time.time()

    # –ü—Ä–æ—Ñ–∏–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏
    def log_step(step_name, step_start):
        duration = time.time() - step_start
        logging.info(f"[–û—Ç—á–µ—Ç {report_id}] {step_name}: {duration:.2f}s")
        return time.time()

    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-N —Å–∞–º—ã—Ö –≤–∞–∂–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
    table_sizes = {name: len(df) for name, df in dataframes.items()}
    sorted_tables = sorted(table_sizes.items(), key=lambda x: x[1], reverse=True)

    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–¥—Ä–æ–±–Ω–æ —Ç–æ–ª—å–∫–æ —Ç–æ–ø —Ç–∞–±–ª–∏—Ü, –æ—Å—Ç–∞–ª—å–Ω—ã–µ - –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
    priority_tables = [t[0] for t in sorted_tables[:MAX_TABLES_DETAILED_ANALYSIS]]

    logging.info(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {priority_tables}")

    # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
    step_start = time.time()
    single_table_analysis = {}

    with ThreadPoolExecutor(max_workers=3) as executor:
        future_to_table = {
            executor.submit(analyze_single_table, table, df): table
            for table, df in dataframes.items() if table in priority_tables
        }

        for future in future_to_table:
            table = future_to_table[future]
            try:
                single_table_analysis[table] = future.result(timeout=120)  # 2 –º–∏–Ω—É—Ç—ã –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
            except Exception as e:
                logging.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç–∞–±–ª–∏—Ü—ã {table}: {e}")
                single_table_analysis[table] = {"insight": "–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞", "correlations": {}}

    # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü - –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ GPT
    for table, df in dataframes.items():
        if table not in priority_tables:
            numeric_df = df.select_dtypes(include=np.number)
            corr = numeric_df.corr().replace({np.nan: None}).to_dict() if not numeric_df.empty else {}
            single_table_analysis[table] = {
                "insight": f"–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫",
                "correlations": corr,
                "row_count": len(df)
            }

    step_start = log_step("–ê–Ω–∞–ª–∏–∑ –æ—Ç–¥–µ–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü", step_start)

    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏
    joint_table_analysis = analyze_joins(inspector, dataframes)
    step_start = log_step("–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π", step_start)

    # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü
    priority_dataframes = {k: v for k, v in dataframes.items() if k in priority_tables[:MAX_TABLES_TO_VISUALIZE]}
    visualizations = generate_visualizations(priority_dataframes.copy(), report_id)
    step_start = log_step("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π", step_start)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ
    def generate_overall_summary(dataframes, insights, joins):
        prompt = f"""
–¢—ã ‚Äî –∞–Ω–∞–ª–∏—Ç–∏–∫. –°–¥–µ–ª–∞–π —Å—Ç—Ä–æ–≥–∏–π –æ–±–∑–æ—Ä –ë–î –Ω–∞ —Ä—É—Å—Å–∫–æ–º –≤ Markdown –±–µ–∑ –ø—Ä–µ–∞–º–±—É–ª –∏ –±–µ–∑ ¬´–µ—Å–ª–∏ –Ω—É–∂–Ω–æ, –º–æ–≥—É‚Ä¶¬ª.

–î–∞–Ω–æ:
- –¢–∞–±–ª–∏—Ü—ã: {list(dataframes.keys())}
- –ò–Ω—Å–∞–π—Ç—ã: {json.dumps(insights, ensure_ascii=False)[:5000]}
- –°–≤—è–∑–∏: {json.dumps(joins, ensure_ascii=False)[:3000]}

–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–≤–µ—Ç–∞ (—Ä–æ–≤–Ω–æ —ç—Ç–∏ —Å–µ–∫—Ü–∏–∏ –∏ –ø–æ—Ä—è–¥–æ–∫):

# –û–±—â–∏–π –æ–±–∑–æ—Ä

## –ö–ª—é—á–µ–≤—ã–µ —Ç—Ä–µ–Ω–¥—ã

## –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏

## –ê–Ω–æ–º–∞–ª–∏–∏ –∏ –≤—ã–±—Ä–æ—Å—ã

## –ü–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è –ø–∞—Ç—Ç–µ—Ä–Ω—ã

## –†–∏—Å–∫–∏ –∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∞–Ω–∞–ª–∏–∑–∞

## –ß—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ

–ü—Ä–∞–≤–∏–ª–∞:
- –¢–æ–ª—å–∫–æ –ø–æ –¥–∞–Ω–Ω—ã–º –≤—ã—à–µ, –±–µ–∑ –≤—ã–¥—É–º–æ–∫. –ü—Ä–∏–≤—è–∑—ã–≤–∞–π –≤—ã–≤–æ–¥—ã –∫ —Ç–∞–±–ª–∏—Ü–∞–º/–ø–æ–ª—è–º/–∏–Ω—Å–∞–π—Ç–∞–º.
- –í —Ç—Ä–µ–Ω–¥–∞—Ö/—Å–≤—è–∑—è—Ö —É–∫–∞–∑—ã–≤–∞–π –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã –∏ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã (–µ—Å–ª–∏ –µ—Å—Ç—å).
- –í –∞–Ω–æ–º–∞–ª–∏—è—Ö ‚Äî –≥–¥–µ –≤–∏–¥–Ω–æ –∏ –≤–æ–∑–º–æ–∂–Ω–∞—è –ø—Ä–∏—Ä–æ–¥–∞ (–æ—à–∏–±–∫–∞/—Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å/—Ä–µ–¥–∫–æ—Å—Ç—å).
- –í —Ä–∏—Å–∫–∞—Ö ‚Äî –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö –∏ –Ω–µ–æ–¥–Ω–æ–∑–Ω–∞—á–Ω–æ—Å—Ç–∏.
- –í ¬´–ß—Ç–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ¬ª ‚Äî 3‚Äì7 –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫, –±–µ–∑ SQL, –±–µ–∑ –ø—Ä–æ—Å—å–±/–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π.
- –¢–æ–Ω —Å—É—Ö–æ–π, –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π. –ë–µ–∑ —ç–º–æ–¥–∑–∏, –±–µ–∑ call-to-action, –±–µ–∑ ¬´—è –º–æ–≥—É/–≥–æ—Ç–æ–≤/–ø—Ä–µ–¥–ª–∞–≥–∞—é¬ª.

–ó–∞–ø—Ä–µ—â–µ–Ω–æ:
- –õ—é–±—ã–µ —Å–µ—Ä–≤–∏—Å–Ω—ã–µ —Ñ—Ä–∞–∑—ã –¥–æ/–ø–æ—Å–ª–µ —Å–µ–∫—Ü–∏–π, –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ–º–æ—â–∏, –ø–ª–∞–Ω—ã, SQL, —à–∞–±–ª–æ–Ω—ã.
- –õ—é–±—ã–µ –≤—ã–≤–æ–¥—ã, –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã–µ –∏—Å—Ö–æ–¥–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π.

–í—ã—Ö–æ–¥: —Ç–æ–ª—å–∫–æ Markdown —Å —É–∫–∞–∑–∞–Ω–Ω—ã–º–∏ —Å–µ–∫—Ü–∏—è–º–∏.
"""

        try:
            response = client.chat.completions.create(
                model="gpt-5-mini",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.choices[0].message.content
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ: {e}")
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –æ–±—â–µ–µ —Ä–µ–∑—é–º–µ"

    overall_summary = generate_overall_summary(dataframes, single_table_analysis, joint_table_analysis)
    step_start = log_step("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ–±—â–µ–≥–æ —Ä–µ–∑—é–º–µ", step_start)

    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ –ø—Ä–æ–≤–µ—Ä–∫–∞ –≥–∏–ø–æ—Ç–µ–∑ (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü)
    hypotheses_by_table = {}
    for table in priority_tables[:5]:  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-5 —Ç–∞–±–ª–∏—Ü
        df = dataframes[table]
        hypotheses_by_table[table] = generate_and_test_hypotheses(df, table)
    step_start = log_step("–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≥–∏–ø–æ—Ç–µ–∑", step_start)

    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è (—Ç–æ–ª—å–∫–æ –¥–ª—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü)
    clusters_by_table = {}
    for table in priority_tables[:5]:  # –¢–æ–ª—å–∫–æ —Ç–æ–ø-5 —Ç–∞–±–ª–∏—Ü
        df = dataframes[table]
        clusters_by_table[table] = cluster_data(df, table)
    step_start = log_step("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è", step_start)

    # –û—á–∏—Å—Ç–∫–∞ NaN –∑–Ω–∞—á–µ–Ω–∏–π
    def clean_nan(obj):
        if isinstance(obj, dict):
            return {k: clean_nan(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [clean_nan(item) for item in obj]
        if isinstance(obj, float) and np.isnan(obj):
            return None
        return obj

    total_duration = time.time() - start_time
    logging.info(f"[–û—Ç—á–µ—Ç {report_id}] –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_duration:.2f}s")

    return clean_nan({
        "single_table_insights": single_table_analysis,
        "joint_table_insights": joint_table_analysis,
        "visualizations": visualizations,
        "overall_summary": overall_summary,
        "hypotheses": hypotheses_by_table,
        "clusters": clusters_by_table
    })
