# services/dataframe_analyzer.py - –ü–û–õ–ù–ê–Ø –í–ï–†–°–ò–Ø —Å SmartGPT

import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import logging
from services.dataframe_manager import DataFrameManager
from datetime import datetime
import re
import sys
import os
from services.gpt_analyzer import SmartGPTAnalyzer

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.json_serializer import convert_to_serializable, clean_dataframe_for_json

logger = logging.getLogger(__name__)


class DataFrameAnalyzer:
    """–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DataFrame —Å –ø–æ–ª–Ω–æ–π SmartGPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""

    def __init__(self, df_manager: DataFrameManager):
        self.df_manager = df_manager
        self.analysis_cache = {}
        self.gpt_analyzer = SmartGPTAnalyzer()

    def _extract_mentioned_tables(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ç–∞–±–ª–∏—Ü—ã"""
        mentioned = []
        question_lower = question.lower()

        for table_name in self.df_manager.tables.keys():
            if table_name.lower() in question_lower:
                mentioned.append(table_name)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
        if not mentioned and self.df_manager.tables:
            largest_table = max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]
            mentioned = [largest_table]

        return mentioned

    def _find_date_columns(self, df: pd.DataFrame) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏"""
        date_cols = []

        for col in df.columns:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'updated', 'timestamp']):
                date_cols.append(col)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö
            elif df[col].dtype == 'datetime64[ns]':
                date_cols.append(col)
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫ –¥–∞—Ç–µ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π
            else:
                try:
                    sample = df[col].dropna().head(5)
                    if len(sample) > 0:
                        pd.to_datetime(sample.iloc[0], errors='raise')
                        date_cols.append(col)
                except:
                    continue

        return date_cols

    def _categorize_question(self, question: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower()

        if any(keyword in question_lower for keyword in ['–æ–±–∑–æ—Ä', '–æ–±—â–∏–π', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 'overview', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞']):
            return 'overview'
        elif any(keyword in question_lower for keyword in ['—Ç–∞–±–ª–∏—Ü–∞', 'table', '–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã']):
            return 'table_analysis'
        elif any(keyword in question_lower for keyword in ['—Å–≤—è–∑—å', '—Å–≤—è–∑–∏', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', 'relation']):
            return 'relationship_analysis'
        elif any(keyword in question_lower for keyword in ['–±–∏–∑–Ω–µ—Å', '–º–µ—Ç—Ä–∏–∫–∏', 'kpi', '–∏–Ω—Å–∞–π—Ç']):
            return 'business_insights'
        elif any(keyword in question_lower for keyword in ['–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è', '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å', '—Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å']):
            return 'correlation'
        elif any(keyword in question_lower for keyword in ['—Ç—Ä–µ–Ω–¥', '–¥–∏–Ω–∞–º–∏–∫–∞', '–≤—Ä–µ–º—è', '–≤—Ä–µ–º–µ–Ω–Ω–æ–π']):
            return 'trend_analysis'
        elif any(keyword in question_lower for keyword in ['–∞–Ω–æ–º–∞–ª–∏–∏', '–≤—ã–±—Ä–æ—Å—ã', 'anomaly', 'outlier']):
            return 'anomalies'
        else:
            return 'general'

    def analyze_question(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ DataFrame –æ–ø–µ—Ä–∞—Ü–∏–π"""
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {question}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞
        analysis_type = self._categorize_question(question)

        try:
            if analysis_type == 'overview':
                return self._analyze_overview()
            elif analysis_type == 'table_analysis':
                table_name = self._extract_table_name(question)
                return self._analyze_single_table(table_name)
            elif analysis_type == 'relationship_analysis':
                return self._analyze_relationships()
            elif analysis_type == 'aggregation':
                return self._analyze_aggregations(question)
            elif analysis_type == 'trend_analysis':
                return self._analyze_trends(question)
            elif analysis_type == 'correlation':
                return self._analyze_correlations(question)
            elif analysis_type == 'comparison':
                return self._analyze_comparison(question)
            elif analysis_type == 'anomalies':
                return self._analyze_anomalies(question)
            elif analysis_type == 'business_insights':
                return self._analyze_business_metrics(question)
            elif analysis_type == 'data_quality':
                return self._analyze_data_quality(question)
            elif analysis_type == 'statistical_insights':
                return self._analyze_statistical_insights(question)
            elif analysis_type == 'predictive_analysis':
                return self._analyze_predictive_patterns(question)
            else:
                return self._analyze_general(question)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–ø—Ä–æ—Å–∞ '{question}': {e}")
            return {
                'question': question,
                'error': str(e),
                'data': [],
                'summary': f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {str(e)}',
                'analyzed_tables': []
            }

    def _extract_table_name(self, question: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower()

        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for table_name in self.df_manager.tables.keys():
            if table_name.lower() in question_lower:
                return table_name

        # –ë–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if self.df_manager.tables:
            return max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]

        return "unknown"

    def _analyze_overview(self) -> Dict[str, Any]:
        """–û–±—â–∏–π –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å GPT –∞–Ω–∞–ª–∏–∑–æ–º"""
        overview_data = []

        for table_name, df in self.df_manager.tables.items():
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
            categorical_cols = len(df.select_dtypes(include=['object']).columns)
            datetime_cols = len(df.select_dtypes(include=['datetime64']).columns)
            null_count = df.isnull().sum().sum()

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            duplicate_rows = df.duplicated().sum()
            unique_rows = len(df.drop_duplicates())

            overview_data.append({
                'table_name': table_name,
                'row_count': int(len(df)),
                'column_count': int(len(df.columns)),
                'numeric_columns': int(numeric_cols),
                'categorical_columns': int(categorical_cols),
                'datetime_columns': int(datetime_cols),
                'null_values': int(null_count),
                'null_percentage': round(float((null_count / (len(df) * len(df.columns))) * 100), 2),
                'duplicate_rows': int(duplicate_rows),
                'unique_rows': int(unique_rows),
                'data_completeness': round(float((1 - null_count / (len(df) * len(df.columns))) * 100), 2),
                'memory_usage_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 2)
            })

        overview_df = pd.DataFrame(overview_data)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–≤—è–∑—è—Ö
        relations_info = []
        for rel in self.df_manager.relations:
            relations_info.append({
                'from_table': rel.from_table,
                'to_table': rel.to_table,
                'from_column': rel.from_column,
                'to_column': rel.to_column,
                'relation_type': rel.relation_type
            })

        total_rows = overview_df['row_count'].sum() if not overview_df.empty else 0
        total_tables = len(self.df_manager.tables)
        relations_count = len(self.df_manager.relations)
        total_memory = overview_df['memory_usage_mb'].sum() if not overview_df.empty else 0
        avg_completeness = overview_df['data_completeness'].mean() if not overview_df.empty else 0

        # GPT –∞–Ω–∞–ª–∏–∑ –æ–±–∑–æ—Ä–∞
        gpt_context = {
            'total_tables': total_tables,
            'total_rows': total_rows,
            'relations_count': relations_count,
            'avg_completeness': avg_completeness,
            'overview_data': overview_data[:3]  # –ü–µ—Ä–≤—ã–µ 3 —Ç–∞–±–ª–∏—Ü—ã –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        }

        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=overview_df,
                table_name='database_overview',
                analysis_type='business_insights',
                context=gpt_context
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –æ–±–∑–æ—Ä–∞: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –æ–±–∑–æ—Ä–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        summary = (f"üè† **–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç {total_tables} —Ç–∞–±–ª–∏—Ü —Å {total_rows:,} –∑–∞–ø–∏—Å—è–º–∏**\n\n"
                   f"üìä –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {relations_count} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏\n"
                   f"üíæ –û–±—â–∏–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏: {total_memory:.2f} MB\n"
                   f"üìà –°—Ä–µ–¥–Ω—è—è –ø–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {avg_completeness:.1f}%\n\n"
                   f"ü§ñ **GPT –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}")

        return {
            'question': '–£–º–Ω—ã–π –æ–±–∑–æ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö',
            'data': clean_dataframe_for_json(overview_df),
            'summary': summary,
            'analyzed_tables': list(self.df_manager.tables.keys()),
            'relations': relations_info,
            'gpt_insights': gpt_insights,
            'overview_stats': {
                'total_tables': total_tables,
                'total_rows': total_rows,
                'relations_count': relations_count,
                'avg_completeness': round(avg_completeness, 1),
                'total_memory_mb': round(total_memory, 2)
            },
            'chart_data': self._prepare_chart_data_safe(overview_df, 'bar', 'table_name', 'row_count')
        }

    def _create_table_not_found_result(self, table_name: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è —Å–ª—É—á–∞—è, –∫–æ–≥–¥–∞ —Ç–∞–±–ª–∏—Ü–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"""
        available_tables = list(self.df_manager.tables.keys())

        return {
            'question': f'–ê–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}',
            'error': f'–¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞',
            'data': [],
            'summary': f'–¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã: {", ".join(available_tables)}',
            'analyzed_tables': [],
            'available_tables': available_tables
        }


    def _analyze_single_table(self, table_name: str) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã —Å –ø–æ–ª–Ω—ã–º GPT –∞–Ω–∞–ª–∏–∑–æ–º"""
        if table_name not in self.df_manager.tables:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
            matching_tables = [t for t in self.df_manager.tables.keys()
                               if table_name.lower() in t.lower()]
            if matching_tables:
                table_name = matching_tables[0]
                logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞ –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é: {table_name}")
            else:
                return self._create_table_not_found_result(table_name)

        df = self.df_manager.tables[table_name]

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        basic_info = {
            'rows': int(len(df)),
            'columns': int(len(df.columns)),
            'memory_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 3),
            'columns_list': list(df.columns)
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_stats = {}
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            desc_stats = df[numeric_cols].describe()
            numeric_stats = convert_to_serializable(desc_stats.to_dict())

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        categorical_stats = self._analyze_categorical_columns(df)

        # –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π
        anomalies = self._detect_anomalies_dataframe(df)

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_analysis = self._analyze_missing_values(df)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        correlations = self._analyze_correlations_single_table(df, table_name)

        # –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô GPT –ê–ù–ê–õ–ò–ó
        logger.info(f"–ó–∞–ø—É—Å–∫ –∫–æ–º–ø–ª–µ–∫—Å–Ω–æ–≥–æ GPT –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}")

        # –ë–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã
        try:
            gpt_business_insights = self.gpt_analyzer.analyze_data_with_gpt(
                df=df,
                table_name=table_name,
                analysis_type="business_insights",
                context={
                    'basic_info': basic_info,
                    'anomalies': anomalies,
                    'correlations': correlations,
                    'missing_analysis': missing_analysis
                }
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑–∞: {e}")
            gpt_business_insights = {'gpt_analysis': '–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        try:
            gpt_data_quality = self.gpt_analyzer.analyze_data_with_gpt(
                df=df,
                table_name=table_name,
                analysis_type="data_quality",
                context={
                    'missing_analysis': missing_analysis,
                    'anomalies': anomalies,
                    'categorical_stats': categorical_stats
                }
            )
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            gpt_data_quality = {'gpt_analysis': '–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'}

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Å–∞–π—Ç—ã
        try:
            gpt_statistical = self.gpt_analyzer.analyze_data_with_gpt(
                df=df,
                table_name=table_name,
                analysis_type="statistical_insights",
                context={
                    'numeric_stats': numeric_stats,
                    'correlations': correlations
                }
            ) if len(numeric_cols) > 0 else {'gpt_analysis': ''}
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            gpt_statistical = {'gpt_analysis': ''}

        # GPT –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        correlation_insights = ""
        if correlations:
            try:
                correlation_insights = self.gpt_analyzer.analyze_correlations_with_context(
                    correlations, df, table_name
                )
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")
                correlation_insights = '–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        # –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—É—é —Å–≤–æ–¥–∫—É
        summary = f"üéØ **–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'**\n\n"
        summary += f"üìä **–ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:** {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫\n"

        if anomalies:
            summary += f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π\n"
        if correlations:
            summary += f"üîó –ù–∞–π–¥–µ–Ω–æ {len(correlations)} –∑–Ω–∞—á–∏–º—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n"
        if missing_analysis['total_missing'] > 0:
            summary += f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {missing_analysis['total_missing']} ({missing_analysis['missing_percentage']:.1f}%)\n"

        summary += f"\nüíº **–ë–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã:**\n{gpt_business_insights.get('gpt_analysis', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')}\n"

        if gpt_data_quality.get('gpt_analysis'):
            summary += f"\nüîç **–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö:**\n{gpt_data_quality.get('gpt_analysis')}\n"

        if gpt_statistical.get('gpt_analysis'):
            summary += f"\nüìà **–°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –Ω–∞—Ö–æ–¥–∫–∏:**\n{gpt_statistical.get('gpt_analysis')}\n"

        if correlation_insights:
            summary += f"\nüîó **–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π:**\n{correlation_insights}"

        return {
            'question': f'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}',
            'data': clean_dataframe_for_json(df.head(10)),
            'summary': summary,
            'analyzed_tables': [table_name],

            # GPT —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            'gpt_business_insights': gpt_business_insights.get('gpt_analysis', ''),
            'gpt_data_quality': gpt_data_quality.get('gpt_analysis', ''),
            'gpt_statistical': gpt_statistical.get('gpt_analysis', ''),
            'correlation_insights': correlation_insights,

            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ
            'basic_info': basic_info,
            'numeric_stats': numeric_stats,
            'categorical_stats': categorical_stats,
            'anomalies': anomalies,
            'correlations': correlations,
            'missing_analysis': missing_analysis,

            'chart_data': self._prepare_chart_data_safe(df, 'histogram', df.columns[0], None)
        }

    def _analyze_business_metrics(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ —Å GPT"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]

        main_table = tables_mentioned[0]
        df = self.df_manager.tables[main_table]

        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏
        business_metrics = self._calculate_business_metrics(df)

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑
        advanced_metrics = self._calculate_advanced_business_metrics(df, main_table)
        business_metrics.update(advanced_metrics)

        # GPT –∞–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫
        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=df,
                table_name=main_table,
                analysis_type="business_insights",
                context={
                    "metrics": business_metrics,
                    "question": question,
                    "focus": "business_performance"
                }
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', 'GPT –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑–∞: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        metrics_df = pd.DataFrame([business_metrics])

        summary = f"üíº **–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{main_table}'**\n\n"
        summary += f"üìä **–ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏:**\n"
        summary += f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π: {business_metrics.get('total_records', 0):,}\n"
        summary += f"‚Ä¢ –ü–æ–ª–Ω–æ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö: {business_metrics.get('data_completeness', 0):.1f}%\n"
        summary += f"‚Ä¢ –£—Ä–æ–≤–µ–Ω—å –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {business_metrics.get('duplicate_rate', 0):.1f}%\n"

        if business_metrics.get('revenue_metrics'):
            summary += f"‚Ä¢ –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –Ω–∞–π–¥–µ–Ω—ã\n"

        summary += f"\nü§ñ **GPT –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"

        return {
            'question': question,
            'data': clean_dataframe_for_json(metrics_df),
            'summary': summary,
            'analyzed_tables': [main_table],
            'gpt_insights': gpt_insights,
            'business_metrics': business_metrics,
            'chart_data': self._prepare_business_metrics_chart(business_metrics)
        }

    def _calculate_business_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏"""
        metrics = {
            'total_records': len(df),
            'data_completeness': round((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 1),
            'duplicate_rate': round((df.duplicated().sum() / len(df)) * 100, 1),
            'unique_entities': {},
            'numeric_summaries': {},
            'revenue_metrics': {},
            'customer_metrics': {}
        }

        # –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            col_lower = col.lower()
            try:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    metrics['numeric_summaries'][col] = {
                        'total': float(col_data.sum()),
                        'average': round(float(col_data.mean()), 2),
                        'median': round(float(col_data.median()), 2),
                        'min': float(col_data.min()),
                        'max': float(col_data.max())
                    }

                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
                    if any(keyword in col_lower for keyword in ['revenue', 'sales', 'amount', 'price', 'cost']):
                        metrics['revenue_metrics'][col] = {
                            'total_revenue': float(col_data.sum()),
                            'avg_transaction': round(float(col_data.mean()), 2),
                            'max_transaction': float(col_data.max()),
                            'transactions_count': len(col_data)
                        }

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            try:
                if df[col].nunique() < 100:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                    metrics['unique_entities'][col] = int(df[col].nunique())

                    # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–ª–∏–µ–Ω—Ç—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
                    col_lower = col.lower()
                    if any(keyword in col_lower for keyword in ['customer', 'user', 'client']):
                        top_customers = df[col].value_counts().head(5)
                        metrics['customer_metrics'][col] = {
                            'unique_customers': int(df[col].nunique()),
                            'total_interactions': len(df),
                            'avg_interactions_per_customer': round(len(df) / df[col].nunique(), 2),
                            'top_customers': convert_to_serializable(top_customers.to_dict())
                        }

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

        return metrics

    def _calculate_advanced_business_metrics(self, df: pd.DataFrame, table_name: str) -> Dict[str, Any]:
        """–†–∞—Å—á–µ—Ç –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫"""
        advanced = {}

        try:
            # –ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            date_cols = self._find_date_columns(df)
            if date_cols:
                date_col = date_cols[0]
                try:
                    df_temp = df.copy()
                    df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    df_clean = df_temp.dropna(subset=[date_col])

                    if len(df_clean) > 1:
                        date_range = df_clean[date_col].max() - df_clean[date_col].min()
                        advanced['time_span_days'] = date_range.days
                        advanced['records_per_day'] = round(len(df_clean) / max(date_range.days, 1), 2)

                        # –ê–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
                        if len(df_clean) > 7:
                            weekday_activity = df_clean[date_col].dt.dayofweek.value_counts().sort_index()
                            advanced['weekday_pattern'] = convert_to_serializable(weekday_activity.to_dict())

                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: {e}")

            # –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–µ–π (–µ—Å–ª–∏ –µ—Å—Ç—å ID –∫–æ–ª–æ–Ω–∫–∏)
            id_cols = [col for col in df.columns if 'id' in col.lower()]
            if id_cols:
                id_col = id_cols[0]
                try:
                    unique_ids = df[id_col].nunique()
                    total_records = len(df)
                    advanced['referential_integrity'] = {
                        'unique_ids': int(unique_ids),
                        'total_records': int(total_records),
                        'integrity_ratio': round(unique_ids / total_records, 3),
                        'potential_duplicates': int(total_records - unique_ids)
                    }
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {e}")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã—Ö –º–µ—Ç—Ä–∏–∫: {e}")

        return advanced

    def _analyze_relationships(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ —Å GPT –∏–Ω—Å–∞–π—Ç–∞–º–∏"""
        if not self.df_manager.relations:
            return {
                'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
                'data': [],
                'summary': 'üîó –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ë–î.',
                'analyzed_tables': [],
                'gpt_insights': '–°–≤—è–∑–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã - –∞–Ω–∞–ª–∏–∑ —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–≤–æ–∑–º–æ–∂–µ–Ω'
            }

        relationship_data = []
        for relation in self.df_manager.relations:
            try:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–∏
                left_df = self.df_manager.tables[relation.from_table]
                right_df = self.df_manager.tables[relation.to_table]

                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–≤—è–∑–∏
                left_values = left_df[relation.from_column].dropna()
                right_values = right_df[relation.to_column].dropna()

                left_unique = set(left_values.astype(str))
                right_unique = set(right_values.astype(str))
                common_values = left_unique.intersection(right_unique)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏
                left_not_in_right = len(left_unique - right_unique)
                right_not_in_left = len(right_unique - left_unique)
                relationship_strength = len(common_values) / max(len(left_unique), 1) * 100

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                cardinality_ratio = len(left_unique) / max(len(right_unique), 1)

                relationship_data.append({
                    'from_table': relation.from_table,
                    'to_table': relation.to_table,
                    'from_column': relation.from_column,
                    'to_column': relation.to_column,
                    'common_values_count': int(len(common_values)),
                    'left_unique_count': int(len(left_unique)),
                    'right_unique_count': int(len(right_unique)),
                    'relationship_strength': round(float(relationship_strength), 2),
                    'integrity_issues': int(left_not_in_right),
                    'reverse_integrity_issues': int(right_not_in_left),
                    'cardinality_ratio': round(float(cardinality_ratio), 3),
                    'relation_type': relation.relation_type,
                    'quality_score': self._calculate_relationship_quality(
                        relationship_strength, left_not_in_right, right_not_in_left
                    )
                })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–∏ {relation.from_table}->{relation.to_table}: {e}")
                relationship_data.append({
                    'from_table': relation.from_table,
                    'to_table': relation.to_table,
                    'from_column': relation.from_column,
                    'to_column': relation.to_column,
                    'error': str(e),
                    'quality_score': 0
                })

        if not relationship_data:
            return {
                'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
                'data': [],
                'summary': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–∏',
                'analyzed_tables': []
            }

        relationships_df = pd.DataFrame(relationship_data)

        # GPT –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π
        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=relationships_df,
                table_name='database_relationships',
                analysis_type='data_quality',
                context={
                    'relationships_count': len(relationship_data),
                    'integrity_issues': sum(r.get('integrity_issues', 0) for r in relationship_data),
                    'avg_strength': relationships_df[
                        'relationship_strength'].mean() if not relationships_df.empty else 0
                }
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–µ–π: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        if not relationships_df.empty:
            strongest = relationships_df.loc[relationships_df['relationship_strength'].idxmax()]
            weakest = relationships_df.loc[relationships_df['relationship_strength'].idxmin()]
            avg_strength = relationships_df['relationship_strength'].mean()
            total_issues = relationships_df['integrity_issues'].sum()

            summary = f"üîó **–ê–Ω–∞–ª–∏–∑ {len(self.df_manager.relations)} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏**\n\n"
            summary += f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:**\n"
            summary += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞ —Å–≤—è–∑–µ–π: {avg_strength:.1f}%\n"
            summary += f"‚Ä¢ –°–∞–º–∞—è —Å–∏–ª—å–Ω–∞—è: {strongest['from_table']} ‚Üí {strongest['to_table']} ({strongest['relationship_strength']:.1f}%)\n"
            summary += f"‚Ä¢ –°–∞–º–∞—è —Å–ª–∞–±–∞—è: {weakest['from_table']} ‚Üí {weakest['to_table']} ({weakest['relationship_strength']:.1f}%)\n"
            summary += f"‚Ä¢ –ü—Ä–æ–±–ª–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏: {total_issues}\n\n"
            summary += f"ü§ñ **GPT –ê–Ω–∞–ª–∏–∑:**\n{gpt_insights}"
        else:
            summary = "–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"

        analyzed_tables = list(set([r['from_table'] for r in relationship_data] +
                                   [r['to_table'] for r in relationship_data]))

        return {
            'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
            'data': clean_dataframe_for_json(relationships_df),
            'summary': summary,
            'analyzed_tables': analyzed_tables,
            'gpt_insights': gpt_insights,
            'relationship_stats': {
                'total_relations': len(relationship_data),
                'avg_strength': round(avg_strength, 1) if not relationships_df.empty else 0,
                'total_integrity_issues': int(total_issues) if not relationships_df.empty else 0
            },
            'chart_data': self._prepare_chart_data_safe(relationships_df, 'bar', 'from_table', 'relationship_strength')
        }

    def _calculate_relationship_quality(self, strength: float, left_issues: int, right_issues: int) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–∏ (0-100)"""
        base_score = strength  # –ù–∞—á–∏–Ω–∞–µ–º —Å —Å–∏–ª—ã —Å–≤—è–∑–∏

        # –°–Ω–∏–∂–∞–µ–º –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        integrity_penalty = (left_issues + right_issues) * 2
        quality_score = max(0, int(base_score - integrity_penalty))

        return round(quality_score, 1)

    def _analyze_aggregations(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏—è–º–∏ –∏ GPT –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            if self.df_manager.tables:
                tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]
            else:
                return {
                    'question': question,
                    'data': [],
                    'summary': '–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                    'analyzed_tables': []
                }

        results = []
        aggregated_data = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ß–∏—Å–ª–æ–≤—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                agg_funcs = ['count', 'sum', 'mean', 'min', 'max', 'std']
                try:
                    agg_result = df[numeric_cols].agg(agg_funcs).round(2)

                    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                    for col in numeric_cols:
                        col_data = {
                            'table': table_name,
                            'column': col,
                            'type': 'numeric'
                        }

                        for func in agg_funcs:
                            try:
                                value = agg_result.loc[func, col]
                                if pd.isna(value) or np.isinf(value):
                                    value = 0
                                col_data[func] = float(value)
                            except:
                                col_data[func] = 0

                        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                        try:
                            col_series = df[col].dropna()
                            if len(col_series) > 0:
                                col_data['median'] = float(col_series.median())
                                col_data['q25'] = float(col_series.quantile(0.25))
                                col_data['q75'] = float(col_series.quantile(0.75))
                                col_data['non_zero_count'] = int((col_series != 0).sum())
                        except:
                            col_data.update({'median': 0, 'q25': 0, 'q75': 0, 'non_zero_count': 0})

                        aggregated_data.append(col_data)

                    results.append({
                        'table': table_name,
                        'type': 'numeric_aggregation',
                        'columns_processed': len(numeric_cols),
                        'data': convert_to_serializable(agg_result)
                    })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á–∏—Å–ª–æ–≤–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –≤ {table_name}: {e}")

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                for col in categorical_cols[:5]:  # –û–≥—Ä–∞–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        if df[col].nunique() < 50:  # –ò–∑–±–µ–≥–∞–µ–º –∫–æ–ª–æ–Ω–æ–∫ —Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–∏–º —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ–º
                            value_counts = df[col].value_counts().head(10)
                            if len(value_counts) > 0:
                                for value, count in value_counts.items():
                                    aggregated_data.append({
                                        'table': table_name,
                                        'column': col,
                                        'type': 'categorical',
                                        'value': str(value),
                                        'count': int(count),
                                        'percentage': round(float(count / len(df) * 100), 2)
                                    })

                                results.append({
                                    'table': table_name,
                                    'type': 'categorical_aggregation',
                                    'column': col,
                                    'unique_values': int(df[col].nunique()),
                                    'top_values': convert_to_serializable(value_counts.head(5).to_dict())
                                })
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ {col} –≤ {table_name}: {e}")

        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame
        if aggregated_data:
            main_result = pd.DataFrame(aggregated_data)

            # GPT –∞–Ω–∞–ª–∏–∑ –∞–≥—Ä–µ–≥–∞—Ü–∏–π
            try:
                gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                    df=main_result,
                    table_name='aggregation_results',
                    analysis_type='statistical_insights',
                    context={
                        'question': question,
                        'tables_analyzed': tables_mentioned,
                        'aggregation_results': results[:3]  # –ü–µ—Ä–≤—ã–µ 3 —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    }
                )
                gpt_insights = gpt_analysis.get('gpt_analysis', '')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–π: {e}")
                gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –∞–≥—Ä–µ–≥–∞—Ü–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

            summary = f"üìä **–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü**\n\n"
            summary += f"üìà –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len([r for r in results if r['type'] == 'numeric_aggregation'])} —á–∏—Å–ª–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫\n"
            summary += f"üìã –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len([r for r in results if r['type'] == 'categorical_aggregation'])} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n\n"
            summary += f"ü§ñ **GPT –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"
        else:
            main_result = pd.DataFrame()
            summary = "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö"
            gpt_insights = ''

        return {
            'question': question,
            'data': clean_dataframe_for_json(main_result),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'detailed_results': convert_to_serializable(results),
            'aggregation_stats': {
                'tables_processed': len(tables_mentioned),
                'total_aggregations': len(aggregated_data),
                'numeric_aggregations': len([d for d in aggregated_data if d['type'] == 'numeric']),
                'categorical_aggregations': len([d for d in aggregated_data if d['type'] == 'categorical'])
            },
            'chart_data': self._create_aggregation_chart(main_result)
        }

    def _create_aggregation_chart(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏"""
        try:
            if df.empty:
                return None

            # –ò—â–µ–º –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
            if 'type' in df.columns and 'count' in df.columns:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø—É
                grouped = df.groupby('type')['count'].sum().reset_index()
                return self._prepare_chart_data_safe(grouped, 'bar', 'type', 'count')

            elif 'table' in df.columns and 'count' in df.columns:
                # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
                grouped = df.groupby('table')['count'].sum().reset_index()
                return self._prepare_chart_data_safe(grouped, 'bar', 'table', 'count')

            elif len(df.columns) >= 2:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—ã–µ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
                return self._prepare_chart_data_safe(df, 'bar', df.columns[0], df.columns[1])

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è aggregation chart: {e}")
            return None

    def _analyze_trends(self, question: str) -> Dict[str, Any]:
        """–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ —Å GPT –ø—Ä–æ–≥–Ω–æ–∑–∞–º–∏"""
        trend_results = []

        for table_name, df in self.df_manager.tables.items():
            # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏
            date_cols = self._find_date_columns(df)
            if date_cols:
                for date_col in date_cols[:2]:  # –ú–∞–∫—Å–∏–º—É–º 2 –¥–∞—Ç—ã –Ω–∞ —Ç–∞–±–ª–∏—Ü—É
                    try:
                        df_copy = df.copy()
                        df_copy[date_col] = pd.to_datetime(df_copy[date_col], errors='coerce')

                        # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                        df_clean = df_copy.dropna(subset=[date_col])

                        if len(df_clean) > 5:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç—Ä–µ–Ω–¥–∞
                            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
                            period_type = self._determine_period_type(df_clean[date_col])
                            df_clean['period'] = df_clean[date_col].dt.to_period(period_type)

                            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
                            trend_data = df_clean.groupby('period').agg({
                                date_col: 'count'
                            }).rename(columns={date_col: 'count'}).reset_index()
                            trend_data['period_str'] = trend_data['period'].astype(str)

                            # –î–æ–±–∞–≤–ª—è–µ–º –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
                            numeric_cols = df_clean.select_dtypes(include=[np.number]).columns
                            if len(numeric_cols) > 0:
                                numeric_trends = df_clean.groupby('period')[numeric_cols].agg(['sum', 'mean']).round(2)
                                # –î–æ–±–∞–≤–ª—è–µ–º –∫ trend_data
                                for col in numeric_cols[:3]:  # –ü–µ—Ä–≤—ã–µ 3 —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
                                    try:
                                        trend_data[f'{col}_sum'] = numeric_trends[(col, 'sum')].values
                                        trend_data[f'{col}_avg'] = numeric_trends[(col, 'mean')].values
                                    except:
                                        continue

                            # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
                            if len(trend_data) > 1:
                                first_val = int(trend_data['count'].iloc[0])
                                last_val = int(trend_data['count'].iloc[-1])
                                trend_direction = "—Ä–æ—Å—Ç" if last_val > first_val else "–ø–∞–¥–µ–Ω–∏–µ"
                                trend_percent = abs((last_val - first_val) / first_val * 100) if first_val > 0 else 0

                                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                                volatility = trend_data['count'].std() / trend_data['count'].mean() * 100 if trend_data[
                                                                                                                 'count'].mean() > 0 else 0
                                peak_period = trend_data.loc[trend_data['count'].idxmax(), 'period_str']
                                min_period = trend_data.loc[trend_data['count'].idxmin(), 'period_str']
                            else:
                                trend_direction = "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"
                                trend_percent = 0
                                volatility = 0
                                peak_period = trend_data['period_str'].iloc[0] if len(trend_data) > 0 else 'N/A'
                                min_period = peak_period

                            # –í—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω
                            min_date = df_clean[date_col].min()
                            max_date = df_clean[date_col].max()
                            try:
                                date_range = f"{min_date.strftime('%Y-%m-%d')} - {max_date.strftime('%Y-%m-%d')}"
                                duration_days = (max_date - min_date).days
                            except:
                                date_range = f"{str(min_date)[:10]} - {str(max_date)[:10]}"
                                duration_days = 0

                            trend_results.append({
                                'table': table_name,
                                'date_column': date_col,
                                'trend_data': clean_dataframe_for_json(trend_data),
                                'total_records': int(len(df_clean)),
                                'periods_count': int(len(trend_data)),
                                'trend_direction': trend_direction,
                                'trend_percent': round(float(trend_percent), 1),
                                'volatility': round(float(volatility), 1),
                                'peak_period': peak_period,
                                'low_period': min_period,
                                'date_range': date_range,
                                'duration_days': int(duration_days),
                                'period_type': period_type,
                                'avg_records_per_period': round(len(df_clean) / len(trend_data), 1)
                            })

                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è {table_name}.{date_col}: {e}")

        if trend_results:
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥
            main_trend = max(trend_results, key=lambda x: x['total_records'])

            # GPT –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤
            try:
                gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                    df=pd.DataFrame(trend_results),
                    table_name='trend_analysis',
                    analysis_type='predictive_analysis',
                    context={
                        'question': question,
                        'main_trend': main_trend,
                        'total_trends': len(trend_results)
                    }
                )
                gpt_insights = gpt_analysis.get('gpt_analysis', '')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤: {e}")
                gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

            summary = f"üìà **–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω –≤ {len(trend_results)} —Ç–∞–±–ª–∏—Ü–∞—Ö**\n\n"
            summary += f"üéØ **–û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–¥** (—Ç–∞–±–ª–∏—Ü–∞ '{main_trend['table']}'):\n"
            summary += f"‚Ä¢ –ü–µ—Ä–∏–æ–¥: {main_trend['date_range']}\n"
            summary += f"‚Ä¢ –ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ: {main_trend['trend_direction']} –Ω–∞ {main_trend['trend_percent']:.1f}%\n"
            summary += f"‚Ä¢ –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {main_trend['volatility']:.1f}%\n"
            summary += f"‚Ä¢ –ü–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {main_trend['peak_period']}\n"
            summary += f"‚Ä¢ –ú–∏–Ω–∏–º—É–º –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏: {main_trend['low_period']}\n\n"
            summary += f"ü§ñ **GPT –ü—Ä–æ–≥–Ω–æ–∑—ã –∏ –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"

            main_data = main_trend['trend_data']
        else:
            main_data = []
            summary = "üìÖ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            gpt_insights = ''

        analyzed_tables = [r['table'] for r in trend_results]

        return {
            'question': question,
            'data': main_data,
            'summary': summary,
            'analyzed_tables': analyzed_tables,
            'gpt_insights': gpt_insights,
            'all_trends': trend_results,
            'trend_stats': {
                'tables_with_trends': len(trend_results),
                'total_periods_analyzed': sum(t['periods_count'] for t in trend_results),
                'avg_trend_strength': round(np.mean([abs(t['trend_percent']) for t in trend_results]), 1) if trend_results else 0
            },
            'chart_data': self._prepare_chart_data_safe_from_list(main_data, 'line', 'period_str',
                                                                  'count') if main_data else None
        }

    def _prepare_chart_data_safe_from_list(self, data_list: List[Dict], chart_type: str,
                                           x_key: str, y_key: str) -> Optional[Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤ –∏–∑ —Å–ø–∏—Å–∫–∞ —Å–ª–æ–≤–∞—Ä–µ–π"""
        try:
            if not data_list or not isinstance(data_list, list):
                return None

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–π –≤ –ø–µ—Ä–≤–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ
            if not data_list or x_key not in data_list[0] or y_key not in data_list[0]:
                return None

            chart_data = {
                'chart_type': chart_type,
                'x_column': x_key,
                'y_column': y_key,
                'data': {
                    'labels': [str(item.get(x_key, '')) for item in data_list[:50]],
                    'values': [float(item.get(y_key, 0)) for item in data_list[:50]]
                }
            }

            return chart_data

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ chart_data –∏–∑ —Å–ø–∏—Å–∫–∞: {e}")
            return None

    def _determine_period_type(self, date_series: pd.Series) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Ç–∏–ø –ø–µ—Ä–∏–æ–¥–∞ –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        date_range = date_series.max() - date_series.min()

        if date_range.days <= 31:
            return 'D'  # –î–Ω–∏
        elif date_range.days <= 365:
            return 'W'  # –ù–µ–¥–µ–ª–∏
        elif date_range.days <= 365 * 3:
            return 'M'  # –ú–µ—Å—è—Ü—ã
        else:
            return 'Y'  # –ì–æ–¥—ã

    def _analyze_correlations(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å GPT –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        all_correlations = []
        correlation_matrices = {}

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]
            correlations = self._analyze_correlations_single_table(df, table_name)
            all_correlations.extend(correlations)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Ç—Ä–∏—Ü—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –¥–ª—è GPT –∞–Ω–∞–ª–∏–∑–∞
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) >= 2:
                try:
                    corr_matrix = df[numeric_cols].corr()
                    correlation_matrices[table_name] = convert_to_serializable(corr_matrix.to_dict())
                except:
                    pass

        if all_correlations:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º–∏
            corr_df = pd.DataFrame(all_correlations)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            corr_df['abs_correlation'] = abs(corr_df['correlation'])
            corr_df = corr_df.sort_values('abs_correlation', ascending=False)
            corr_df = corr_df.drop('abs_correlation', axis=1)

            # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            strong_correlations = corr_df[abs(corr_df['correlation']) > 0.7]
            moderate_correlations = corr_df[(abs(corr_df['correlation']) > 0.4) & (abs(corr_df['correlation']) <= 0.7)]
            weak_correlations = corr_df[abs(corr_df['correlation']) <= 0.4]

            # GPT –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
            try:
                gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                    df=corr_df,
                    table_name='correlation_analysis',
                    analysis_type='statistical_insights',
                    context={
                        'question': question,
                        'strong_count': len(strong_correlations),
                        'moderate_count': len(moderate_correlations),
                        'correlation_matrices': correlation_matrices,
                        'top_correlations': all_correlations[:5]
                    }
                )
                gpt_insights = gpt_analysis.get('gpt_analysis', '')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")
                gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

            summary = f"üîó **–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü–∞—Ö**\n\n"
            summary += f"üìä **–ù–∞–π–¥–µ–Ω–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π:**\n"
            summary += f"‚Ä¢ –°–∏–ª—å–Ω—ã—Ö (>0.7): {len(strong_correlations)}\n"
            summary += f"‚Ä¢ –£–º–µ—Ä–µ–Ω–Ω—ã—Ö (0.4-0.7): {len(moderate_correlations)}\n"
            summary += f"‚Ä¢ –°–ª–∞–±—ã—Ö (<0.4): {len(weak_correlations)}\n\n"

            if len(strong_correlations) > 0:
                top_correlation = strong_correlations.iloc[0]
                summary += f"üéØ **–°–∞–º–∞—è —Å–∏–ª—å–Ω–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è:** {top_correlation['column1']} ‚Üî {top_correlation['column2']} "
                summary += f"({top_correlation['correlation']:.3f}) –≤ —Ç–∞–±–ª–∏—Ü–µ {top_correlation['table']}\n\n"

            summary += f"ü§ñ **GPT –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"
        else:
            corr_df = pd.DataFrame()
            summary = "üîó –ó–Ω–∞—á–∏–º—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"
            gpt_insights = ''

        return {
            'question': question,
            'data': clean_dataframe_for_json(corr_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'correlation_stats': {
                'total_correlations': len(all_correlations),
                'strong_correlations': len(strong_correlations) if all_correlations else 0,
                'moderate_correlations': len(moderate_correlations) if all_correlations else 0,
                'tables_analyzed': len(tables_mentioned)
            },
            'correlation_matrices': correlation_matrices,
            'chart_data': self._prepare_chart_data_safe(corr_df, 'scatter', 'column1',
                                                        'correlation') if not corr_df.empty else None
        }

    def _compare_columns_in_table(self, table_name: str, question: str) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        df = self.df_manager.tables[table_name]

        # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) < 2:
            return {
                'question': question,
                'data': [],
                'summary': f'–í —Ç–∞–±–ª–∏—Ü–µ {table_name} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è',
                'analyzed_tables': [table_name]
            }

        comparison_data = []

        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        for col in numeric_cols[:5]:
            try:
                col_data = df[col].dropna()
                if len(col_data) > 0:
                    comparison_data.append({
                        'column': col,
                        'mean': round(float(col_data.mean()), 2),
                        'median': round(float(col_data.median()), 2),
                        'std': round(float(col_data.std()), 2),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'range': float(col_data.max() - col_data.min()),
                        'count': int(len(col_data))
                    })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

        if comparison_data:
            comparison_df = pd.DataFrame(comparison_data)

            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
            max_mean_col = comparison_df.loc[comparison_df['mean'].idxmax(), 'column']
            min_mean_col = comparison_df.loc[comparison_df['mean'].idxmin(), 'column']

            summary = f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ '{table_name}'. "
            summary += f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {max_mean_col}. "
            summary += f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {min_mean_col}."
        else:
            comparison_df = pd.DataFrame()
            summary = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏"

        return {
            'question': question,
            'data': clean_dataframe_for_json(comparison_df),
            'summary': summary,
            'analyzed_tables': [table_name],
            'chart_data': self._prepare_chart_data_safe(comparison_df, 'bar', 'column', 'mean')
        }


    def _analyze_comparison(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ —Å GPT –≤—ã–≤–æ–¥–∞–º–∏"""
        tables_mentioned = self._extract_mentioned_tables(question)

        if len(tables_mentioned) < 2:
            if len(tables_mentioned) == 1:
                return self._compare_columns_in_table(tables_mentioned[0], question)
            else:
                if len(self.df_manager.tables) >= 2:
                    largest_tables = sorted(self.df_manager.tables.items(),
                                            key=lambda x: len(x[1]), reverse=True)[:2]
                    tables_mentioned = [t[0] for t in largest_tables]
                else:
                    return {
                        'question': question,
                        'data': [],
                        'summary': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∞–±–ª–∏—Ü –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è',
                        'analyzed_tables': []
                    }

        comparison_data = []
        detailed_comparison = {}

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            categorical_cols = df.select_dtypes(include=['object']).columns

            stats = {
                'table': table_name,
                'rows': int(len(df)),
                'columns': int(len(df.columns)),
                'numeric_columns': int(len(numeric_cols)),
                'categorical_columns': int(len(categorical_cols)),
                'null_values': int(df.isnull().sum().sum()),
                'null_percentage': round(float(df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100), 2),
                'duplicate_rows': int(df.duplicated().sum()),
                'memory_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 2)
            }

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
            if len(numeric_cols) > 0:
                try:
                    numeric_desc = df[numeric_cols].describe()
                    stats.update({
                        'avg_mean': round(float(numeric_desc.loc['mean'].mean()), 2),
                        'avg_std': round(float(numeric_desc.loc['std'].mean()), 2),
                        'avg_min': round(float(numeric_desc.loc['min'].mean()), 2),
                        'avg_max': round(float(numeric_desc.loc['max'].mean()), 2)
                    })
                except:
                    stats.update({'avg_mean': 0, 'avg_std': 0, 'avg_min': 0, 'avg_max': 0})

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
            stats.update({
                'data_density': round(float(len(df.drop_duplicates()) / len(df) * 100), 2) if len(df) > 0 else 0,
                'avg_column_completeness': round(float((1 - df.isnull().mean().mean()) * 100), 2),
                'schema_complexity': len(df.columns) * len(numeric_cols) / max(len(df.columns), 1)
            })

            comparison_data.append(stats)
            detailed_comparison[table_name] = stats

        comparison_df = pd.DataFrame(comparison_data)

        # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π
        differences_analysis = {}
        if len(comparison_data) >= 2:
            for i in range(len(comparison_data)):
                for j in range(i + 1, len(comparison_data)):
                    table1, table2 = comparison_data[i], comparison_data[j]
                    pair_key = f"{table1['table']}_vs_{table2['table']}"

                    differences_analysis[pair_key] = {
                        'size_ratio': table1['rows'] / max(table2['rows'], 1),
                        'complexity_diff': abs(table1['columns'] - table2['columns']),
                        'quality_diff': abs(table1['avg_column_completeness'] - table2['avg_column_completeness']),
                        'memory_ratio': table1['memory_mb'] / max(table2['memory_mb'], 1)
                    }

        # GPT –∞–Ω–∞–ª–∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=comparison_df,
                table_name='table_comparison',
                analysis_type='business_insights',
                context={
                    'question': question,
                    'tables_compared': tables_mentioned,
                    'differences_analysis': differences_analysis,
                    'detailed_stats': detailed_comparison
                }
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        # –°–æ–∑–¥–∞–Ω–∏–µ —Å–≤–æ–¥–∫–∏
        if len(comparison_data) >= 2:
            summary = f"‚öñÔ∏è **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü: {' vs '.join(tables_mentioned)}**\n\n"

            # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è
            table1, table2 = comparison_data[0], comparison_data[1]
            summary += f"üìä **–ö–ª—é—á–µ–≤—ã–µ —Ä–∞–∑–ª–∏—á–∏—è:**\n"
            summary += f"‚Ä¢ –†–∞–∑–º–µ—Ä: {table1['table']} ({table1['rows']:,} —Å—Ç—Ä–æ–∫) vs {table2['table']} ({table2['rows']:,} —Å—Ç—Ä–æ–∫)\n"
            summary += f"‚Ä¢ –°–ª–æ–∂–Ω–æ—Å—Ç—å: {table1['columns']} vs {table2['columns']} –∫–æ–ª–æ–Ω–æ–∫\n"
            summary += f"‚Ä¢ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {table1['avg_column_completeness']:.1f}% vs {table2['avg_column_completeness']:.1f}%\n"
            summary += f"‚Ä¢ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {table1['memory_mb']:.1f} vs {table2['memory_mb']:.1f} MB\n\n"
            summary += f"ü§ñ **GPT –ê–Ω–∞–ª–∏–∑:**\n{gpt_insights}"
        else:
            summary = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"

        return {
            'question': question,
            'data': clean_dataframe_for_json(comparison_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'detailed_comparison': detailed_comparison,
            'differences_analysis': differences_analysis,
            'chart_data': self._prepare_chart_data_safe(comparison_df, 'bar', 'table', 'rows')
        }

    def _analyze_anomalies(self, question: str) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π —Å GPT –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–æ–π"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        all_anomalies = []
        anomaly_details = {}

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]
            anomalies = self._detect_anomalies_dataframe(df)

            for anomaly in anomalies:
                anomaly['table'] = table_name
                all_anomalies.append(anomaly)

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
            additional_anomalies = self._detect_advanced_anomalies(df, table_name)
            all_anomalies.extend(additional_anomalies)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è GPT
            anomaly_details[table_name] = {
                'total_anomalies': len(anomalies) + len(additional_anomalies),
                'anomaly_types': list(set([a['type'] for a in anomalies + additional_anomalies])),
                'severity': self._calculate_anomaly_severity(anomalies + additional_anomalies, df)
            }

        if all_anomalies:
            anomalies_df = pd.DataFrame(all_anomalies)
            total_anomalies = sum(a.get('count', 1) for a in all_anomalies)

            # –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–∏–ø–∞–º –∞–Ω–æ–º–∞–ª–∏–π
            anomaly_types = {}
            for anomaly in all_anomalies:
                anom_type = anomaly.get('type', 'outlier')
                if anom_type not in anomaly_types:
                    anomaly_types[anom_type] = []
                anomaly_types[anom_type].append(anomaly)

            # GPT –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π
            try:
                gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                    df=anomalies_df,
                    table_name='anomaly_analysis',
                    analysis_type='data_quality',
                    context={
                        'question': question,
                        'anomaly_details': anomaly_details,
                        'total_anomalies': total_anomalies,
                        'anomaly_types': anomaly_types,
                        'tables_with_issues': len([t for t in anomaly_details.values() if t['total_anomalies'] > 0])
                    }
                )
                gpt_insights = gpt_analysis.get('gpt_analysis', '')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∞–Ω–æ–º–∞–ª–∏–π: {e}")
                gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

            # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∞—Ü–∏—è –∞–Ω–æ–º–∞–ª–∏–π
            high_priority = [a for a in all_anomalies if a.get('severity', 'medium') == 'high']
            medium_priority = [a for a in all_anomalies if a.get('severity', 'medium') == 'medium']

            summary = f"üö® **–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(all_anomalies)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –≤ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü–∞—Ö**\n\n"
            summary += f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π:**\n"
            summary += f"‚Ä¢ –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {len(high_priority)}\n"
            summary += f"‚Ä¢ –°—Ä–µ–¥–Ω–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {len(medium_priority)}\n"
            summary += f"‚Ä¢ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {total_anomalies:,}\n\n"

            if anomaly_types:
                summary += f"üîç **–¢–∏–ø—ã –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π:**\n"
                for anom_type, items in anomaly_types.items():
                    summary += f"‚Ä¢ {anom_type}: {len(items)} —Å–ª—É—á–∞–µ–≤\n"
                summary += "\n"

            summary += f"ü§ñ **GPT –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞:**\n{gpt_insights}"
        else:
            anomalies_df = pd.DataFrame()
            summary = "‚úÖ –ê–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã - –¥–∞–Ω–Ω—ã–µ –≤—ã–≥–ª—è–¥—è—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏"
            gpt_insights = ''

        return {
            'question': question,
            'data': clean_dataframe_for_json(anomalies_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'anomaly_details': anomaly_details,
            'anomaly_stats': {
                'total_anomaly_types': len(all_anomalies),
                'total_anomaly_records': sum(a.get('count', 1) for a in all_anomalies),
                'tables_with_anomalies': len([t for t in anomaly_details.values() if t['total_anomalies'] > 0]),
                'high_priority_count': len([a for a in all_anomalies if a.get('severity', 'medium') == 'high'])
            },
            'chart_data': self._prepare_chart_data_safe(anomalies_df, 'bar', 'table',
                                                        'count') if not anomalies_df.empty else None
        }

    def _detect_advanced_anomalies(self, df: pd.DataFrame, table_name: str) -> List[Dict[str, Any]]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π"""
        anomalies = []

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            for col in df.columns:
                if df[col].dtype in ['object']:
                    value_counts = df[col].value_counts()
                    if len(value_counts) > 0:
                        top_value_freq = value_counts.iloc[0]
                        if top_value_freq / len(df) > 0.9:  # 90% –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                            anomalies.append({
                                'type': 'excessive_repetition',
                                'column': col,
                                'count': int(top_value_freq),
                                'percentage': round(float(top_value_freq / len(df) * 100), 2),
                                'description': f'–ó–Ω–∞—á–µ–Ω–∏–µ "{value_counts.index[0]}" –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {top_value_freq} –∏–∑ {len(df)} –∑–∞–ø–∏—Å–µ–π',
                                'severity': 'medium'
                            })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if df[col].count() > 10:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –Ω—É–ª–µ–π
                    zero_count = (df[col] == 0).sum()
                    if zero_count / len(df) > 0.5:  # –ë–æ–ª—å—à–µ 50% –Ω—É–ª–µ–π
                        anomalies.append({
                            'type': 'excessive_zeros',
                            'column': col,
                            'count': int(zero_count),
                            'percentage': round(float(zero_count / len(df) * 100), 2),
                            'description': f'–ö–æ–ª–æ–Ω–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç {zero_count} –Ω—É–ª–µ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –∏–∑ {len(df)}',
                            'severity': 'low'
                        })

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –∫—Ä—É–≥–ª—ã–µ —á–∏—Å–ª–∞
                    if df[col].dtype in ['int64', 'float64']:
                        round_numbers = df[col][df[col] % 10 == 0]
                        if len(round_numbers) / len(df[col].dropna()) > 0.8:  # 80% –∫—Ä—É–≥–ª—ã—Ö —á–∏—Å–µ–ª
                            anomalies.append({
                                'type': 'excessive_round_numbers',
                                'column': col,
                                'count': len(round_numbers),
                                'percentage': round(float(len(round_numbers) / len(df[col].dropna()) * 100), 2),
                                'description': f'–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ –∫—Ä—É–≥–ª—ã—Ö —á–∏—Å–µ–ª –≤ –∫–æ–ª–æ–Ω–∫–µ',
                                'severity': 'low'
                            })

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å—Ç—Ä–∞–Ω–Ω—ã–µ –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            date_cols = self._find_date_columns(df)
            for date_col in date_cols:
                try:
                    df_temp = df.copy()
                    df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    df_dates = df_temp.dropna(subset=[date_col])

                    if len(df_dates) > 10:
                        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Å–µ –∑–∞–ø–∏—Å–∏ –≤ –æ–¥–∏–Ω –¥–µ–Ω—å
                        dates_only = df_dates[date_col].dt.date
                        if dates_only.nunique() == 1:
                            anomalies.append({
                                'type': 'single_date_anomaly',
                                'column': date_col,
                                'count': len(df_dates),
                                'description': f'–í—Å–µ {len(df_dates)} –∑–∞–ø–∏—Å–µ–π –∏–º–µ—é—Ç –æ–¥–∏–Ω–∞–∫–æ–≤—É—é –¥–∞—Ç—É: {dates_only.iloc[0]}',
                                'severity': 'medium'
                            })
                except:
                    continue

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∞–Ω–æ–º–∞–ª–∏–π –≤ {table_name}: {e}")

        return anomalies

    def _calculate_anomaly_severity(self, anomalies: List[Dict], df: pd.DataFrame) -> str:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –æ–±—â—É—é —Å–µ—Ä—å–µ–∑–Ω–æ—Å—Ç—å –∞–Ω–æ–º–∞–ª–∏–π"""
        if not anomalies:
            return 'none'

        high_count = len([a for a in anomalies if a.get('severity') == 'high'])
        medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])

        if high_count > 0:
            return 'high'
        elif medium_count > 2:
            return 'high'
        elif medium_count > 0:
            return 'medium'
        else:
            return 'low'

    def _prepare_chart_data_safe(self, df: pd.DataFrame, chart_type: str,
                                 x_col: str, y_col: Optional[str]) -> Optional[Dict]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–æ–≤"""
        try:
            if df.empty or x_col not in df.columns:
                return None

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            df_limited = df.head(50)

            chart_data = {
                'chart_type': chart_type,
                'x_column': x_col,
                'y_column': y_col
            }

            if chart_type == 'histogram':
                if pd.api.types.is_numeric_dtype(df_limited[x_col]):
                    chart_data['data'] = {'values': df_limited[x_col].dropna().astype(float).tolist()}
                else:
                    value_counts = df_limited[x_col].value_counts().head(10)
                    chart_data['data'] = {
                        'labels': value_counts.index.astype(str).tolist(),
                        'values': value_counts.values.tolist()
                    }

            elif chart_type == 'bar' and y_col and y_col in df_limited.columns:
                chart_data['data'] = {
                    'labels': df_limited[x_col].astype(str).tolist(),
                    'values': df_limited[y_col].astype(float).tolist()
                }

            elif chart_type == 'scatter' and y_col and y_col in df_limited.columns:
                chart_data['data'] = {
                    'points': [
                        {'x': float(x), 'y': float(y)}
                        for x, y in zip(df_limited[x_col].astype(float), df_limited[y_col].astype(float))
                        if pd.notna(x) and pd.notna(y)
                    ]
                }

            elif chart_type == 'line' and y_col and y_col in df_limited.columns:
                chart_data['data'] = {
                    'labels': df_limited[x_col].astype(str).tolist(),
                    'values': df_limited[y_col].astype(float).tolist()
                }

            return chart_data

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ chart_data: {e}")
            return None

    def _analyze_data_quality(self, question: str) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        quality_results = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            total_cells = len(df) * len(df.columns)
            null_cells = df.isnull().sum().sum()
            duplicate_rows = df.duplicated().sum()

            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞
            completeness = (1 - null_cells / total_cells) * 100
            uniqueness = (1 - duplicate_rows / len(df)) * 100
            consistency = self._calculate_consistency_score(df)
            validity = self._calculate_validity_score(df)

            # –û–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞
            quality_score = (completeness * 0.3 + uniqueness * 0.2 +
                             consistency * 0.25 + validity * 0.25)

            quality_results.append({
                'table': table_name,
                'quality_score': round(quality_score, 1),
                'completeness': round(completeness, 1),
                'uniqueness': round(uniqueness, 1),
                'consistency': round(consistency, 1),
                'validity': round(validity, 1),
                'total_records': len(df),
                'null_values': int(null_cells),
                'duplicate_records': int(duplicate_rows)
            })

        quality_df = pd.DataFrame(quality_results)

        # GPT –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö
        try:
            avg_quality = quality_df['quality_score'].mean() if not quality_df.empty else 0
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=quality_df,
                table_name='data_quality_analysis',
                analysis_type='data_quality',
                context={
                    'question': question,
                    'avg_quality': avg_quality,
                    'quality_results': quality_results
                }
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        if not quality_df.empty:
            avg_quality = quality_df['quality_score'].mean()
            summary = f"üîç **–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª {avg_quality:.1f}/100**\n\n"

            best_table = quality_df.loc[quality_df['quality_score'].idxmax()]
            worst_table = quality_df.loc[quality_df['quality_score'].idxmin()]

            summary += f"‚úÖ **–õ—É—á—à–∞—è —Ç–∞–±–ª–∏—Ü–∞:** {best_table['table']} ({best_table['quality_score']:.1f}/100)\n"
            summary += f"‚ö†Ô∏è **–¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è:** {worst_table['table']} ({worst_table['quality_score']:.1f}/100)\n\n"
            summary += f"ü§ñ **GPT –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:**\n{gpt_insights}"
        else:
            summary = "–î–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        return {
            'question': question,
            'data': clean_dataframe_for_json(quality_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'quality_stats': {
                'avg_quality_score': round(quality_df['quality_score'].mean(), 1) if not quality_df.empty else 0,
                'tables_analyzed': len(quality_results),
                'high_quality_tables': len(
                    quality_df[quality_df['quality_score'] >= 80]) if not quality_df.empty else 0,
                'low_quality_tables': len(quality_df[quality_df['quality_score'] < 60]) if not quality_df.empty else 0
            },
            'chart_data': self._prepare_chart_data_safe(quality_df, 'bar', 'table', 'quality_score')
        }

    def _calculate_consistency_score(self, df: pd.DataFrame) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –±–∞–ª–ª –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        consistency_score = 100.0

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ñ–æ—Ä–º–∞—Ç–æ–≤ –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–∫–∞—Ö
            for col in df.select_dtypes(include=['object']).columns:
                unique_vals = df[col].dropna().unique()
                if len(unique_vals) > 5:  # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–º–µ—à–∞–Ω–Ω—ã–µ —Ä–µ–≥–∏—Å—Ç—Ä—ã
                    mixed_case = sum(1 for val in unique_vals
                                     if isinstance(val, str) and val != val.lower() and val != val.upper())
                    if mixed_case > len(unique_vals) * 0.1:
                        consistency_score -= 5

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç–∏: {e}")
            consistency_score = 90  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ

        return max(0, consistency_score)

    def _calculate_validity_score(self, df: pd.DataFrame) -> float:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –±–∞–ª–ª –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö"""
        validity_score = 100.0

        try:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–¥–µ –æ–Ω–∏ –Ω–µ –æ–∂–∏–¥–∞—é—Ç—Å—è
            for col in df.select_dtypes(include=[np.number]).columns:
                if any(keyword in col.lower() for keyword in ['age', 'count', 'quantity', 'price']):
                    negative_count = (df[col] < 0).sum()
                    if negative_count > 0:
                        validity_score -= (negative_count / len(df)) * 20

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            for col in df.select_dtypes(include=[np.number]).columns:
                inf_count = np.isinf(df[col]).sum()
                if inf_count > 0:
                    validity_score -= (inf_count / len(df)) * 30

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏: {e}")
            validity_score = 95  # –î–µ—Ñ–æ–ª—Ç–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –ø—Ä–∏ –æ—à–∏–±–∫–µ

        return max(0, validity_score)

    def _analyze_statistical_insights(self, question: str) -> Dict[str, Any]:
        """–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å GPT –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]

        main_table = tables_mentioned[0]
        df = self.df_manager.tables[main_table]

        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return {
                'question': question,
                'data': [],
                'summary': '–í –¥–∞–Ω–Ω—ã—Ö –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞',
                'analyzed_tables': [main_table]
            }

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        statistical_results = []

        for col in numeric_cols:
            col_data = df[col].dropna()
            if len(col_data) > 5:
                try:
                    # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    basic_stats = {
                        'column': col,
                        'count': len(col_data),
                        'mean': float(col_data.mean()),
                        'median': float(col_data.median()),
                        'std': float(col_data.std()),
                        'min': float(col_data.min()),
                        'max': float(col_data.max()),
                        'range': float(col_data.max() - col_data.min())
                    }

                    # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    basic_stats.update({
                        'skewness': float(col_data.skew()),
                        'kurtosis': float(col_data.kurtosis()),
                        'coefficient_variation': float(col_data.std() / col_data.mean()) if col_data.mean() != 0 else 0,
                        'outliers_count': len(self._detect_outliers_iqr(col_data)),
                        'distribution_type': self._classify_distribution(col_data.skew(), col_data.kurtosis()),
                        'percentiles': {
                            '25th': float(col_data.quantile(0.25)),
                            '75th': float(col_data.quantile(0.75)),
                            '95th': float(col_data.quantile(0.95))
                        }
                    })

                    statistical_results.append(basic_stats)

                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è {col}: {e}")

        stats_df = pd.DataFrame(statistical_results)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        correlation_matrix = {}
        if len(numeric_cols) >= 2:
            try:
                corr_matrix = df[numeric_cols].corr()
                correlation_matrix = convert_to_serializable(corr_matrix.to_dict())
            except:
                pass

        # GPT –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –Ω–∞—Ö–æ–¥–æ–∫
        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=stats_df,
                table_name=main_table,
                analysis_type='statistical_insights',
                context={
                    'question': question,
                    'statistical_results': statistical_results[:3],  # –ü–µ—Ä–≤—ã–µ 3 –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                    'correlation_matrix': correlation_matrix
                }
            )
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        summary = f"üìà **–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{main_table}'**\n\n"
        summary += f"üìä –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(statistical_results)} —á–∏—Å–ª–æ–≤—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n\n"

        if statistical_results:
            # –ù–∞—Ö–æ–¥–∏–º –Ω–∞–∏–±–æ–ª–µ–µ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
            high_variance_cols = [r for r in statistical_results if r['coefficient_variation'] > 1]
            skewed_cols = [r for r in statistical_results if abs(r['skewness']) > 1]

            if high_variance_cols:
                summary += f"üìä –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞–π–¥–µ–Ω–∞ –≤ {len(high_variance_cols)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n"
            if skewed_cols:
                summary += f"üìà –ê—Å–∏–º–º–µ—Ç—Ä–∏—á–Ω–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –≤ {len(skewed_cols)} –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö\n"

            summary += f"\nü§ñ **GPT –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"

        return {
            'question': question,
            'data': clean_dataframe_for_json(stats_df),
            'summary': summary,
            'analyzed_tables': [main_table],
            'gpt_insights': gpt_insights,
            'statistical_insights': statistical_results,
            'correlation_matrix': correlation_matrix,
            'statistical_stats': {
                'variables_analyzed': len(statistical_results),
                'high_variance_count': len([r for r in statistical_results if r.get('coefficient_variation', 0) > 1]),
                'outliers_detected': sum(r.get('outliers_count', 0) for r in statistical_results)
            },
            'chart_data': self._prepare_chart_data_safe(stats_df, 'scatter', 'mean',
                                                        'std') if not stats_df.empty else None
        }

    def _classify_distribution(self, skewness: float, kurtosis: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if abs(skewness) < 0.5 and abs(kurtosis) < 0.5:
            return '–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ'
        elif skewness > 0.5:
            return '–ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è'
        elif skewness < -0.5:
            return '–ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è'
        elif kurtosis > 0.5:
            return '–≤—ã—Å–æ–∫–∏–π —ç–∫—Å—Ü–µ—Å—Å'
        elif kurtosis < -0.5:
            return '–Ω–∏–∑–∫–∏–π —ç–∫—Å—Ü–µ—Å—Å'
        else:
            return '—Å–º–µ—à–∞–Ω–Ω–æ–µ'

    def _detect_outliers_iqr(self, data: pd.Series) -> List:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º IQR"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return data[(data < lower_bound) | (data > upper_bound)].tolist()

    def _analyze_predictive_patterns(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        predictive_insights = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            date_cols = self._find_date_columns(df)
            numeric_cols = df.select_dtypes(include=[np.number]).columns

            if date_cols and len(numeric_cols) > 0:
                date_col = date_cols[0]
                try:
                    df_temp = df.copy()
                    df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    df_clean = df_temp.dropna(subset=[date_col])

                    if len(df_clean) > 10:
                        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º
                        df_clean['period'] = df_clean[date_col].dt.to_period('M')

                        for num_col in numeric_cols[:3]:
                            monthly_data = df_clean.groupby('period')[num_col].agg(
                                ['count', 'sum', 'mean']).reset_index()

                            if len(monthly_data) > 3:
                                # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
                                trend_data = monthly_data['mean'].values
                                trend_direction = '—Ä–∞—Å—Ç—É—â–∏–π' if len(trend_data) > 1 and trend_data[-1] > trend_data[
                                    0] else '—É–±—ã–≤–∞—é—â–∏–π'

                                # –ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏
                                seasonality_score = float(np.std(trend_data) / np.mean(trend_data)) if np.mean(
                                    trend_data) > 0 else 0

                                # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º–æ—Å—Ç—å
                                predictability = self._assess_predictability(trend_data)

                                predictive_insights.append({
                                    'table': table_name,
                                    'metric': num_col,
                                    'trend_direction': trend_direction,
                                    'seasonality_score': round(seasonality_score, 3),
                                    'periods_analyzed': len(monthly_data),
                                    'predictability': predictability,
                                    'data_points': len(df_clean),
                                    'ml_readiness': self._assess_ml_readiness(df_clean, num_col)
                                })

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –≤ {table_name}: {e}")

        predictive_df = pd.DataFrame(predictive_insights)

        # GPT –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞
        try:
            gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                df=predictive_df,
                table_name='predictive_analysis',
                analysis_type='predictive_analysis',
                context={
                    'question': question,
                    'predictive_insights': predictive_insights[:3]
                }
            ) if not predictive_df.empty else {'gpt_analysis': ''}
            gpt_insights = gpt_analysis.get('gpt_analysis', '')
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ GPT –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞: {e}")
            gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

        if not predictive_df.empty:
            high_predictability = len(predictive_df[predictive_df['predictability'] == '–≤—ã—Å–æ–∫–∞—è'])
            ml_ready = len(predictive_df[predictive_df['ml_readiness'] >= 7])

            summary = f"üîÆ **–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {len(predictive_insights)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤**\n\n"
            summary += f"üìä **–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏:**\n"
            summary += f"‚Ä¢ –í—ã—Å–æ–∫–∞—è –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å: {high_predictability} –º–µ—Ç—Ä–∏–∫\n"
            summary += f"‚Ä¢ –ì–æ—Ç–æ–≤—ã –¥–ª—è ML: {ml_ready} –º–µ—Ç—Ä–∏–∫\n"
            summary += f"‚Ä¢ –°—Ä–µ–¥–Ω—è—è —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—å: {predictive_df['seasonality_score'].mean():.3f}\n\n"
            summary += f"ü§ñ **GPT –ü—Ä–æ–≥–Ω–æ–∑—ã:**\n{gpt_insights}"
        else:
            summary = "üîÆ –í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        return {
            'question': question,
            'data': clean_dataframe_for_json(predictive_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'gpt_insights': gpt_insights,
            'predictive_patterns': predictive_insights,
            'predictive_stats': {
                'time_series_found': len(predictive_insights),
                'high_predictability_count': len([p for p in predictive_insights if p['predictability'] == '–≤—ã—Å–æ–∫–∞—è']),
                'ml_ready_count': len([p for p in predictive_insights if p.get('ml_readiness', 0) >= 7])
            },
            'chart_data': self._prepare_chart_data_safe(predictive_df, 'scatter', 'seasonality_score',
                                                        'data_points') if not predictive_df.empty else None
        }

    def _assess_predictability(self, trend_data: np.ndarray) -> str:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ—Å—Ç—å –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞"""
        if len(trend_data) < 3:
            return '–Ω–∏–∑–∫–∞—è'

        try:
            # –ö–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç –≤–∞—Ä–∏–∞—Ü–∏–∏
            cv = np.std(trend_data) / np.mean(trend_data) if np.mean(trend_data) > 0 else float('inf')

            if cv < 0.3:
                return '–≤—ã—Å–æ–∫–∞—è'
            elif cv < 0.6:
                return '—Å—Ä–µ–¥–Ω—è—è'
            else:
                return '–Ω–∏–∑–∫–∞—è'
        except:
            return '–Ω–∏–∑–∫–∞—è'

    def _assess_ml_readiness(self, df: pd.DataFrame, target_col: str) -> int:
        """–û—Ü–µ–Ω–∏–≤–∞–µ—Ç –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è (0-10)"""
        score = 0

        try:
            # –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö
            if len(df) > 1000:
                score += 3
            elif len(df) > 100:
                score += 2
            elif len(df) > 50:
                score += 1

            # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_count = len(df.select_dtypes(include=[np.number]).columns) - 1  # -1 –¥–ª—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
            if feature_count > 10:
                score += 2
            elif feature_count > 5:
                score += 1

            # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö
            missing_pct = df[target_col].isnull().sum() / len(df) * 100
            if missing_pct < 5:
                score += 2
            elif missing_pct < 15:
                score += 1

            # –ù–∞–ª–∏—á–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            categorical_count = len(df.select_dtypes(include=['object']).columns)
            if 0 < categorical_count < 10:
                score += 1

            # –í—Ä–µ–º–µ–Ω–Ω–∞—è —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∞—è
            date_cols = self._find_date_columns(df)
            if date_cols:
                score += 1

        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ü–µ–Ω–∫–∏ ML –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏: {e}")

        return min(10, score)

    def _analyze_general(self, question: str) -> Dict[str, Any]:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ —Å GPT –ø–æ–º–æ—â—å—é"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            if self.df_manager.tables:
                largest_table = max(self.df_manager.tables.items(), key=lambda x: len(x[1]))
                tables_mentioned = [largest_table[0]]
            else:
                return {
                    'question': question,
                    'data': [],
                    'summary': '–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                    'analyzed_tables': []
                }

        table_name = tables_mentioned[0]
        df = self.df_manager.tables[table_name]

        # –°–æ–∑–¥–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        general_stats = {
            'table_name': table_name,
            'total_rows': int(len(df)),
            'total_columns': int(len(df.columns)),
            'columns_list': list(df.columns),
            'data_types': convert_to_serializable(df.dtypes.value_counts().to_dict()),
            'missing_values': int(df.isnull().sum().sum()),
            'duplicate_rows': int(df.duplicated().sum()),
            'memory_usage_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 2)
        }

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∞
        numeric_summary = {}
        categorical_summary = {}

        # –ß–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            for col in numeric_cols[:5]:  # –ü–µ—Ä–≤—ã–µ 5 —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                try:
                    numeric_summary[col] = {
                        'mean': float(df[col].mean()),
                        'median': float(df[col].median()),
                        'std': float(df[col].std()),
                        'min': float(df[col].min()),
                        'max': float(df[col].max()),
                        'count': int(df[col].count())
                    }
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                    numeric_summary[col] = {'error': str(e)}

                # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
            categorical_cols = df.select_dtypes(include=['object']).columns
            for col in categorical_cols[:5]:  # –ü–µ—Ä–≤—ã–µ 5 –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
                try:
                    value_counts = df[col].value_counts()
                    if len(value_counts) > 0:
                        categorical_summary[col] = {
                            'unique_values': int(df[col].nunique()),
                            'most_common': str(value_counts.index[0]),
                            'most_common_count': int(value_counts.iloc[0]),
                            'top_5_values': convert_to_serializable(value_counts.head(5).to_dict())
                        }
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                    categorical_summary[col] = {'error': str(e)}

            # GPT –∞–Ω–∞–ª–∏–∑ –æ–±—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö
            try:
                gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
                    df=df,
                    table_name=table_name,
                    analysis_type="business_insights",
                    context={
                        'question': question,
                        'general_stats': general_stats,
                        'numeric_summary': numeric_summary,
                        'categorical_summary': categorical_summary
                    }
                )
                gpt_insights = gpt_analysis.get('gpt_analysis', 'GPT –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ GPT –∞–Ω–∞–ª–∏–∑–∞: {e}")
                gpt_insights = 'GPT –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω'

            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
            sample_data = clean_dataframe_for_json(df.head(10))

            summary = f"üìä **–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'**\n\n"
            summary += f"üìà –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫\n"
            summary += f"‚ùå –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {general_stats['missing_values']}\n"
            summary += f"üîÑ –î—É–±–ª–∏–∫–∞—Ç–æ–≤: {general_stats['duplicate_rows']}\n\n"
            summary += f"ü§ñ **GPT –ò–Ω—Å–∞–π—Ç—ã:**\n{gpt_insights}"

            return {
                'question': question,
                'data': sample_data,
                'summary': summary,
                'analyzed_tables': [table_name],
                'gpt_insights': gpt_insights,
                'general_stats': general_stats,
                'numeric_summary': numeric_summary,
                'categorical_summary': categorical_summary,
                'chart_data': self._prepare_chart_data_safe(df.head(100), 'histogram', df.columns[0], None)
            }

        # =============== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –ú–ï–¢–û–î–´ ===============

        def _compare_columns_in_table(self, table_name: str, question: str) -> Dict[str, Any]:
            """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
            df = self.df_manager.tables[table_name]

            # –ü–æ–ª—É—á–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
            numeric_cols = df.select_dtypes(include=[np.number]).columns

            if len(numeric_cols) < 2:
                return {
                    'question': question,
                    'data': [],
                    'summary': f'–í —Ç–∞–±–ª–∏—Ü–µ {table_name} –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è',
                    'analyzed_tables': [table_name]
                }

            comparison_data = []

            # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
            for col in numeric_cols[:5]:
                try:
                    col_data = df[col].dropna()
                    if len(col_data) > 0:
                        comparison_data.append({
                            'column': col,
                            'mean': round(float(col_data.mean()), 2),
                            'median': round(float(col_data.median()), 2),
                            'std': round(float(col_data.std()), 2),
                            'min': float(col_data.min()),
                            'max': float(col_data.max()),
                            'range': float(col_data.max() - col_data.min()),
                            'count': int(len(col_data))
                        })
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

            if comparison_data:
                comparison_df = pd.DataFrame(comparison_data)

                # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º —Å—Ä–µ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º
                max_mean_col = comparison_df.loc[comparison_df['mean'].idxmax(), 'column']
                min_mean_col = comparison_df.loc[comparison_df['mean'].idxmin(), 'column']

                summary = f"üìä –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ '{table_name}'. "
                summary += f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {max_mean_col}. "
                summary += f"–ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {min_mean_col}."
            else:
                comparison_df = pd.DataFrame()
                summary = "–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ä–∞–≤–Ω–∏—Ç—å –∫–æ–ª–æ–Ω–∫–∏"

            return {
                'question': question,
                'data': clean_dataframe_for_json(comparison_df),
                'summary': summary,
                'analyzed_tables': [table_name],
                'chart_data': self._prepare_chart_data_safe(comparison_df, 'bar', 'column', 'mean')
            }

        # =============== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ê–ù–ê–õ–ò–ó–´ ===============

        logger.info("DataFrame Analyzer –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é")

    def _analyze_categorical_columns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        categorical_analysis = {}
        categorical_cols = df.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            try:
                value_counts = df[col].value_counts()
                categorical_analysis[col] = {
                    'unique_count': int(len(value_counts)),
                    'top_values': convert_to_serializable(value_counts.head(5).to_dict()),
                    'null_count': int(df[col].isnull().sum()),
                    'most_common': str(value_counts.index[0]) if len(value_counts) > 0 else 'N/A',
                    'most_common_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0,
                    'diversity_score': round(len(value_counts) / len(df) * 100, 2),  # –ü—Ä–æ—Ü–µ–Ω—Ç —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
                    'fill_rate': round((1 - df[col].isnull().sum() / len(df)) * 100, 2)
                }

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã—Ö ID –∫–æ–ª–æ–Ω–æ–∫
                if 'id' in col.lower():
                    categorical_analysis[col]['is_likely_id'] = df[col].nunique() / len(df) > 0.8

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                categorical_analysis[col] = {
                    'unique_count': 0,
                    'top_values': {},
                    'null_count': int(df[col].isnull().sum()),
                    'most_common': 'Error',
                    'most_common_count': 0,
                    'error': str(e)
                }

        return categorical_analysis

    def _detect_anomalies_dataframe(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ DataFrame"""
        anomalies = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            try:
                if df[col].count() > 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1

                    if IQR > 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

                        if len(outliers) > 0:
                            sample_values = outliers[col].head(5).tolist()
                            anomalies.append({
                                'type': 'outlier',
                                'column': col,
                                'count': int(len(outliers)),
                                'percentage': round(float(len(outliers) / len(df) * 100), 2),
                                'lower_bound': round(float(lower_bound), 2),
                                'upper_bound': round(float(upper_bound), 2),
                                'sample_values': [float(v) for v in sample_values],
                                'severity': 'high' if len(outliers) / len(df) > 0.1 else 'medium',
                                'description': f'–ù–∞–π–¥–µ–Ω–æ {len(outliers)} –≤—ã–±—Ä–æ—Å–æ–≤ –≤ –∫–æ–ª–æ–Ω–∫–µ {col}'
                            })

                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∞–Ω–æ–º–∞–ª–∏–π
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ —á–∞—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if df[col].dtype in ['int64', 'float64']:
                        value_counts = df[col].value_counts()
                        if len(value_counts) > 0:
                            most_frequent_count = value_counts.iloc[0]
                            if most_frequent_count / len(df) > 0.5:  # –ë–æ–ª—å—à–µ 50% –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                                anomalies.append({
                                    'type': 'frequent_value',
                                    'column': col,
                                    'count': int(most_frequent_count),
                                    'percentage': round(float(most_frequent_count / len(df) * 100), 2),
                                    'value': float(value_counts.index[0]),
                                    'severity': 'medium',
                                    'description': f'–ó–Ω–∞—á–µ–Ω–∏–µ {value_counts.index[0]} –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ {most_frequent_count} –∏–∑ {len(df)} –∑–∞–ø–∏—Å–µ–π'
                                })

                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –∫–æ–ª–æ–Ω–∫–∞—Ö, –≥–¥–µ –æ–Ω–∏ –Ω–µ –æ–∂–∏–¥–∞—é—Ç—Å—è
                    if any(keyword in col.lower() for keyword in ['price', 'cost', 'amount', 'quantity', 'age']):
                        negative_count = (df[col] < 0).sum()
                        if negative_count > 0:
                            anomalies.append({
                                'type': 'negative_values',
                                'column': col,
                                'count': int(negative_count),
                                'percentage': round(float(negative_count / len(df) * 100), 2),
                                'severity': 'high',
                                'description': f'–ù–∞–π–¥–µ–Ω–æ {negative_count} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ {col}, –≥–¥–µ –æ–Ω–∏ –Ω–µ –æ–∂–∏–¥–∞—é—Ç—Å—è'
                            })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {e}")

        # –ü—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            try:
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                unique_ratio = df[col].nunique() / len(df)
                if unique_ratio > 0.9:  # –ë–æ–ª—å—à–µ 90% —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    anomalies.append({
                        'type': 'high_cardinality',
                        'column': col,
                        'count': int(df[col].nunique()),
                        'percentage': round(float(unique_ratio * 100), 2),
                        'severity': 'low',
                        'description': f'–ö–æ–ª–æ–Ω–∫–∞ {col} –∏–º–µ–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({df[col].nunique()} –∏–∑ {len(df)})'
                    })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {e}")

        return anomalies

    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        try:
            missing_data = df.isnull().sum()
            total_missing = missing_data.sum()
            total_cells = len(df) * len(df.columns)

            missing_analysis = {
                'total_missing': int(total_missing),
                'missing_percentage': round(float(total_missing / total_cells * 100), 2),
                'columns_with_missing': {},
                'missing_patterns': {},
                'data_quality_score': 0
            }

            # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–æ–ª–æ–Ω–∫–∞–º
            for col, missing_count in missing_data.items():
                if missing_count > 0:
                    missing_percentage = missing_count / len(df) * 100
                    missing_analysis['columns_with_missing'][col] = {
                        'count': int(missing_count),
                        'percentage': round(float(missing_percentage), 2),
                        'severity': 'critical' if missing_percentage > 50 else 'high' if missing_percentage > 20 else 'medium'
                    }

            # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –ø—Ä–æ–ø—É—Å–∫–æ–≤
            if len(missing_analysis['columns_with_missing']) > 1:
                # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –ø–æ—Ö–æ–∂–∏–º–∏ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º–∏ –ø—Ä–æ–ø—É—Å–∫–æ–≤
                missing_patterns = {}
                for col1, info1 in missing_analysis['columns_with_missing'].items():
                    for col2, info2 in missing_analysis['columns_with_missing'].items():
                        if col1 != col2:
                            diff = abs(info1['percentage'] - info2['percentage'])
                            if diff < 5:  # –†–∞–∑–Ω–∏—Ü–∞ –º–µ–Ω—å—à–µ 5%
                                pattern_key = f"{col1}_similar_to_{col2}"
                                missing_patterns[pattern_key] = {
                                    'columns': [col1, col2],
                                    'similarity': round(100 - diff, 1)
                                }

                missing_analysis['missing_patterns'] = missing_patterns

            # –û–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö (–ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º)
            completeness = (1 - total_missing / total_cells) * 100
            missing_analysis['data_quality_score'] = round(completeness, 1)

            return missing_analysis

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {e}")
            return {
                'total_missing': 0,
                'missing_percentage': 0,
                'columns_with_missing': {},
                'error': str(e)
            }

    def _analyze_correlations_single_table(self, df: pd.DataFrame, table_name: str = None) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ"""
        correlations = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) >= 2:
            try:
                corr_matrix = df[numeric_cols].corr()

                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∑–Ω–∞—á–∏–º—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                for i in range(len(corr_matrix.columns)):
                    for j in range(i + 1, len(corr_matrix.columns)):
                        corr_value = corr_matrix.iloc[i, j]

                        if not pd.isna(corr_value) and abs(corr_value) > 0.3:  # –ó–Ω–∞—á–∏–º–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                            col1 = corr_matrix.columns[i]
                            col2 = corr_matrix.columns[j]

                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Å–∏–ª—É –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                            abs_corr = abs(corr_value)
                            if abs_corr > 0.8:
                                strength = '–æ—á–µ–Ω—å —Å–∏–ª—å–Ω–∞—è'
                            elif abs_corr > 0.6:
                                strength = '—Å–∏–ª—å–Ω–∞—è'
                            elif abs_corr > 0.4:
                                strength = '—É–º–µ—Ä–µ–Ω–Ω–∞—è'
                            else:
                                strength = '—Å–ª–∞–±–∞—è'

                            correlations.append({
                                'table': table_name or 'unknown',
                                'column1': col1,
                                'column2': col2,
                                'correlation': round(float(corr_value), 3),
                                'correlation_abs': round(float(abs_corr), 3),
                                'strength': strength,
                                'direction': '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è' if corr_value > 0 else '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è',
                                'business_meaning': self._interpret_correlation(col1, col2, corr_value),
                                'statistical_significance': '–≤—ã—Å–æ–∫–∞—è' if abs_corr > 0.7 else '—É–º–µ—Ä–µ–Ω–Ω–∞—è'
                            })

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                correlations.sort(key=lambda x: x['correlation_abs'], reverse=True)

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")

        return correlations

    def _interpret_correlation(self, col1: str, col2: str, correlation: float) -> str:
        """–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–µ—Ç –±–∏–∑–Ω–µ—Å-—Å–º—ã—Å–ª –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏"""
        try:
            col1_lower = col1.lower()
            col2_lower = col2.lower()

            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            if any(word in col1_lower for word in ['price', 'cost']) and any(
                    word in col2_lower for word in ['revenue', 'sales']):
                return "–°–≤—è–∑—å –º–µ–∂–¥—É —Ü–µ–Ω–æ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º –∏ –¥–æ—Ö–æ–¥–∞–º–∏"
            elif any(word in col1_lower for word in ['quantity', 'amount']) and any(
                    word in col2_lower for word in ['revenue', 'total']):
                return "–û–±—ä–µ–º–Ω–æ-—Å—Ç–æ–∏–º–æ—Å—Ç–Ω–∞—è –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å"

            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            elif any(word in col1_lower for word in ['time', 'date', 'age']) and any(
                    word in col2_lower for word in ['value', 'score']):
                return "–í—Ä–µ–º–µ–Ω–Ω–∞—è –¥–∏–Ω–∞–º–∏–∫–∞ –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è"

            # –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            elif any(word in col1_lower for word in ['quality', 'rating']) and any(
                    word in col2_lower for word in ['price', 'cost']):
                return "–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç—å –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å—Ç–æ–∏–º–æ—Å—Ç–∏"

            # –û–±—â–∞—è –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è
            else:
                if correlation > 0:
                    return f"–ü—Ä–∏ —Ä–æ—Å—Ç–µ {col1} —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è {col2}"
                else:
                    return f"–ü—Ä–∏ —Ä–æ—Å—Ç–µ {col1} —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è {col2}"

        except Exception:
            return "–¢—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞"

    def _prepare_business_metrics_chart(self, metrics: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å–ª–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
            chart_metrics = []

            for key, value in metrics.items():
                if isinstance(value, (int, float)) and not key.endswith('_count') and key not in ['total_records']:
                    # –§–æ—Ä–º–∞—Ç–∏—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫–∏
                    formatted_name = key.replace('_', ' ').title()

                    # –î–æ–±–∞–≤–ª—è–µ–º –µ–¥–∏–Ω–∏—Ü—ã –∏–∑–º–µ—Ä–µ–Ω–∏—è –≥–¥–µ –≤–æ–∑–º–æ–∂–Ω–æ
                    if 'percentage' in key.lower() or 'rate' in key.lower():
                        formatted_name += ' (%)'
                    elif 'total' in key.lower() or 'sum' in key.lower():
                        formatted_name += ' (Total)'
                    elif 'average' in key.lower() or 'avg' in key.lower():
                        formatted_name += ' (Avg)'

                    chart_metrics.append({
                        'metric': formatted_name,
                        'value': float(value),
                        'category': self._categorize_metric(key)
                    })

            if len(chart_metrics) > 0:
                # –°–æ–∑–¥–∞–µ–º DataFrame –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞
                chart_df = pd.DataFrame(chart_metrics)

                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∑–Ω–∞—á–µ–Ω–∏—é –¥–ª—è –ª—É—á—à–µ–π –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                chart_df = chart_df.sort_values('value', ascending=False)

                return self._prepare_chart_data_safe(chart_df, 'bar', 'metric', 'value')

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ business metrics chart: {e}")
            return None

    def _categorize_metric(self, metric_name: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –º–µ—Ç—Ä–∏–∫—É –¥–ª—è –ª—É—á—à–µ–π –≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∏"""
        metric_lower = metric_name.lower()

        if any(word in metric_lower for word in ['revenue', 'sales', 'amount', 'total', 'sum']):
            return 'financial'
        elif any(word in metric_lower for word in ['percentage', 'rate', 'ratio']):
            return 'percentage'
        elif any(word in metric_lower for word in ['average', 'mean', 'median']):
            return 'average'
        elif any(word in metric_lower for word in ['count', 'number', 'quantity']):
            return 'count'
        elif any(word in metric_lower for word in ['completeness', 'quality', 'score']):
            return 'quality'
        else:
            return 'other'
