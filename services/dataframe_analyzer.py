# services/dataframe_analyzer.py
import pandas as pd
import numpy as np
from typing import Dict, List, Any, Optional, Tuple
import logging
from services.dataframe_manager import DataFrameManager
from datetime import datetime
import re
import sys
import os
from services.gpt_analyzer import SmartGPTAnalyzer  # –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –∏–º–ø–æ—Ä—Ç

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ utils
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from utils.json_serializer import convert_to_serializable, clean_dataframe_for_json

logger = logging.getLogger(__name__)


class DataFrameAnalyzer:
    """–ê–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏–π –¥–≤–∏–∂–æ–∫ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å DataFrame"""

    def __init__(self, df_manager: DataFrameManager):
        self.df_manager = df_manager
        self.analysis_cache = {}
        self.smart_gpt = SmartGPTAnalyzer()

    def _detect_user_focus(self, question: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ñ–æ–∫—É—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"""
        question_lower = question.lower()

        if any(word in question_lower for word in ['–ø—Ä–æ–±–ª–µ–º', '–æ—à–∏–±–∫', '–∞–Ω–æ–º–∞–ª–∏', '–Ω–µ–ø—Ä–∞–≤–∏–ª—å']):
            return 'problem_solving'
        elif any(word in question_lower for word in ['–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç', '–ø–æ—Ç–µ–Ω—Ü–∏–∞–ª', '—Ä–æ—Å—Ç', '—É–ª—É—á—à–µ–Ω']):
            return 'opportunity_discovery'
        elif any(word in question_lower for word in ['—Å—Ä–∞–≤–Ω', '—Ä–∞–∑–ª–∏—á', 'vs', '–ø—Ä–æ—Ç–∏–≤']):
            return 'comparative_analysis'
        elif any(word in question_lower for word in ['—Ç—Ä–µ–Ω–¥', '–¥–∏–Ω–∞–º–∏–∫', '–∏–∑–º–µ–Ω–µ–Ω', '—Ä–∞–∑–≤–∏—Ç–∏']):
            return 'trend_analysis'
        else:
            return 'general_insights'

    def _analyze_statistical_insights(self, question: str) -> Dict[str, Any]:
        """–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å GPT –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]

        main_table = tables_mentioned[0]
        df = self.df_manager.tables[main_table]

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        statistical_results = []

        if len(numeric_cols) > 0:
            # –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            desc_stats = df[numeric_cols].describe()

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
            for col in numeric_cols:
                col_data = df[col].dropna()
                if len(col_data) > 5:
                    skewness = col_data.skew()
                    kurtosis = col_data.kurtosis()

                    statistical_results.append({
                        'column': col,
                        'mean': float(col_data.mean()),
                        'median': float(col_data.median()),
                        'std': float(col_data.std()),
                        'skewness': float(skewness),
                        'kurtosis': float(kurtosis),
                        'distribution_type': self._classify_distribution(skewness, kurtosis),
                        'outliers_count': len(self._detect_outliers_iqr(col_data)),
                        'coefficient_variation': float(col_data.std() / col_data.mean()) if col_data.mean() != 0 else 0
                    })

        stats_df = pd.DataFrame(statistical_results)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω–∞—è –º–∞—Ç—Ä–∏—Ü–∞ —Å –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π
        correlations = self._analyze_correlations_single_table(df, main_table)

        summary = f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{main_table}' –∑–∞–≤–µ—Ä—à–µ–Ω"

        return {
            'question': question,
            'data': clean_dataframe_for_json(stats_df),
            'summary': summary,
            'analyzed_tables': [main_table],
            'statistical_insights': statistical_results,
            'correlations': correlations,
            'chart_data': self._prepare_chart_data_safe(stats_df, 'bar', 'column', 'mean')
        }

    def _detect_outliers_iqr(self, data: pd.Series) -> List:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º IQR"""
        Q1 = data.quantile(0.25)
        Q3 = data.quantile(0.75)
        IQR = Q3 - Q1
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        return data[(data < lower_bound) | (data > upper_bound)].tolist()

    def _classify_distribution(self, skewness: float, kurtosis: float) -> str:
        """–ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ç–∏–ø —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è"""
        if abs(skewness) < 0.5 and abs(kurtosis) < 0.5:
            return '–Ω–æ—Ä–º–∞–ª—å–Ω–æ–µ'
        elif skewness > 0.5:
            return '–ø—Ä–∞–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è'
        elif skewness < -0.5:
            return '–ª–µ–≤–æ—Å—Ç–æ—Ä–æ–Ω–Ω—è—è –∞—Å–∏–º–º–µ—Ç—Ä–∏—è'
        elif kurtosis > 0.5:
            return '–≤—ã—Å–æ–∫–∏–π —ç–∫—Å—Ü–µ—Å—Å'
        elif kurtosis < -0.5:
            return '–Ω–∏–∑–∫–∏–π —ç–∫—Å—Ü–µ—Å—Å'
        else:
            return '—Å–º–µ—à–∞–Ω–Ω–æ–µ'

    def _create_business_context(self, question: str, analysis_type: str) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–µ—Ç –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è —É–º–Ω–æ–≥–æ GPT-–∞–Ω–∞–ª–∏–∑–∞"""
        context = {
            'question_intent': question,
            'analysis_type': analysis_type,
            'user_focus': self._detect_user_focus(question)
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–æ–º–µ–Ω–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –Ω–∞–∑–≤–∞–Ω–∏–π —Ç–∞–±–ª–∏—Ü
        table_names = list(self.df_manager.tables.keys())
        context['entities'] = table_names

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞
        question_lower = question.lower()
        if any(word in question_lower for word in ['–ø—Ä–∏–±—ã–ª—å', '–¥–æ—Ö–æ–¥', '—Å—Ç–æ–∏–º–æ—Å—Ç—å', '—Ü–µ–Ω–∞']):
            context['priority_metrics'] = ['financial_performance', 'profitability']
        elif any(word in question_lower for word in ['–∫–ª–∏–µ–Ω—Ç', '–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å', '–ø–æ–∫—É–ø–∞—Ç–µ–ª—å']):
            context['priority_metrics'] = ['customer_satisfaction', 'retention']
        elif any(word in question_lower for word in ['–ø—Ä–æ–¥–∞–∂–∏', '–∫–æ–Ω–≤–µ—Ä—Å–∏—è', '–≤–æ—Ä–æ–Ω–∫–∞']):
            context['priority_metrics'] = ['sales_performance', 'conversion_rates']
        else:
            context['priority_metrics'] = ['general_performance']

        return context

    def analyze_question(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤–æ–ø—Ä–æ—Å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å —É–º–Ω—ã–º GPT-–∞–Ω–∞–ª–∏–∑–æ–º"""
        logger.info(f"–ê–Ω–∞–ª–∏–∑ –≤–æ–ø—Ä–æ—Å–∞: {question}")

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –∞–Ω–∞–ª–∏–∑–∞
        analysis_type = self._categorize_question(question)

        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π DataFrame –∞–Ω–∞–ª–∏–∑
            if analysis_type == 'overview':
                base_results = self._analyze_overview()
            elif analysis_type == 'table_analysis':
                table_name = self._extract_table_name(question)
                base_results = self._analyze_single_table(table_name)
            elif analysis_type == 'relationship_analysis':
                base_results = self._analyze_relationships()
            elif analysis_type == 'aggregation':
                base_results = self._analyze_aggregations(question)
            elif analysis_type == 'trend_analysis':
                base_results = self._analyze_trends(question)
            elif analysis_type == 'correlation':
                base_results = self._analyze_correlations(question)
            elif analysis_type == 'comparison':
                base_results = self._analyze_comparison(question)
            elif analysis_type == 'anomalies':
                base_results = self._analyze_anomalies(question)
            elif analysis_type == 'business_insights':
                base_results = self._analyze_business_metrics(question)
            elif analysis_type == 'statistical_insights':
                base_results = self._analyze_statistical_insights(question)
            elif analysis_type == 'predictive_analysis':
                base_results = self._analyze_predictive_patterns(question)
            elif analysis_type == 'data_quality':
                base_results = self._analyze_data_quality_comprehensive(question)
            else:
                base_results = self._analyze_general(question)

            # –ü—Ä–∏–º–µ–Ω—è–µ–º —É–º–Ω—ã–π GPT-–∞–Ω–∞–ª–∏–∑ –∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º
            if not base_results.get('error') and base_results.get('data'):
                try:
                    # –°–æ–∑–¥–∞–µ–º –±–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è GPT
                    business_context = self._create_business_context(question, analysis_type)

                    # –ü–æ–ª—É—á–∞–µ–º —É–º–Ω—ã–µ –∏–Ω—Å–∞–π—Ç—ã
                    smart_analysis = self.smart_gpt.analyze_findings_with_context(
                        dataframe_results=base_results,
                        business_context=business_context
                    )

                    # –û–±–æ–≥–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã GPT-–∏–Ω—Å–∞–π—Ç–∞–º–∏
                    base_results['smart_gpt_insights'] = smart_analysis
                    base_results['business_insights'] = smart_analysis.get('business_insights', '')
                    base_results['action_items'] = smart_analysis.get('action_items', [])
                    base_results['risk_assessment'] = smart_analysis.get('risk_assessment', '')
                    base_results['opportunities'] = smart_analysis.get('opportunities', [])
                    base_results['gpt_confidence'] = smart_analysis.get('confidence', 'medium')

                    # –û–±–Ω–æ–≤–ª—è–µ–º summary —Å –±–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç–∞–º–∏
                    if smart_analysis.get('business_insights'):
                        base_results['summary'] = (
                            f"**–ë–ò–ó–ù–ï–°-–ê–ù–ê–õ–ò–ó:**\n{smart_analysis['business_insights']}\n\n"
                            f"**–¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –î–ê–ù–ù–´–ï:**\n{base_results.get('summary', '')}"
                        )

                except Exception as gpt_error:
                    logger.error(f"–û—à–∏–±–∫–∞ —É–º–Ω–æ–≥–æ GPT-–∞–Ω–∞–ª–∏–∑–∞: {gpt_error}")
                    base_results['smart_gpt_insights'] = {
                        'business_insights': 'GPT-–∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω',
                        'confidence': 'low'
                    }

            return base_results

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤–æ–ø—Ä–æ—Å–∞ '{question}': {e}")
            return {
                'question': question,
                'error': str(e),
                'data': [],
                'summary': f'–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å: {str(e)}',
                'analyzed_tables': []
            }

    def _analyze_data_quality_comprehensive(self, question: str) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        quality_results = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
            total_cells = len(df) * len(df.columns)
            null_cells = df.isnull().sum().sum()
            duplicate_rows = df.duplicated().sum()

            # –ê–Ω–∞–ª–∏–∑ –ø–æ —Ç–∏–ø–∞–º –∫–æ–ª–æ–Ω–æ–∫
            numeric_quality = self._analyze_numeric_quality(df)
            categorical_quality = self._analyze_categorical_quality(df)

            # –¶–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å –¥–∞–Ω–Ω—ã—Ö
            integrity_issues = []

            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≥–¥–µ –æ–Ω–∏ –Ω–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å
            for col in df.select_dtypes(include=[np.number]).columns:
                if 'price' in col.lower() or 'amount' in col.lower() or 'quantity' in col.lower():
                    negative_count = (df[col] < 0).sum()
                    if negative_count > 0:
                        integrity_issues.append(f"{col}: {negative_count} –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

            quality_score = self._calculate_quality_score(
                null_percentage=null_cells / total_cells * 100,
                duplicate_percentage=duplicate_rows / len(df) * 100,
                integrity_issues_count=len(integrity_issues)
            )

            quality_results.append({
                'table': table_name,
                'quality_score': round(quality_score, 1),
                'completeness': round((1 - null_cells / total_cells) * 100, 1),
                'uniqueness': round((1 - duplicate_rows / len(df)) * 100, 1),
                'integrity_issues': len(integrity_issues),
                'numeric_quality': numeric_quality,
                'categorical_quality': categorical_quality,
                'recommendations': self._generate_quality_recommendations(
                    null_cells / total_cells * 100, duplicate_rows / len(df) * 100, integrity_issues
                )
            })

        quality_df = pd.DataFrame(quality_results)

        avg_quality = quality_df['quality_score'].mean() if not quality_df.empty else 0
        summary = f"üîç –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö: —Å—Ä–µ–¥–Ω–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ {avg_quality:.1f}/100"

        return {
            'question': question,
            'data': clean_dataframe_for_json(quality_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'quality_metrics': quality_results,
            'chart_data': self._prepare_chart_data_safe(quality_df, 'bar', 'table', 'quality_score')
        }

    def _generate_quality_recommendations(self, null_pct: float, duplicate_pct: float, issues: List[str]) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        recommendations = []

        if null_pct > 10:
            recommendations.append("–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤—ã—Å–æ–∫–∏–π —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–ø—É—Å–∫–æ–≤ - —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è")
        elif null_pct > 5:
            recommendations.append("–£–º–µ—Ä–µ–Ω–Ω—ã–π —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–ø—É—Å–∫–æ–≤ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏–º–ø—É—Ç–∞—Ü–∏—é")

        if duplicate_pct > 5:
            recommendations.append("–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –¥—É–±–ª–∏–∫–∞—Ç—ã - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è")

        if issues:
            recommendations.append("–ù–∞–π–¥–µ–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö - —Ç—Ä–µ–±—É–µ—Ç—Å—è –≤–∞–ª–∏–¥–∞—Ü–∏—è")

        if not recommendations:
            recommendations.append("–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö –≤ –Ω–æ—Ä–º–µ")

        return recommendations

    def _analyze_categorical_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) == 0:
            return {'columns_analyzed': 0}

        categorical_issues = []
        for col in categorical_cols:
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ —Å–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
            unique_ratio = df[col].nunique() / len(df)
            if unique_ratio > 0.8:
                categorical_issues.append(f"{col}: –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ –º–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π ({unique_ratio:.1%})")

        return {
            'columns_analyzed': len(categorical_cols),
            'issues_found': len(categorical_issues),
            'issues': categorical_issues
        }

    def _calculate_quality_score(self, null_percentage: float, duplicate_percentage: float,
                                 integrity_issues_count: int) -> float:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –æ–±—â–∏–π –±–∞–ª–ª –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö"""
        base_score = 100.0

        # –°–Ω–∏–∂–∞–µ–º –∑–∞ –ø—Ä–æ–ø—É—Å–∫–∏
        base_score -= null_percentage * 0.5

        # –°–Ω–∏–∂–∞–µ–º –∑–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
        base_score -= duplicate_percentage * 0.3

        # –°–Ω–∏–∂–∞–µ–º –∑–∞ –ø—Ä–æ–±–ª–µ–º—ã —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏
        base_score -= integrity_issues_count * 5

        return max(0.0, base_score)


    def _analyze_numeric_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) == 0:
            return {'columns_analyzed': 0}

        numeric_issues = []
        for col in numeric_cols:
            inf_count = np.isinf(df[col]).sum()
            if inf_count > 0:
                numeric_issues.append(f"{col}: {inf_count} –±–µ—Å–∫–æ–Ω–µ—á–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")

        return {
            'columns_analyzed': len(numeric_cols),
            'issues_found': len(numeric_issues),
            'issues': numeric_issues
        }

    def _analyze_predictive_patterns(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏"""
        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        predictive_insights = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ü–æ–∏—Å–∫ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            date_cols = self._find_date_columns(df)
            numeric_cols = df.select_dtypes(include=[np.number]).columns

            if date_cols and len(numeric_cols) > 0:
                date_col = date_cols[0]
                try:
                    df_temp = df.copy()
                    df_temp[date_col] = pd.to_datetime(df_temp[date_col], errors='coerce')
                    df_clean = df_temp.dropna(subset=[date_col])

                    if len(df_clean) > 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                        # –ê–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–æ–≤ –ø–æ –º–µ—Å—è—Ü–∞–º
                        df_clean['period'] = df_clean[date_col].dt.to_period('M')

                        for num_col in numeric_cols[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                            monthly_data = df_clean.groupby('period')[num_col].agg(
                                ['count', 'sum', 'mean']).reset_index()

                            if len(monthly_data) > 3:
                                # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ç—Ä–µ–Ω–¥–∞
                                trend_data = monthly_data['mean'].values
                                trend_direction = '—Ä–∞—Å—Ç—É—â–∏–π' if trend_data[-1] > trend_data[0] else '—É–±—ã–≤–∞—é—â–∏–π'

                                # –°–µ–∑–æ–Ω–Ω–æ—Å—Ç—å (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è)
                                seasonality_score = np.std(trend_data) / np.mean(trend_data) if np.mean(
                                    trend_data) > 0 else 0

                                predictive_insights.append({
                                    'table': table_name,
                                    'metric': num_col,
                                    'trend_direction': trend_direction,
                                    'seasonality_score': round(float(seasonality_score), 3),
                                    'periods_analyzed': len(monthly_data),
                                    'predictability': '–≤—ã—Å–æ–∫–∞—è' if seasonality_score < 0.3 else '—Å—Ä–µ–¥–Ω—è—è' if seasonality_score < 0.6 else '–Ω–∏–∑–∫–∞—è',
                                    'data_points': len(df_clean)
                                })

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ä—è–¥–∞ –≤ {table_name}: {e}")

        predictive_df = pd.DataFrame(predictive_insights)

        if not predictive_df.empty:
            summary = f"üîÆ –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤: –Ω–∞–π–¥–µ–Ω–æ {len(predictive_insights)} –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤"
        else:
            summary = "–í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        return {
            'question': question,
            'data': clean_dataframe_for_json(predictive_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'predictive_patterns': predictive_insights,
            'chart_data': self._prepare_chart_data_safe(predictive_df, 'scatter', 'seasonality_score', 'data_points')
        }

    def _analyze_overview(self) -> Dict[str, Any]:
        """–û–±—â–∏–π –æ–±–∑–æ—Ä –¥–∞–Ω–Ω—ã—Ö"""
        overview_data = []

        for table_name, df in self.df_manager.tables.items():
            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            numeric_cols = len(df.select_dtypes(include=[np.number]).columns)
            categorical_cols = len(df.select_dtypes(include=['object']).columns)
            datetime_cols = len(df.select_dtypes(include=['datetime64']).columns)
            null_count = df.isnull().sum().sum()

            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            duplicate_rows = df.duplicated().sum()
            unique_rows = len(df.drop_duplicates())

            overview_data.append({
                'table_name': table_name,
                'row_count': int(len(df)),
                'column_count': int(len(df.columns)),
                'numeric_columns': int(numeric_cols),
                'categorical_columns': int(categorical_cols),
                'datetime_columns': int(datetime_cols),
                'null_values': int(null_count),
                'null_percentage': round(float((null_count / (len(df) * len(df.columns))) * 100), 2),
                'duplicate_rows': int(duplicate_rows),
                'unique_rows': int(unique_rows),
                'memory_usage_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 2)
            })

        overview_df = pd.DataFrame(overview_data)

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–≤—è–∑—è—Ö
        relations_info = []
        for rel in self.df_manager.relations:
            relations_info.append(f"{rel.from_table}.{rel.from_column} -> {rel.to_table}.{rel.to_column}")

        total_rows = overview_df['row_count'].sum() if not overview_df.empty else 0
        total_tables = len(self.df_manager.tables)
        relations_count = len(self.df_manager.relations)
        total_memory = overview_df['memory_usage_mb'].sum() if not overview_df.empty else 0

        summary = (f"–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å–æ–¥–µ—Ä–∂–∏—Ç {total_tables} —Ç–∞–±–ª–∏—Ü —Å {total_rows} –∑–∞–ø–∏—Å—è–º–∏. "
                   f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {relations_count} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏. "
                   f"–û–±—â–∏–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏: {total_memory:.2f} MB.")

        return {
            'question': '–û–±—â–∏–π –æ–±–∑–æ—Ä –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö',
            'data': clean_dataframe_for_json(overview_df),
            'summary': summary,
            'analyzed_tables': list(self.df_manager.tables.keys()),
            'relations': relations_info,
            'chart_data': self._prepare_chart_data_safe(overview_df, 'bar', 'table_name', 'row_count')
        }

    def _analyze_single_table(self, table_name: str) -> Dict[str, Any]:
        """–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        if table_name not in self.df_manager.tables:
            # –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ —Ç–∞–±–ª–∏—Ü—É –ø–æ —á–∞—Å—Ç–∏—á–Ω–æ–º—É —Å–æ–≤–ø–∞–¥–µ–Ω–∏—é
            matching_tables = [t for t in self.df_manager.tables.keys()
                               if table_name.lower() in t.lower()]
            if matching_tables:
                table_name = matching_tables[0]
            else:
                return {
                    'question': f'–ê–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}',
                    'error': f'–¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞',
                    'data': [],
                    'summary': f'–¢–∞–±–ª–∏—Ü–∞ {table_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö',
                    'analyzed_tables': []
                }

        df = self.df_manager.tables[table_name]

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        basic_info = {
            'rows': int(len(df)),
            'columns': int(len(df.columns)),
            'memory_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 3)
        }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_stats = {}
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            desc_stats = df[numeric_cols].describe()
            numeric_stats = convert_to_serializable(desc_stats.to_dict())

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        categorical_stats = self._analyze_categorical_columns(df)

        # –ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π
        anomalies = self._detect_anomalies_dataframe(df)

        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
        missing_analysis = self._analyze_missing_values(df)

        # –ö–æ—Ä—Ä–µ–ª—è—Ü–∏–æ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑
        correlations = self._analyze_correlations_single_table(df, table_name)

        logger.info(f"–ó–∞–ø—É—Å–∫ GPT –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã {table_name}")

        gpt_business_insights = self.gpt_analyzer.analyze_data_with_gpt(
            df=df,
            table_name=table_name,
            analysis_type="business_insights"
        )

        gpt_data_quality = self.gpt_analyzer.analyze_data_with_gpt(
            df=df,
            table_name=table_name,
            analysis_type="data_quality"
        )

        # GPT –∞–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        correlation_insights = ""
        if correlations:
            correlation_insights = self.gpt_analyzer.analyze_correlations_with_context(
                correlations, df, table_name
            )

        # –°–æ–∑–¥–∞–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—É—é —Å–≤–æ–¥–∫—É
        summary = f"üéØ **GPT-–ê–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}'**\n\n"
        summary += f"üìä –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è: {len(df):,} –∑–∞–ø–∏—Å–µ–π, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫\n\n"

        if anomalies:
            summary += f"‚ö†Ô∏è –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(anomalies)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π\n"
        if correlations:
            summary += f"üîó –ù–∞–π–¥–µ–Ω–æ {len(correlations)} –∑–Ω–∞—á–∏–º—ã—Ö –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π\n"

        summary += "\nüìà **–ë–∏–∑–Ω–µ—Å-–∏–Ω—Å–∞–π—Ç—ã:**\n" + gpt_business_insights.get('gpt_analysis', '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–æ')

        return {
            'question': f'–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã {table_name}',
            'data': clean_dataframe_for_json(df.head(10)),
            'summary': summary,
            'analyzed_tables': [table_name],
            'gpt_business_insights': gpt_business_insights.get('gpt_analysis', ''),
            'gpt_data_quality': gpt_data_quality.get('gpt_analysis', ''),
            'correlation_insights': correlation_insights,
            'basic_info': basic_info,
            'anomalies': anomalies,
            'correlations': correlations
        }

    def _analyze_business_metrics(self, question: str) -> Dict[str, Any]:
        """–ù–û–í–´–ô –º–µ—Ç–æ–¥: –ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ —Å GPT"""

        tables_mentioned = self._extract_mentioned_tables(question)
        if not tables_mentioned:
            tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]

        main_table = tables_mentioned[0]
        df = self.df_manager.tables[main_table]

        # –í—ã—á–∏—Å–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        metrics = self._calculate_business_metrics(df)

        # GPT –∞–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
        gpt_analysis = self.gpt_analyzer.analyze_data_with_gpt(
            df=df,
            table_name=main_table,
            analysis_type="business_insights",
            context={"metrics": metrics, "question": question}
        )

        # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        metrics_df = pd.DataFrame([metrics])

        summary = f"üöÄ **–ë–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{main_table}'**\n\n"
        summary += gpt_analysis.get('gpt_analysis', 'GPT –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω')

        return {
            'question': question,
            'data': clean_dataframe_for_json(metrics_df),
            'summary': summary,
            'analyzed_tables': [main_table],
            'gpt_insights': gpt_analysis.get('gpt_analysis', ''),
            'business_metrics': metrics
        }

    def _calculate_business_metrics(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–í—ã—á–∏—Å–ª—è–µ—Ç –∫–ª—é—á–µ–≤—ã–µ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫–∏"""

        metrics = {
            'total_records': len(df),
            'data_completeness': round((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 1),
            'unique_entities': {}
        }

        # –ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            for col in numeric_cols:
                if 'value' in col.lower() or 'amount' in col.lower() or 'price' in col.lower():
                    metrics[f'{col}_total'] = float(df[col].sum())
                    metrics[f'{col}_average'] = round(float(df[col].mean()), 2)
                    metrics[f'{col}_median'] = round(float(df[col].median()), 2)

        # –ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        categorical_cols = df.select_dtypes(include=['object']).columns
        for col in categorical_cols:
            if df[col].nunique() < 50:  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
                metrics['unique_entities'][col] = int(df[col].nunique())

        return metrics

    def _analyze_relationships(self) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        if not self.df_manager.relations:
            return {
                'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
                'data': [],
                'summary': '–°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã',
                'analyzed_tables': []
            }

        relationship_data = []

        for relation in self.df_manager.relations:
            try:
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ —Å–≤—è–∑–∏
                left_df = self.df_manager.tables[relation.from_table]
                right_df = self.df_manager.tables[relation.to_table]

                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–≤—è–∑–∏
                left_values = left_df[relation.from_column].dropna()
                right_values = right_df[relation.to_column].dropna()

                left_unique = set(left_values.astype(str))
                right_unique = set(right_values.astype(str))
                common_values = left_unique.intersection(right_unique)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å —Å–≤—è–∑–∏
                left_not_in_right = len(left_unique - right_unique)
                right_not_in_left = len(right_unique - left_unique)

                relationship_strength = len(common_values) / max(len(left_unique), 1) * 100

                relationship_data.append({
                    'from_table': relation.from_table,
                    'to_table': relation.to_table,
                    'from_column': relation.from_column,
                    'to_column': relation.to_column,
                    'common_values_count': int(len(common_values)),
                    'left_unique_count': int(len(left_unique)),
                    'right_unique_count': int(len(right_unique)),
                    'relationship_strength': round(float(relationship_strength), 2),
                    'integrity_issues': int(left_not_in_right),
                    'reverse_integrity_issues': int(right_not_in_left),
                    'relation_type': relation.relation_type
                })

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–≤—è–∑–∏ {relation.from_table}->{relation.to_table}: {e}")

        if not relationship_data:
            return {
                'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
                'data': [],
                'summary': '–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å–≤—è–∑–∏',
                'analyzed_tables': []
            }

        relationships_df = pd.DataFrame(relationship_data)

        # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å–≤—è–∑–∏
        if not relationships_df.empty:
            strongest = relationships_df.loc[relationships_df['relationship_strength'].idxmax()]
            weakest = relationships_df.loc[relationships_df['relationship_strength'].idxmin()]
            avg_strength = relationships_df['relationship_strength'].mean()

            summary = (f"–ù–∞–π–¥–µ–Ω–æ {len(self.df_manager.relations)} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏. "
                       f"–°—Ä–µ–¥–Ω—è—è —Å–∏–ª–∞ —Å–≤—è–∑–µ–π: {avg_strength:.1f}%. "
                       f"–°–∞–º–∞—è —Å–∏–ª—å–Ω–∞—è: {strongest['from_table']}->{strongest['to_table']} ({strongest['relationship_strength']:.1f}%). "
                       f"–°–∞–º–∞—è —Å–ª–∞–±–∞—è: {weakest['from_table']}->{weakest['to_table']} ({weakest['relationship_strength']:.1f}%).")
        else:
            summary = "–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"

        analyzed_tables = list(set([r['from_table'] for r in relationship_data] +
                                   [r['to_table'] for r in relationship_data]))

        return {
            'question': '–ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
            'data': clean_dataframe_for_json(relationships_df),
            'summary': summary,
            'analyzed_tables': analyzed_tables,
            'chart_data': self._prepare_chart_data_safe(relationships_df, 'bar',
                                                        'from_table', 'relationship_strength')
        }

    def _analyze_aggregations(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å –∞–≥—Ä–µ–≥–∞—Ü–∏—è–º–∏"""
        tables_mentioned = self._extract_mentioned_tables(question)

        if not tables_mentioned:
            # –ë–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
            if self.df_manager.tables:
                tables_mentioned = [max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]]
            else:
                return {
                    'question': question,
                    'data': [],
                    'summary': '–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                    'analyzed_tables': []
                }

        results = []
        aggregated_data = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ß–∏—Å–ª–æ–≤—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) > 0:
                agg_funcs = ['count', 'sum', 'mean', 'min', 'max', 'std']
                agg_result = df[numeric_cols].agg(agg_funcs).round(2)

                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
                for col in numeric_cols:
                    col_data = {
                        'table': table_name,
                        'column': col,
                        'type': 'numeric'
                    }

                    for func in agg_funcs:
                        try:
                            value = agg_result.loc[func, col]
                            if pd.isna(value):
                                value = 0
                            col_data[func] = float(value)
                        except:
                            col_data[func] = 0

                    aggregated_data.append(col_data)

                results.append({
                    'table': table_name,
                    'type': 'numeric_aggregation',
                    'data': convert_to_serializable(agg_result)
                })

            # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
            categorical_cols = df.select_dtypes(include=['object']).columns
            if len(categorical_cols) > 0:
                for col in categorical_cols[:3]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    try:
                        value_counts = df[col].value_counts().head(10)
                        if len(value_counts) > 0:
                            for value, count in value_counts.items():
                                aggregated_data.append({
                                    'table': table_name,
                                    'column': col,
                                    'type': 'categorical',
                                    'value': str(value),
                                    'count': int(count),
                                    'percentage': round(float(count / len(df) * 100), 2)
                                })

                            results.append({
                                'table': table_name,
                                'type': 'categorical_aggregation',
                                'column': col,
                                'data': convert_to_serializable(value_counts)
                            })
                    except Exception as e:
                        logger.error(f"–û—à–∏–±–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

        # –°–æ–∑–¥–∞–µ–º –∏—Ç–æ–≥–æ–≤—ã–π DataFrame
        if aggregated_data:
            main_result = pd.DataFrame(aggregated_data)
            summary = f"–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(results)} —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫."
        else:
            main_result = pd.DataFrame()
            summary = "–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∞–≥—Ä–µ–≥–∞—Ü–∏—é –¥–∞–Ω–Ω—ã—Ö"

        return {
            'question': question,
            'data': clean_dataframe_for_json(main_result),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'detailed_results': convert_to_serializable(results),
            'chart_data': self._create_aggregation_chart(main_result)
        }

    def _analyze_trends(self, question: str) -> Dict[str, Any]:
        """–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑"""
        trend_results = []

        for table_name, df in self.df_manager.tables.items():
            # –ò—â–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏
            date_cols = self._find_date_columns(df)

            if date_cols:
                date_col = date_cols[0]
                try:
                    df_copy = df.copy()
                    df_copy[date_col] = pd.to_datetime(df_copy[date_col], errors='coerce')

                    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ –¥–∞—Ç–∞–º–∏
                    df_clean = df_copy.dropna(subset=[date_col])

                    if len(df_clean) > 0:
                        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –ø–µ—Ä–∏–æ–¥–∞–º (–º–µ—Å—è—Ü–∞–º)
                        df_clean['period'] = df_clean[date_col].dt.to_period('M')
                        trend_data = df_clean.groupby('period').size().reset_index(name='count')
                        trend_data['period_str'] = trend_data['period'].astype(str)

                        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ç—Ä–µ–Ω–¥–∞
                        if len(trend_data) > 1:
                            # –ü—Ä–æ—Å—Ç–æ–π –∞–Ω–∞–ª–∏–∑ —Ä–æ—Å—Ç–∞/–ø–∞–¥–µ–Ω–∏—è
                            first_val = int(trend_data['count'].iloc[0])
                            last_val = int(trend_data['count'].iloc[-1])
                            trend_direction = "—Ä–æ—Å—Ç" if last_val > first_val else "–ø–∞–¥–µ–Ω–∏–µ"
                            trend_percent = abs((last_val - first_val) / first_val * 100) if first_val > 0 else 0
                        else:
                            trend_direction = "–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"
                            trend_percent = 0

                        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º timestamp –≤ —Å—Ç—Ä–æ–∫–∏
                        min_date = df_clean[date_col].min()
                        max_date = df_clean[date_col].max()
                        try:
                            date_range = f"{min_date.strftime('%Y-%m-%d')} - {max_date.strftime('%Y-%m-%d')}"
                        except:
                            date_range = f"{str(min_date)[:10]} - {str(max_date)[:10]}"

                        trend_results.append({
                            'table': table_name,
                            'date_column': date_col,
                            'trend_data': clean_dataframe_for_json(trend_data),
                            'total_records': int(len(df_clean)),
                            'periods_count': int(len(trend_data)),
                            'trend_direction': trend_direction,
                            'trend_percent': round(float(trend_percent), 1),
                            'date_range': date_range
                        })

                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –¥–ª—è {table_name}: {e}")

        if trend_results:
            # –ë–µ—Ä–µ–º —Å–∞–º—ã–π –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã–π —Ç—Ä–µ–Ω–¥
            main_trend = max(trend_results, key=lambda x: x['total_records'])
            summary = (f"–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω –≤ {len(trend_results)} —Ç–∞–±–ª–∏—Ü–∞—Ö. "
                       f"–û—Å–Ω–æ–≤–Ω–æ–π —Ç—Ä–µ–Ω–¥ –≤ —Ç–∞–±–ª–∏—Ü–µ '{main_trend['table']}' –ø–æ –∫–æ–ª–æ–Ω–∫–µ '{main_trend['date_column']}': "
                       f"{main_trend['trend_direction']} –Ω–∞ {main_trend['trend_percent']:.1f}% –∑–∞ –ø–µ—Ä–∏–æ–¥ {main_trend['date_range']}.")

            main_data = main_trend['trend_data']
        else:
            main_data = []
            summary = "–í—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        analyzed_tables = [r['table'] for r in trend_results]

        return {
            'question': question,
            'data': main_data,
            'summary': summary,
            'analyzed_tables': analyzed_tables,
            'all_trends': trend_results,
            'chart_data': self._prepare_chart_data_safe_from_list(main_data, 'line', 'period_str', 'count')
        }

    def _analyze_correlations(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π"""
        tables_mentioned = self._extract_mentioned_tables(question)

        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        all_correlations = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]
            correlations = self._analyze_correlations_single_table(df, table_name)
            all_correlations.extend(correlations)

        if all_correlations:
            # –°–æ–∑–¥–∞–µ–º DataFrame —Å –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º–∏
            corr_df = pd.DataFrame(all_correlations)

            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            corr_df['abs_correlation'] = abs(corr_df['correlation'])
            corr_df = corr_df.sort_values('abs_correlation', ascending=False)
            corr_df = corr_df.drop('abs_correlation', axis=1)  # –£–±–∏—Ä–∞–µ–º –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—É—é –∫–æ–ª–æ–Ω–∫—É

            # –ù–∞—Ö–æ–¥–∏–º —Å–∞–º—ã–µ —Å–∏–ª—å–Ω—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
            strong_correlations = corr_df[abs(corr_df['correlation']) > 0.7]

            summary = (f"–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü–∞—Ö. "
                       f"–ù–∞–π–¥–µ–Ω–æ {len(all_correlations)} –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π, "
                       f"–∏–∑ –Ω–∏—Ö {len(strong_correlations)} —Å–∏–ª—å–Ω—ã—Ö (>0.7).")

        else:
            corr_df = pd.DataFrame()
            summary = "–ó–Ω–∞—á–∏–º—ã–µ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã"

        return {
            'question': question,
            'data': clean_dataframe_for_json(corr_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'chart_data': self._prepare_chart_data_safe(corr_df, 'scatter', 'column1', 'correlation')
        }

    def _analyze_comparison(self, question: str) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"""
        tables_mentioned = self._extract_mentioned_tables(question)

        if len(tables_mentioned) < 2:
            # –ï—Å–ª–∏ —É–ø–æ–º—è–Ω—É—Ç–∞ –æ–¥–Ω–∞ —Ç–∞–±–ª–∏—Ü–∞, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–æ–Ω–∫–∏ –≤–Ω—É—Ç—Ä–∏ –Ω–µ—ë
            if len(tables_mentioned) == 1:
                return self._compare_columns_in_table(tables_mentioned[0], question)
            else:
                # –ë–µ—Ä–µ–º –¥–≤–µ —Å–∞–º—ã–µ –±–æ–ª—å—à–∏–µ —Ç–∞–±–ª–∏—Ü—ã
                if len(self.df_manager.tables) >= 2:
                    largest_tables = sorted(self.df_manager.tables.items(),
                                            key=lambda x: len(x[1]), reverse=True)[:2]
                    tables_mentioned = [t[0] for t in largest_tables]
                else:
                    return {
                        'question': question,
                        'data': [],
                        'summary': '–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ç–∞–±–ª–∏—Ü –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è',
                        'analyzed_tables': []
                    }

        comparison_data = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]

            # –ë–∞–∑–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ç–∞–±–ª–∏—Ü—ã
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            categorical_cols = df.select_dtypes(include=['object']).columns

            stats = {
                'table': table_name,
                'rows': int(len(df)),
                'columns': int(len(df.columns)),
                'numeric_columns': int(len(numeric_cols)),
                'categorical_columns': int(len(categorical_cols)),
                'null_values': int(df.isnull().sum().sum()),
                'memory_mb': round(float(df.memory_usage(deep=True).sum() / 1024 / 1024), 2)
            }

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ —á–∏—Å–ª–æ–≤—ã–º –∫–æ–ª–æ–Ω–∫–∞–º
            if len(numeric_cols) > 0:
                try:
                    numeric_desc = df[numeric_cols].describe()
                    stats.update({
                        'avg_mean': round(float(numeric_desc.loc['mean'].mean()), 2),
                        'avg_std': round(float(numeric_desc.loc['std'].mean()), 2),
                        'avg_min': round(float(numeric_desc.loc['min'].mean()), 2),
                        'avg_max': round(float(numeric_desc.loc['max'].mean()), 2)
                    })
                except:
                    stats.update({
                        'avg_mean': 0,
                        'avg_std': 0,
                        'avg_min': 0,
                        'avg_max': 0
                    })

            comparison_data.append(stats)

        comparison_df = pd.DataFrame(comparison_data)

        if len(comparison_data) >= 2:
            # –ù–∞—Ö–æ–¥–∏–º —Ä–∞–∑–ª–∏—á–∏—è
            table1, table2 = comparison_data[0], comparison_data[1]
            differences = []

            for key in ['rows', 'columns', 'numeric_columns', 'categorical_columns']:
                if key in table1 and key in table2:
                    diff = abs(table1[key] - table2[key])
                    if diff > 0:
                        differences.append(f"{key}: {table1[key]} vs {table2[key]} (—Ä–∞–∑–Ω–∏—Ü–∞: {diff})")

            summary = (f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü {' vs '.join(tables_mentioned)}. "
                       f"–û—Å–Ω–æ–≤–Ω—ã–µ —Ä–∞–∑–ª–∏—á–∏—è: {'; '.join(differences[:3])}.")
        else:
            summary = "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"

        return {
            'question': question,
            'data': clean_dataframe_for_json(comparison_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'chart_data': self._prepare_chart_data_safe(comparison_df, 'bar', 'table', 'rows')
        }

    def _analyze_anomalies(self, question: str) -> Dict[str, Any]:
        """–°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∞–Ω–æ–º–∞–ª–∏–π"""
        tables_mentioned = self._extract_mentioned_tables(question)

        if not tables_mentioned:
            tables_mentioned = list(self.df_manager.tables.keys())

        all_anomalies = []

        for table_name in tables_mentioned:
            df = self.df_manager.tables[table_name]
            anomalies = self._detect_anomalies_dataframe(df)

            for anomaly in anomalies:
                anomaly['table'] = table_name
                all_anomalies.append(anomaly)

        if all_anomalies:
            anomalies_df = pd.DataFrame(all_anomalies)
            total_anomalies = sum(a['count'] for a in all_anomalies)

            summary = (f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {len(all_anomalies)} —Ç–∏–ø–æ–≤ –∞–Ω–æ–º–∞–ª–∏–π –≤ {len(tables_mentioned)} —Ç–∞–±–ª–∏—Ü–∞—Ö. "
                       f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∞–Ω–æ–º–∞–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {total_anomalies}.")
        else:
            anomalies_df = pd.DataFrame()
            summary = "–ê–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–∞–Ω–Ω—ã—Ö –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã"

        return {
            'question': question,
            'data': clean_dataframe_for_json(anomalies_df),
            'summary': summary,
            'analyzed_tables': tables_mentioned,
            'chart_data': self._prepare_chart_data_safe(anomalies_df, 'bar', 'table', 'count')
        }

    def _analyze_general(self, question: str) -> Dict[str, Any]:
        """–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –Ω–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
        # –ü—ã—Ç–∞–µ–º—Å—è –∏–∑–≤–ª–µ—á—å —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        tables_mentioned = self._extract_mentioned_tables(question)

        if not tables_mentioned:
            # –ë–µ—Ä–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
            if self.df_manager.tables:
                largest_table = max(self.df_manager.tables.items(), key=lambda x: len(x[1]))
                tables_mentioned = [largest_table[0]]
            else:
                return {
                    'question': question,
                    'data': [],
                    'summary': '–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞',
                    'analyzed_tables': []
                }

        # –í—ã–ø–æ–ª–Ω—è–µ–º –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–≤–æ–π —É–ø–æ–º—è–Ω—É—Ç–æ–π —Ç–∞–±–ª–∏—Ü—ã
        table_name = tables_mentioned[0]
        df = self.df_manager.tables[table_name]

        # –°–æ–∑–¥–∞–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        general_stats = {
            'table_name': table_name,
            'total_rows': int(len(df)),
            'total_columns': int(len(df.columns)),
            'columns_list': list(df.columns),
            'data_types': convert_to_serializable(df.dtypes.value_counts().to_dict()),
            'missing_values': int(df.isnull().sum().sum()),
            'duplicate_rows': int(df.duplicated().sum())
        }

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä—ã –¥–∞–Ω–Ω—ã—Ö
        sample_data = clean_dataframe_for_json(df.head(10))

        summary = (f"–û–±—â–∏–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}': {len(df)} –∑–∞–ø–∏—Å–µ–π, "
                   f"{len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫. "
                   f"–ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {general_stats['missing_values']}, "
                   f"–¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {general_stats['duplicate_rows']}.")

        return {
            'question': question,
            'data': sample_data,
            'summary': summary,
            'analyzed_tables': [table_name],
            'general_stats': general_stats,
            'chart_data': None
        }

    # –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–æ–¥—ã
    def _categorize_question(self, question: str) -> str:
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç —Ç–∏–ø –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower()

        if any(keyword in question_lower for keyword in ['–æ–±–∑–æ—Ä', '–æ–±—â–∏–π', '—Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞', 'overview', '—Å—Ç—Ä—É–∫—Ç—É—Ä–∞']):
            return 'overview'
        elif any(keyword in question_lower for keyword in ['—Ç–∞–±–ª–∏—Ü–∞', 'table', '–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã']):
            return 'table_analysis'
        elif any(keyword in question_lower for keyword in ['—Å–≤—è–∑—å', '—Å–≤—è–∑–∏', '–æ—Ç–Ω–æ—à–µ–Ω–∏—è', 'relation']):
            return 'relationship_analysis'
        elif any(keyword in question_lower for keyword in
                 ['—Å—É–º–º–∞', '—Å—Ä–µ–¥–Ω–µ–µ', '–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ', '–∞–≥—Ä–µ–≥–∞—Ü–∏—è', '–≥—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞']):
            return 'aggregation'
        elif any(keyword in question_lower for keyword in ['—Ç—Ä–µ–Ω–¥', '–¥–∏–Ω–∞–º–∏–∫–∞', '–≤—Ä–µ–º—è', '–≤—Ä–µ–º–µ–Ω–Ω–æ–π', '–¥–∞—Ç–∞']):
            return 'trend_analysis'
        elif any(keyword in question_lower for keyword in ['–∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è', '–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å', '—Å–≤—è–∑–∞–Ω–Ω–æ—Å—Ç—å']):
            return 'correlation'
        elif any(keyword in question_lower for keyword in ['—Å—Ä–∞–≤–Ω–µ–Ω–∏–µ', '—Å—Ä–∞–≤–Ω–∏—Ç—å', '—Ä–∞–∑–ª–∏—á–∏—è', 'vs']):
            return 'comparison'
        elif any(keyword in question_lower for keyword in ['–∞–Ω–æ–º–∞–ª–∏–∏', '–≤—ã–±—Ä–æ—Å—ã', 'anomaly', 'outlier']):
            return 'anomalies'
        else:
            return 'general'

    def _extract_table_name(self, question: str) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –∏–∑ –≤–æ–ø—Ä–æ—Å–∞"""
        question_lower = question.lower()

        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for table_name in self.df_manager.tables.keys():
            if table_name.lower() in question_lower:
                return table_name

        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for table_name in self.df_manager.tables.keys():
            table_words = table_name.lower().split('_')
            if any(word in question_lower for word in table_words if len(word) > 3):
                return table_name

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É –∫–∞–∫ fallback
        if self.df_manager.tables:
            return max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]
        else:
            return "unknown"

    def _extract_mentioned_tables(self, question: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ —É–ø–æ–º—è–Ω—É—Ç—ã–µ —Ç–∞–±–ª–∏—Ü—ã"""
        mentioned = []
        question_lower = question.lower()

        for table_name in self.df_manager.tables.keys():
            if table_name.lower() in question_lower:
                mentioned.append(table_name)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å–∞–º—É—é –±–æ–ª—å—à—É—é —Ç–∞–±–ª–∏—Ü—É
        if not mentioned and self.df_manager.tables:
            largest_table = max(self.df_manager.tables.items(), key=lambda x: len(x[1]))[0]
            mentioned = [largest_table]

        return mentioned

    def _find_date_columns(self, df: pd.DataFrame) -> List[str]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–æ–ª–æ–Ω–∫–∏ —Å –¥–∞—Ç–∞–º–∏"""
        date_cols = []

        for col in df.columns:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            if any(keyword in col.lower() for keyword in ['date', 'time', 'created', 'updated', 'timestamp']):
                date_cols.append(col)
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ —Ç–∏–ø—É –¥–∞–Ω–Ω—ã—Ö
            elif df[col].dtype == 'datetime64[ns]':
                date_cols.append(col)
            # –ü—ã—Ç–∞–µ–º—Å—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∫ –¥–∞—Ç–µ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –∑–Ω–∞—á–µ–Ω–∏–π
            else:
                try:
                    sample = df[col].dropna().head(5)
                    if len(sample) > 0:
                        pd.to_datetime(sample.iloc[0], errors='raise')
                        date_cols.append(col)
                except:
                    continue

        return date_cols

    def _analyze_categorical_columns(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫"""
        categorical_analysis = {}
        categorical_cols = df.select_dtypes(include=['object']).columns

        for col in categorical_cols:
            try:
                value_counts = df[col].value_counts()
                categorical_analysis[col] = {
                    'unique_count': int(len(value_counts)),
                    'top_values': convert_to_serializable(value_counts.head(5).to_dict()),
                    'null_count': int(df[col].isnull().sum()),
                    'most_common': str(value_counts.index[0]) if len(value_counts) > 0 else 'N/A',
                    'most_common_count': int(value_counts.iloc[0]) if len(value_counts) > 0 else 0
                }
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")
                categorical_analysis[col] = {
                    'unique_count': 0,
                    'top_values': {},
                    'null_count': int(df[col].isnull().sum()),
                    'most_common': 'Error',
                    'most_common_count': 0
                }

        return categorical_analysis

    def _detect_anomalies_dataframe(self, df: pd.DataFrame) -> List[Dict[str, Any]]:
        """–û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π –≤ DataFrame"""
        anomalies = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        for col in numeric_cols:
            try:
                if df[col].count() > 10:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1

                    if IQR > 0:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ 0
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]

                        if len(outliers) > 0:
                            sample_values = outliers[col].head(5).tolist()
                            anomalies.append({
                                'column': col,
                                'count': int(len(outliers)),
                                'percentage': round(float(len(outliers) / len(df) * 100), 2),
                                'lower_bound': round(float(lower_bound), 2),
                                'upper_bound': round(float(upper_bound), 2),
                                'sample_values': [float(v) for v in sample_values]
                            })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –∞–Ω–æ–º–∞–ª–∏–π –≤ –∫–æ–ª–æ–Ω–∫–µ {col}: {e}")

        return anomalies

    def _analyze_missing_values(self, df: pd.DataFrame) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π"""
        try:
            missing_data = df.isnull().sum()
            total_missing = missing_data.sum()

            missing_analysis = {
                'total_missing': int(total_missing),
                'missing_percentage': round(float(total_missing / (len(df) * len(df.columns)) * 100), 2),
                'columns_with_missing': {}
            }

            for col, missing_count in missing_data.items():
                if missing_count > 0:
                    missing_analysis['columns_with_missing'][col] = {
                        'count': int(missing_count),
                        'percentage': round(float(missing_count / len(df) * 100), 2)
                    }

            return missing_analysis
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π: {e}")
            return {
                'total_missing': 0,
                'missing_percentage': 0,
                'columns_with_missing': {}
            }

    def _analyze_correlations_single_table(self, df: pd.DataFrame, table_name: str = None) -> List[Dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –≤ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü–µ"""
        correlations = []
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) >= 2:
            try:
                corr_matrix = df[numeric_cols].corr()

                for i in range(len(corr_matrix.columns)):
                    for j in range(i + 1, len(corr_matrix.columns)):
                        corr_value = corr_matrix.iloc[i, j]
                        if not pd.isna(corr_value) and abs(corr_value) > 0.5:  # –ó–Ω–∞—á–∏–º–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è
                            correlations.append({
                                'table': table_name or 'unknown',
                                'column1': corr_matrix.columns[i],
                                'column2': corr_matrix.columns[j],
                                'correlation': round(float(corr_value), 3),
                                'strength': '—Å–∏–ª—å–Ω–∞—è' if abs(corr_value) > 0.7 else '—É–º–µ—Ä–µ–Ω–Ω–∞—è',
                                'direction': '–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è' if corr_value > 0 else '–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è'
                            })
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π: {e}")

        return correlations

    def _compare_columns_in_table(self, table_name: str, question: str) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã"""
        df = self.df_manager.tables[table_name]

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        comparison_data = []

        for col in numeric_cols:
            try:
                col_stats = {
                    'column': col,
                    'count': int(df[col].count()),
                    'mean': round(float(df[col].mean()), 2),
                    'median': round(float(df[col].median()), 2),
                    'std': round(float(df[col].std()), 2),
                    'min': round(float(df[col].min()), 2),
                    'max': round(float(df[col].max()), 2),
                    'range': round(float(df[col].max() - df[col].min()), 2)
                }
                comparison_data.append(col_stats)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –∫–æ–ª–æ–Ω–∫–∏ {col}: {e}")

        comparison_df = pd.DataFrame(comparison_data)

        if not comparison_df.empty:
            # –ù–∞—Ö–æ–¥–∏–º –∫–æ–ª–æ–Ω–∫—É —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º–∏ –∏ –Ω–∞–∏–º–µ–Ω—å—à–∏–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏
            max_mean_col = comparison_df.loc[comparison_df['mean'].idxmax(), 'column']
            min_mean_col = comparison_df.loc[comparison_df['mean'].idxmin(), 'column']

            summary = (f"–°—Ä–∞–≤–Ω–µ–Ω–∏–µ {len(numeric_cols)} —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –≤ —Ç–∞–±–ª–∏—Ü–µ '{table_name}'. "
                       f"–ù–∞–∏–±–æ–ª—å—à–µ–µ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–æ–ª–æ–Ω–∫–µ '{max_mean_col}', "
                       f"–Ω–∞–∏–º–µ–Ω—å—à–µ–µ –≤ '{min_mean_col}'.")
        else:
            summary = f"–í —Ç–∞–±–ª–∏—Ü–µ '{table_name}' –Ω–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"

        return {
            'question': question,
            'data': clean_dataframe_for_json(comparison_df),
            'summary': summary,
            'analyzed_tables': [table_name],
            'chart_data': self._prepare_chart_data_safe(comparison_df, 'bar', 'column', 'mean')
        }

    def _prepare_chart_data_safe(self, df: pd.DataFrame, chart_type: str, x_col: str, y_col: str) -> Optional[
        Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞"""
        if df is None or df.empty or x_col not in df.columns or y_col not in df.columns:
            return None

        try:
            x_data = df[x_col].head(20).astype(str).tolist()
            y_data = []

            for val in df[y_col].head(20):
                if pd.isna(val):
                    y_data.append(0)
                else:
                    y_data.append(float(val))

            return {
                'type': chart_type,
                'x': x_data,
                'y': y_data,
                'title': f'{y_col} –ø–æ {x_col}'
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞: {e}")
            return None

    def _prepare_chart_data_safe_from_list(self, data: List[Dict], chart_type: str, x_col: str, y_col: str) -> Optional[
        Dict[str, Any]]:
        """–ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑ —Å–ø–∏—Å–∫–∞"""
        if not data or not isinstance(data, list) or len(data) == 0:
            return None

        try:
            x_data = []
            y_data = []

            for item in data[:20]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                if isinstance(item, dict) and x_col in item and y_col in item:
                    x_data.append(str(item[x_col]))

                    y_val = item[y_col]
                    if pd.isna(y_val):
                        y_data.append(0)
                    else:
                        y_data.append(float(y_val))

            if x_data and y_data:
                return {
                    'type': chart_type,
                    'x': x_data,
                    'y': y_data,
                    'title': f'{y_col} –ø–æ {x_col}'
                }
            else:
                return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥—Ä–∞—Ñ–∏–∫–∞ –∏–∑ —Å–ø–∏—Å–∫–∞: {e}")
            return None

    def _create_table_chart(self, df: pd.DataFrame, table_name: str) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–∞–±–ª–∏—Ü—ã"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns

            if len(numeric_cols) > 0:
                # –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º–∞ –¥–ª—è –ø–µ—Ä–≤–æ–π —á–∏—Å–ª–æ–≤–æ–π –∫–æ–ª–æ–Ω–∫–∏
                col = numeric_cols[0]
                data_values = df[col].dropna().head(100).tolist()

                return {
                    'type': 'histogram',
                    'data': [float(x) for x in data_values],
                    'title': f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col} –≤ {table_name}'
                }
            else:
                # –ö—Ä—É–≥–æ–≤–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ –¥–ª—è –ø–µ—Ä–≤–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω–æ–π –∫–æ–ª–æ–Ω–∫–∏
                categorical_cols = df.select_dtypes(include=['object']).columns
                if len(categorical_cols) > 0:
                    col = categorical_cols[0]
                    value_counts = df[col].value_counts().head(10)

                    return {
                        'type': 'pie',
                        'labels': [str(x) for x in value_counts.index.tolist()],
                        'values': [int(x) for x in value_counts.values.tolist()],
                        'title': f'–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ {col} –≤ {table_name}'
                    }

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã: {e}")
            return None

    def _create_aggregation_chart(self, df: pd.DataFrame) -> Optional[Dict[str, Any]]:
        """–°–æ–∑–¥–∞–µ—Ç –≥—Ä–∞—Ñ–∏–∫ –¥–ª—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        if df.empty:
            return None

        try:
            if 'table' in df.columns:
                if 'count' in df.columns:
                    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
                    table_counts = df.groupby('table')['count'].sum().reset_index()

                    return {
                        'type': 'bar',
                        'x': [str(x) for x in table_counts['table'].tolist()],
                        'y': [int(x) for x in table_counts['count'].tolist()],
                        'title': '–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º'
                    }
                elif 'sum' in df.columns:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º sum
                    table_sums = df.groupby('table')['sum'].sum().reset_index()

                    return {
                        'type': 'bar',
                        'x': [str(x) for x in table_sums['table'].tolist()],
                        'y': [float(x) for x in table_sums['sum'].tolist()],
                        'title': '–°—É–º–º—ã –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º'
                    }

            return None

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏: {e}")
            return None
