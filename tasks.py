# tasks.py - –ø–æ–ª–Ω–∞—è DataFrame —Å–∏—Å—Ç–µ–º–∞
import logging
from celery.exceptions import Ignore
from sqlalchemy import create_engine
import crud
import database
from services.dataframe_manager import DataFrameManager
from services.dataframe_analyzer import DataFrameAnalyzer
from utils.json_serializer import convert_to_serializable
from celery_worker import celery_app
from datetime import datetime
import json
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ utils
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, time_limit=7200, name='tasks.generate_dataframe_report')
def generate_dataframe_report(self, connection_id: int, user_id: int, report_id: int, max_questions: int = 15):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ DataFrame –±–µ–∑ SQL
    """
    db_session = next(database.get_db())

    try:
        logger.info(f"[DATAFRAME REPORT] –ó–∞–ø—É—Å–∫ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, –æ—Ç—á–µ—Ç {report_id}")

        # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
        self.update_state(
            state='INITIALIZING',
            meta={'progress': '–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataFrame —Å–∏—Å—Ç–µ–º—ã...'}
        )

        connection_string = crud.get_decrypted_connection_string(db_session, connection_id, user_id)
        if not connection_string:
            raise ValueError("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        engine = create_engine(connection_string, connect_args={'connect_timeout': 60})

        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í DATAFRAME ===
        self.update_state(
            state='LOADING_DATA',
            meta={'progress': '–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü –≤ –ø–∞–º—è—Ç—å...', 'progress_percentage': 10}
        )

        df_manager = DataFrameManager(engine)
        tables_loaded = df_manager.load_all_tables(max_rows_per_table=50000)  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏

        if not tables_loaded:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞.")

        logger.info(f"[DATAFRAME REPORT] –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(tables_loaded)} —Ç–∞–±–ª–∏—Ü")

        # === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–ú–Ø–¢–ò ===
        self.update_state(
            state='OPTIMIZING',
            meta={'progress': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏...', 'progress_percentage': 20}
        )

        df_manager.optimize_memory()

        # === –°–û–ó–î–ê–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê–¢–û–†–ê ===
        analyzer = DataFrameAnalyzer(df_manager)

        # === –°–û–ó–î–ê–ù–ò–ï –ü–õ–ê–ù–ê –ê–ù–ê–õ–ò–ó–ê ===
        self.update_state(
            state='PLANNING',
            meta={'progress': '–°–æ–∑–¥–∞–Ω–∏–µ –ø–ª–∞–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ DataFrame...', 'progress_percentage': 25}
        )

        analysis_plan = _create_dataframe_analysis_plan(df_manager, max_questions)
        logger.info(f"[DATAFRAME REPORT] –ü–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞: {len(analysis_plan)} –≤–æ–ø—Ä–æ—Å–æ–≤")

        # === –í–´–ü–û–õ–ù–ï–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê ===
        session_memory = []
        successful_analyses = 0

        for i, question in enumerate(analysis_plan):
            if i >= max_questions:
                break

            progress = 25 + (i + 1) / min(len(analysis_plan), max_questions) * 65  # 25-90%

            self.update_state(
                state='ANALYZING',
                meta={
                    'progress': f'–ê–Ω–∞–ª–∏–∑ {i + 1}/{min(len(analysis_plan), max_questions)}: {question}',
                    'progress_percentage': progress
                }
            )

            logger.info(f"[DATAFRAME REPORT] –ê–Ω–∞–ª–∏–∑ {i + 1}: {question}")

            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑ –±–µ–∑ SQL
                result = analyzer.analyze_question(question)

                if not result.get('error'):
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                    data_preview = result.get('data', None)
                    if data_preview:
                        if hasattr(data_preview, 'head'):
                            # –≠—Ç–æ DataFrame
                            data_preview = convert_to_serializable(data_preview.head(10).to_dict('records'))
                        elif isinstance(data_preview, (list, dict)):
                            # –£–∂–µ —Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Å–ª–æ–≤–∞—Ä—å
                            data_preview = convert_to_serializable(data_preview)
                    else:
                        data_preview = []

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    additional_info = result.get('additional_info', {})
                    if additional_info:
                        additional_info = convert_to_serializable(additional_info)

                    finding_entry = {
                        'question': str(question),
                        'summary': str(result.get('summary', '')),
                        'data_preview': data_preview,
                        'chart_data': convert_to_serializable(result.get('chart_data')),
                        'analyzed_tables': list(result.get('analyzed_tables', [])),
                        'method': 'dataframe',
                        'additional_info': additional_info,
                        'timestamp': datetime.now().isoformat()
                    }

                    session_memory.append(finding_entry)
                    successful_analyses += 1

                    logger.info(f"[DATAFRAME REPORT] ‚úÖ –ê–Ω–∞–ª–∏–∑ {i + 1} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

                else:
                    error_msg = str(result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'))
                    logger.error(f"[DATAFRAME REPORT] ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {i + 1}: {error_msg}")

                    session_memory.append({
                        'question': str(question),
                        'summary': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}',
                        'data_preview': [],
                        'error': error_msg,
                        'method': 'dataframe',
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as analysis_error:
                error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(analysis_error)}"
                logger.error(f"[DATAFRAME REPORT] üí• {error_msg}")

                session_memory.append({
                    'question': str(question),
                    'summary': error_msg,
                    'data_preview': [],
                    'error': error_msg,
                    'method': 'dataframe',
                    'timestamp': datetime.now().isoformat()
                })

        # === –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê ===
        self.update_state(
            state='FINALIZING',
            meta={'progress': '–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞...', 'progress_percentage': 90}
        )

        logger.info(
            f"[DATAFRAME REPORT] –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {successful_analyses}/{len(session_memory)} —É—Å–ø–µ—à–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤")

        # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
        table_summary = df_manager.get_table_summary()
        memory_info = df_manager.get_memory_usage()

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
        final_report = {
            "executive_summary": str(_create_executive_summary(session_memory, table_summary, successful_analyses)),
            "detailed_findings": convert_to_serializable(session_memory),
            "method": "pure_dataframe",
            "tables_info": convert_to_serializable(table_summary['tables']),
            "relations_info": convert_to_serializable(table_summary['relations']),
            "analysis_stats": {
                "questions_processed": int(len(session_memory)),
                "successful_analyses": int(successful_analyses),
                "failed_analyses": int(len(session_memory) - successful_analyses),
                "tables_analyzed": int(table_summary['total_tables']),
                "relations_found": int(table_summary['total_relations']),
                "total_memory_mb": round(float(table_summary['total_memory_mb']), 2),
                "analysis_duration_minutes": "N/A",  # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
                "success_rate_percent": round(float(successful_analyses / max(len(session_memory), 1) * 100), 1)
            },
            "memory_usage": convert_to_serializable(memory_info),
            "recommendations": [str(r) for r in
                                _generate_recommendations(session_memory, table_summary, successful_analyses)],
            "report_metadata": {
                "created_at": datetime.now().isoformat(),
                "user_id": int(user_id),
                "connection_id": int(connection_id),
                "report_version": "2.0_dataframe",
                "max_questions_requested": int(max_questions)
            }
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ—Ç—á–µ—Ç–∞ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        try:
            report_json = json.dumps(final_report, ensure_ascii=False)
            report_size_mb = len(report_json.encode('utf-8')) / 1024 / 1024
            logger.info(f"[DATAFRAME REPORT] –†–∞–∑–º–µ—Ä –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {report_size_mb:.2f} MB")

            # –ï—Å–ª–∏ –æ—Ç—á–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if report_size_mb > 10:  # 10MB –ª–∏–º–∏—Ç
                logger.warning(f"[DATAFRAME REPORT] –û—Ç—á–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({report_size_mb:.2f} MB), —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ")
                final_report = _trim_large_report(final_report)

        except Exception as json_error:
            logger.error(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {json_error}")
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            final_report = convert_to_serializable(final_report)

        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –û–¢–ß–ï–¢–ê ===
        self.update_state(
            state='SAVING',
            meta={'progress': '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...', 'progress_percentage': 95}
        )

        try:
            crud.update_report(db_session, report_id, "COMPLETED", final_report)
            logger.info(f"[DATAFRAME REPORT] ‚úÖ –û—Ç—á–µ—Ç {report_id} —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as save_error:
            logger.error(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {save_error}")

            # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
            try:
                db_session.rollback()
            except:
                pass

            # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            simplified_report = {
                "executive_summary": final_report["executive_summary"],
                "method": "pure_dataframe",
                "analysis_stats": final_report["analysis_stats"],
                "error": f"–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å: {str(save_error)}",
                "report_metadata": final_report["report_metadata"]
            }

            crud.update_report(db_session, report_id, "COMPLETED", simplified_report)
            logger.warning(f"[DATAFRAME REPORT] –°–æ—Ö—Ä–∞–Ω–µ–Ω —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç {report_id}")

        self.update_state(
            state='SUCCESS',
            meta={'progress': 'DataFrame-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!', 'progress_percentage': 100}
        )

        return {
            "status": "success",
            "report_id": report_id,
            "questions_processed": len(session_memory),
            "successful_analyses": successful_analyses,
            "tables_loaded": len(tables_loaded)
        }

    except Exception as e:
        logger.error(f"[DATAFRAME REPORT ERROR] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)

        # –û—Ç–∫–∞—Ç—ã–≤–∞–µ–º —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
        try:
            db_session.rollback()
        except:
            pass

        error_report = {
            "error": str(e),
            "method": "dataframe",
            "stage": "critical_error",
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "connection_id": connection_id
        }

        try:
            crud.update_report(db_session, report_id, "FAILED", error_report)
            logger.info(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –æ—Ç—á–µ—Ç {report_id}")
        except Exception as save_error:
            logger.error(f"[DATAFRAME REPORT] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—à–∏–±–∫—É: {save_error}")

        # –£–≤–µ–¥–æ–º–ª—è–µ–º Celery –æ–± –æ—à–∏–±–∫–µ
        self.update_state(
            state='FAILURE',
            meta={
                'progress': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}',
                'error': str(e),
                'progress_percentage': 0
            }
        )

        raise e

    finally:
        try:
            db_session.close()
        except:
            pass


def _create_dataframe_analysis_plan(df_manager: DataFrameManager, max_questions: int) -> List[str]:
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö DataFrame"""

    plan = [
        "–û–±—â–∏–π –æ–±–∑–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏"
    ]

    table_names = list(df_manager.tables.keys())

    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    tables_by_size = sorted(df_manager.tables.items(), key=lambda x: len(x[1]), reverse=True)

    for i, (table_name, df) in enumerate(tables_by_size[:min(6, len(tables_by_size))]):
        if i < 3:  # –ü–µ—Ä–≤—ã–µ 3 —Ç–∞–±–ª–∏—Ü—ã - –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑
            plan.append(f"–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}' —Å –ø–æ–∏—Å–∫–æ–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –∞–Ω–æ–º–∞–ª–∏–π")
        else:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ - —ç–∫—Å–ø—Ä–µ—Å—Å –∞–Ω–∞–ª–∏–∑
            plan.append(f"–≠–∫—Å–ø—Ä–µ—Å—Å-–∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã '{table_name}'")

    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if df_manager.relations:
        plan.append("–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –∏ –∏—Ö —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")

    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–∏–∑—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–∞–Ω–Ω—ã—Ö
    has_numeric_data = any(
        len(df.select_dtypes(include=[np.number]).columns) > 0
        for df in df_manager.tables.values()
    )

    has_datetime_data = any(
        any('date' in col.lower() or 'time' in col.lower() for col in df.columns)
        for df in df_manager.tables.values()
    )

    if has_numeric_data:
        plan.extend([
            "–ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —á–∏—Å–ª–æ–≤—ã–º –º–µ—Ç—Ä–∏–∫–∞–º",
            "–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É —á–∏—Å–ª–æ–≤—ã–º–∏ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏",
            "–ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤—ã–±—Ä–æ—Å–æ–≤ –≤ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö"
        ])

    if has_datetime_data:
        plan.append("–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑ –∏ –ø–æ–∏—Å–∫ —Ç—Ä–µ–Ω–¥–æ–≤ –≤ –¥–∞–Ω–Ω—ã—Ö")

    # –û–±—â–∏–µ –∞–Ω–∞–ª–∏–∑—ã
    plan.extend([
        "–ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö",
        "–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º"
    ])

    # –ï—Å–ª–∏ —Ç–∞–±–ª–∏—Ü –º–Ω–æ–≥–æ, –¥–æ–±–∞–≤–ª—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã
    if len(table_names) > 6:
        plan.append("–ü–æ–∏—Å–∫ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∏ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤–æ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö")

        # –ê–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–≤—è–∑–µ–π
        table_connections = {}
        for relation in df_manager.relations:
            table_connections[relation.from_table] = table_connections.get(relation.from_table, 0) + 1
            table_connections[relation.to_table] = table_connections.get(relation.to_table, 0) + 1

        if table_connections:
            most_connected_table = max(table_connections.items(), key=lambda x: x[1])[0]
            plan.append(f"–£–≥–ª—É–±–ª–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ü–µ–Ω—Ç—Ä–∞–ª—å–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã '{most_connected_table}' –∏ –µ—ë —Å–≤—è–∑–µ–π")

    # –ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –∞–Ω–∞–ª–∏–∑—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–ª–∞–Ω–æ–≤
    if max_questions > 20:
        plan.extend([
            "–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ—á–∏—Å—Ç–∫–µ",
            "–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π",
            "–û—Ü–µ–Ω–∫–∞ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è",
            "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö"
        ])

    return plan[:max_questions]


def _create_executive_summary(session_memory: List[dict], table_summary: dict, successful_analyses: int) -> str:
    """–°–æ–∑–¥–∞–µ—Ç executive summary –¥–ª—è –æ—Ç—á–µ—Ç–∞"""
    try:
        total_questions = len(session_memory)
        total_tables = table_summary.get('total_tables', 0)
        total_relations = table_summary.get('total_relations', 0)
        total_memory = table_summary.get('total_memory_mb', 0)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        analyzed_tables = set()
        for finding in session_memory:
            analyzed_tables.update(finding.get('analyzed_tables', []))

        success_rate = (successful_analyses / max(total_questions, 1)) * 100

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        summary_parts = [
            f"–ü—Ä–æ–≤–µ–¥–µ–Ω –ø–æ–ª–Ω—ã–π DataFrame-–∞–Ω–∞–ª–∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö.",
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful_analyses} –∏–∑ {total_questions} –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (—É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%).",
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_tables} —Ç–∞–±–ª–∏—Ü ({total_memory:.1f} MB) —Å {total_relations} —Å–≤—è–∑—è–º–∏.",
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyzed_tables)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."
        ]

        # –ê–Ω–∞–ª–∏–∑ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤
        findings_with_anomalies = 0
        findings_with_correlations = 0
        findings_with_trends = 0

        for finding in session_memory:
            additional_info = finding.get('additional_info', {})
            if additional_info:
                if additional_info.get('anomalies'):
                    findings_with_anomalies += 1
                if additional_info.get('correlations'):
                    findings_with_correlations += 1
                if '—Ç—Ä–µ–Ω–¥' in finding.get('summary', '').lower():
                    findings_with_trends += 1

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–∞—Ö
        if findings_with_anomalies > 0:
            summary_parts.append(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏ –≤ {findings_with_anomalies} –∞–Ω–∞–ª–∏–∑–∞—Ö.")

        if findings_with_correlations > 0:
            summary_parts.append(f"–ù–∞–π–¥–µ–Ω—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ {findings_with_correlations} –∞–Ω–∞–ª–∏–∑–∞—Ö.")

        if findings_with_trends > 0:
            summary_parts.append(f"–í—ã—è–≤–ª–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –≤ {findings_with_trends} –∞–Ω–∞–ª–∏–∑–∞—Ö.")

        # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∫–∞—á–µ—Å—Ç–≤–µ –¥–∞–Ω–Ω—ã—Ö
        tables_info = table_summary.get('tables', {})
        if tables_info:
            total_rows = sum(info.get('rows', 0) for info in tables_info.values())
            avg_columns = sum(info.get('columns', 0) for info in tables_info.values()) / len(tables_info)

            summary_parts.append(
                f"–û–±—â–∏–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö: {total_rows:,} –∑–∞–ø–∏—Å–µ–π, —Å—Ä–µ–¥–Ω–µ {avg_columns:.1f} –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ —Ç–∞–±–ª–∏—Ü—É.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –¥–∞–ª—å–Ω–µ–π—à–∏–º –¥–µ–π—Å—Ç–≤–∏—è–º
        if total_relations > 0:
            summary_parts.append("–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã–µ —Å–≤—è–∑–∏ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∫–æ–º–ø–ª–µ–∫—Å–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤.")

        if findings_with_anomalies > 0:
            summary_parts.append("–ù–µ–æ–±—Ö–æ–¥–∏–º–æ –≤–Ω–∏–º–∞–Ω–∏–µ –∫ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö –∏–∑-–∑–∞ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π.")

        return " ".join(summary_parts)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è executive summary: {e}")
        return f"DataFrame-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —Å {successful_analyses} —É—Å–ø–µ—à–Ω—ã–º–∏ –∞–Ω–∞–ª–∏–∑–∞–º–∏ –∏–∑ {len(session_memory)} –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö."


def _generate_recommendations(session_memory: List[dict], table_summary: dict, successful_analyses: int) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
    try:
        recommendations = []

        total_tables = table_summary.get('total_tables', 0)
        total_relations = table_summary.get('total_relations', 0)
        total_memory = table_summary.get('total_memory_mb', 0)
        tables_info = table_summary.get('tables', {})

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
        if total_tables > 10:
            recommendations.append(
                "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –∫–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏–∏ –¥–∞–Ω–Ω—ã—Ö - –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ –º–Ω–æ–≥–æ —Ç–∞–±–ª–∏—Ü, —á—Ç–æ –º–æ–∂–µ—Ç —É—Å–ª–æ–∂–Ω–∏—Ç—å –∞–Ω–∞–ª–∏–∑.")

        if total_relations == 0 and total_tables > 1:
            recommendations.append(
                "–ö—Ä–∏—Ç–∏—á–Ω–æ: —Å–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫—É –≤–Ω–µ—à–Ω–∏—Ö –∫–ª—é—á–µ–π –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö.")
        elif total_relations > 0:
            recommendations.append(
                f"–û—Ç–ª–∏—á–Ω–æ: –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_relations} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∏—Ö –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–∞—á–µ—Å—Ç–≤—É –¥–∞–Ω–Ω—ã—Ö
        findings_with_anomalies = sum(1 for f in session_memory
                                      if f.get('additional_info', {}).get('anomalies'))

        if findings_with_anomalies > 0:
            recommendations.append(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –Ω–∞–π–¥–µ–Ω—ã –∞–Ω–æ–º–∞–ª–∏–∏ –≤ {findings_with_anomalies} –∞–Ω–∞–ª–∏–∑–∞—Ö. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è–º
        findings_with_correlations = sum(1 for f in session_memory
                                         if f.get('additional_info', {}).get('correlations'))

        if findings_with_correlations > 0:
            recommendations.append(
                f"–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏ –≤ {findings_with_correlations} –∞–Ω–∞–ª–∏–∑–∞—Ö. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if total_memory > 500:  # –ë–æ–ª—å—à–µ 500 MB
            recommendations.append(
                "–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –ø–∞—Ä—Ç–∏—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")

        if total_memory > 1000:  # –ë–æ–ª—å—à–µ 1 GB
            recommendations.append(
                "–ö—Ä–∏—Ç–∏—á–Ω–æ: –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö. –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Å—ç–º–ø–ª–∏–Ω–≥–∞ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–Ω–∞–ª–∏–∑—É —É—Å–ø–µ—à–Ω–æ—Å—Ç–∏
        success_rate = (successful_analyses / max(len(session_memory), 1)) * 100

        if success_rate < 80:
            recommendations.append(
                f"–í–Ω–∏–º–∞–Ω–∏–µ: –Ω–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ ({success_rate:.1f}%). –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö.")
        elif success_rate > 95:
            recommendations.append("–û—Ç–ª–∏—á–Ω–æ: –≤—ã—Å–æ–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞. –î–∞–Ω–Ω—ã–µ —Ö–æ—Ä–æ—à–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏—Ç–∏–∫–∏.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
        findings_with_charts = sum(1 for f in session_memory if f.get('chart_data'))
        if findings_with_charts > 0:
            recommendations.append(
                f"–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è: —Å–æ–∑–¥–∞–Ω–æ {findings_with_charts} –≥—Ä–∞—Ñ–∏–∫–æ–≤. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –¥–∞—à–±–æ—Ä–¥ –¥–ª—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–º –¥–∞–Ω–Ω—ã–º
        if tables_info:
            tables_with_issues = []
            for table_name, info in tables_info.items():
                schema_info = info.get('schema_info', {})
                if schema_info.get('is_truncated'):
                    tables_with_issues.append(table_name)

            if tables_with_issues:
                recommendations.append(
                    f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü–∞—Ö {', '.join(tables_with_issues)} –±—ã–ª–∏ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞. –î–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —É–≤–µ–ª–∏—á—å—Ç–µ –ª–∏–º–∏—Ç—ã.")

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        has_datetime_analysis = any('–≤—Ä–µ–º—è' in f.get('summary', '').lower() or '—Ç—Ä–µ–Ω–¥' in f.get('summary', '').lower()
                                    for f in session_memory)

        if has_datetime_analysis:
            recommendations.append(
                "–í—Ä–µ–º–µ–Ω–Ω–æ–π –∞–Ω–∞–ª–∏–∑: –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã –≤—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã. –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Ç—Ä–µ–Ω–¥–æ–≤.")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –º–∞—à–∏–Ω–Ω–æ–º—É –æ–±—É—á–µ–Ω–∏—é
        numeric_tables = sum(1 for info in tables_info.values()
                             if info.get('columns', 0) > 3 and info.get('rows', 0) > 100)

        if numeric_tables > 0 and findings_with_correlations > 0:
            recommendations.append(
                "ML –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏: –¥–∞–Ω–Ω—ã–µ –ø–æ–¥—Ö–æ–¥—è—Ç –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.")

        # –û–±—â–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        recommendations.extend([
            "–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏—è: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ DataFrame-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π.",
            "–£–≤–µ–¥–æ–º–ª–µ–Ω–∏—è: —Å–æ–∑–¥–∞–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π –∏ —Ç—Ä–µ–Ω–¥–æ–≤.",
            "–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è: —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞ —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º–∏ BI-—Å–∏—Å—Ç–µ–º–∞–º–∏."
        ])

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π
        return recommendations[:12]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        return [
            "–ó–∞–≤–µ—Ä—à–µ–Ω DataFrame-–∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö",
            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞",
            "–ù–∞—Å—Ç—Ä–æ–π—Ç–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö",
            "–†–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ —Å–æ–∑–¥–∞–Ω–∏–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á–µ—Ç–æ–≤"
        ]


def _trim_large_report(report: dict) -> dict:
    """–°–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    try:
        logger.info("–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç—á–µ—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é –æ—Ç—á–µ—Ç–∞
        trimmed_report = report.copy()

        # –°–æ–∫—Ä–∞—â–∞–µ–º detailed_findings
        detailed_findings = trimmed_report.get('detailed_findings', [])
        if len(detailed_findings) > 0:
            for finding in detailed_findings:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä data_preview
                if isinstance(finding.get('data_preview'), list) and len(finding['data_preview']) > 5:
                    finding['data_preview'] = finding['data_preview'][:5]
                    finding['data_preview'].append(
                        {"note": f"... –ø–æ–∫–∞–∑–∞–Ω–æ 5 –∏–∑ {len(finding['data_preview'])} –∑–∞–ø–∏—Å–µ–π"})

                # –£–ø—Ä–æ—â–∞–µ–º additional_info
                if finding.get('additional_info'):
                    additional_info = finding['additional_info']

                    # –°–æ–∫—Ä–∞—â–∞–µ–º –∞–Ω–æ–º–∞–ª–∏–∏
                    if additional_info.get('anomalies') and len(additional_info['anomalies']) > 3:
                        additional_info['anomalies'] = additional_info['anomalies'][:3]

                    # –°–æ–∫—Ä–∞—â–∞–µ–º –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
                    if additional_info.get('correlations') and len(additional_info['correlations']) > 5:
                        additional_info['correlations'] = additional_info['correlations'][:5]

                    # –£–ø—Ä–æ—â–∞–µ–º numeric_stats
                    if additional_info.get('numeric_stats'):
                        additional_info['numeric_stats'] = {"note": "–ß–∏—Å–ª–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞"}

        # –°–æ–∫—Ä–∞—â–∞–µ–º tables_info
        tables_info = trimmed_report.get('tables_info', {})
        for table_name, info in tables_info.items():
            # –£–±–∏—Ä–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é schema_info
            if 'schema_info' in info:
                schema_info = info['schema_info']
                info['schema_info'] = {
                    'row_count': schema_info.get('row_count'),
                    'memory_usage_mb': schema_info.get('memory_usage_mb'),
                    'is_truncated': schema_info.get('is_truncated'),
                    'note': "–î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ö–µ–º–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∞"
                }

        # –£–ø—Ä–æ—â–∞–µ–º memory_usage
        if 'memory_usage' in trimmed_report:
            trimmed_report['memory_usage'] = {"note": "–î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∞"}

        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–∏
        trimmed_report['report_metadata']['trimmed'] = True
        trimmed_report['report_metadata']['trim_reason'] = "–û—Ç—á–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"

        logger.info("–û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω")
        return trimmed_report

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        return report


# =============== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ó–ê–î–ê–ß–ò ===============

@celery_app.task(bind=True, time_limit=3600, name='tasks.quick_dataframe_analysis')
def quick_dataframe_analysis(self, connection_id: int, user_id: int, report_id: int):
    """–ë—ã—Å—Ç—Ä—ã–π DataFrame –∞–Ω–∞–ª–∏–∑ (–º–µ–Ω—å—à–µ –≤–æ–ø—Ä–æ—Å–æ–≤, –±—ã—Å—Ç—Ä–µ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ)"""
    logger.info(f"[QUICK DATAFRAME] –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 8}
    ).get()


@celery_app.task(bind=True, time_limit=9000, name='tasks.comprehensive_dataframe_analysis')
def comprehensive_dataframe_analysis(self, connection_id: int, user_id: int, report_id: int):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DataFrame –∞–Ω–∞–ª–∏–∑ (–º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –≤–æ–ø—Ä–æ—Å–æ–≤)"""
    logger.info(f"[COMPREHENSIVE DATAFRAME] –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 25}
    ).get()


@celery_app.task(bind=True, time_limit=5400, name='tasks.custom_dataframe_analysis')
def custom_dataframe_analysis(self, connection_id: int, user_id: int, report_id: int,
                              max_questions: int = 15, focus_tables: List[str] = None):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º—ã–π DataFrame –∞–Ω–∞–ª–∏–∑ —Å —Ñ–æ–∫—É—Å–æ–º –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã"""
    logger.info(f"[CUSTOM DATAFRAME] –ó–∞–ø—É—Å–∫ –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

    # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∫–∏ –Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü–∞—Ö
    # –ü–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –≤—ã–∑—ã–≤–∞–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': max_questions}
    ).get()


# =============== LEGACY –ü–û–î–î–ï–†–ñ–ö–ê ===============

@celery_app.task(bind=True, time_limit=3600, name='tasks.generate_advanced_report')
def generate_advanced_report(self, connection_id: int, user_id: int, report_id: int):
    """Legacy —Ñ—É–Ω–∫—Ü–∏—è - –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ DataFrame –∞–Ω–∞–ª–∏–∑"""
    logger.warning(
        f"[LEGACY] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è generate_advanced_report –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ DataFrame –∞–Ω–∞–ª–∏–∑")

    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 12}
    ).get()


@celery_app.task(bind=True, time_limit=1800, name='tasks.quick_ml_analysis')
def quick_ml_analysis(self, connection_id: int, user_id: int, report_id: int):
    """Legacy —Ñ—É–Ω–∫—Ü–∏—è - –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ –±—ã—Å—Ç—Ä—ã–π DataFrame –∞–Ω–∞–ª–∏–∑"""
    logger.warning(
        f"[LEGACY] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç–∞—Ä–µ–≤—à–∞—è —Ñ—É–Ω–∫—Ü–∏—è quick_ml_analysis –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ DataFrame –∞–Ω–∞–ª–∏–∑")

    return quick_dataframe_analysis.apply_async(
        args=[connection_id, user_id, report_id]
    ).get()


# =============== –£–¢–ò–õ–ò–¢–ê–†–ù–´–ï –ó–ê–î–ê–ß–ò ===============

@celery_app.task(bind=True, time_limit=600, name='tasks.test_dataframe_connection')
def test_dataframe_connection(self, connection_id: int, user_id: int):
    """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –±–∞–∑–æ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –¥–∞–Ω–Ω—ã—Ö"""

    db_session = next(database.get_db())

    try:
        logger.info(f"[TEST CONNECTION] –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è {connection_id} –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")

        connection_string = crud.get_decrypted_connection_string(db_session, connection_id, user_id)
        if not connection_string:
            raise ValueError("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

        engine = create_engine(connection_string, connect_args={'connect_timeout': 30})

        # –°–æ–∑–¥–∞–µ–º –±—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç DataFrameManager
        df_manager = DataFrameManager(engine)

        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å—Ç—Ä–æ–∫ –∏–∑ –∫–∞–∂–¥–æ–π —Ç–∞–±–ª–∏—Ü—ã
        from sqlalchemy import inspect
        inspector = inspect(engine)

        test_results = {
            "connection_status": "success",
            "tables_found": [],
            "total_tables": 0,
            "estimated_total_rows": 0,
            "sample_data_available": True
        }

        table_names = [t for t in inspector.get_table_names() if t != 'alembic_version']
        test_results["total_tables"] = len(table_names)

        for table_name in table_names[:10]:  # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–∞–∫—Å–∏–º—É–º 10 —Ç–∞–±–ª–∏—Ü
            try:
                with engine.connect() as conn:
                    count_result = conn.execute(text(f"SELECT COUNT(*) FROM {table_name}"))
                    row_count = count_result.scalar() or 0

                    sample_query = text(f"SELECT * FROM {table_name} LIMIT 1")
                    sample_result = conn.execute(sample_query)
                    columns = list(sample_result.keys())

                    test_results["tables_found"].append({
                        "table_name": table_name,
                        "row_count": int(row_count),
                        "columns": len(columns),
                        "column_names": columns[:10]  # –ü–µ—Ä–≤—ã–µ 10 –∫–æ–ª–æ–Ω–æ–∫
                    })

                    test_results["estimated_total_rows"] += row_count

            except Exception as table_error:
                logger.warning(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–∞–±–ª–∏—Ü—ã {table_name}: {table_error}")
                test_results["tables_found"].append({
                    "table_name": table_name,
                    "error": str(table_error)
                })

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∏
        relations_found = 0
        for table_name in table_names[:5]:  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–≤—è–∑–∏ –¥–ª—è –ø–µ—Ä–≤—ã—Ö 5 —Ç–∞–±–ª–∏—Ü
            try:
                foreign_keys = inspector.get_foreign_keys(table_name)
                relations_found += len(foreign_keys)
            except:
                continue

        test_results["relations_found"] = relations_found
        test_results["dataframe_compatible"] = True
        test_results["estimated_memory_mb"] = round(test_results["estimated_total_rows"] * len(table_names) * 0.001, 2)

        logger.info(f"[TEST CONNECTION] –£—Å–ø–µ—à–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: {len(test_results['tables_found'])} —Ç–∞–±–ª–∏—Ü")
        return test_results

    except Exception as e:
        logger.error(f"[TEST CONNECTION] –û—à–∏–±–∫–∞: {e}")
        return {
            "connection_status": "failed",
            "error": str(e),
            "dataframe_compatible": False
        }

    finally:
        try:
            db_session.close()
        except:
            pass


@celery_app.task(bind=True, time_limit=300, name='tasks.cleanup_old_reports')
def cleanup_old_reports(self, days_old: int = 30):
    """–û—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ä—ã–µ –æ—Ç—á–µ—Ç—ã –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞ –≤ –ë–î"""

    db_session = next(database.get_db())

    try:
        from datetime import timedelta
        cutoff_date = datetime.now() - timedelta(days=days_old)

        # –ù–∞—Ö–æ–¥–∏–º —Å—Ç–∞—Ä—ã–µ –æ—Ç—á–µ—Ç—ã
        old_reports = db_session.query(crud.models.Report).filter(
            crud.models.Report.created_at < cutoff_date,
            crud.models.Report.status.in_(['COMPLETED', 'FAILED'])
        ).all()

        cleaned_count = 0
        for report in old_reports:
            try:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ metadata, —É–¥–∞–ª—è–µ–º detailed_findings
                if report.results and isinstance(report.results, dict):
                    simplified_results = {
                        "executive_summary": report.results.get("executive_summary", ""),
                        "analysis_stats": report.results.get("analysis_stats", {}),
                        "method": report.results.get("method", ""),
                        "cleaned_date": datetime.now().isoformat(),
                        "original_size": "cleaned"
                    }
                    report.results = simplified_results
                    cleaned_count += 1
            except Exception as clean_error:
                logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –æ—Ç—á–µ—Ç–∞ {report.id}: {clean_error}")

        db_session.commit()

        logger.info(f"[CLEANUP] –û—á–∏—â–µ–Ω–æ {cleaned_count} —Å—Ç–∞—Ä—ã—Ö –æ—Ç—á–µ—Ç–æ–≤ (—Å—Ç–∞—Ä—à–µ {days_old} –¥–Ω–µ–π)")

        return {
            "status": "success",
            "cleaned_reports": cleaned_count,
            "cutoff_date": cutoff_date.isoformat()
        }

    except Exception as e:
        logger.error(f"[CLEANUP] –û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏: {e}")
        db_session.rollback()
        return {
            "status": "failed",
            "error": str(e)
        }

    finally:
        try:
            db_session.close()
        except:
            pass


# =============== –ú–û–ù–ò–¢–û–†–ò–ù–ì –ò –°–¢–ê–¢–ò–°–¢–ò–ö–ê ===============

@celery_app.task(bind=True, time_limit=180, name='tasks.get_system_health')
def get_system_health(self):
    """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∑–¥–æ—Ä–æ–≤—å–µ —Å–∏—Å—Ç–µ–º—ã DataFrame –∞–Ω–∞–ª–∏–∑–∞"""

    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º Celery
        i = celery_app.control.inspect()
        active_tasks = i.active()
        scheduled_tasks = i.scheduled()

        active_count = sum(len(tasks) for tasks in (active_tasks or {}).values())
        scheduled_count = sum(len(tasks) for tasks in (scheduled_tasks or {}).values())

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        db_session = next(database.get_db())

        try:
            # –ü–æ–¥—Å—á–µ—Ç –æ—Ç—á–µ—Ç–æ–≤ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º
            from sqlalchemy import func
            report_stats = db_session.query(
                crud.models.Report.status,
                func.count(crud.models.Report.id)
            ).group_by(crud.models.Report.status).all()

            status_counts = dict(report_stats)

            # –ü–æ—Å–ª–µ–¥–Ω–∏–µ –æ—Ç—á–µ—Ç—ã
            recent_reports = db_session.query(crud.models.Report).filter(
                crud.models.Report.created_at >= datetime.now() - timedelta(hours=24)
            ).count()

            health_status = {
                "timestamp": datetime.now().isoformat(),
                "celery": {
                    "active_tasks": active_count,
                    "scheduled_tasks": scheduled_count,
                    "status": "healthy" if active_count < 10 else "busy"
                },
                "database": {
                    "status": "healthy",
                    "recent_reports_24h": recent_reports,
                    "total_completed": status_counts.get("COMPLETED", 0),
                    "total_failed": status_counts.get("FAILED", 0),
                    "total_processing": status_counts.get("PROCESSING", 0)
                },
                "dataframe_system": {
                    "status": "operational",
                    "supported_features": [
                        "automatic_table_loading",
                        "relationship_detection",
                        "anomaly_detection",
                        "correlation_analysis",
                        "trend_analysis",
                        "memory_optimization"
                    ]
                }
            }

            # –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–π —Å—Ç–∞—Ç—É—Å
            if status_counts.get("FAILED", 0) > status_counts.get("COMPLETED", 0):
                health_status["overall_status"] = "warning"
            elif active_count > 15:
                health_status["overall_status"] = "busy"
            else:
                health_status["overall_status"] = "healthy"

        finally:
            db_session.close()

        return health_status

    except Exception as e:
        logger.error(f"[HEALTH CHECK] –û—à–∏–±–∫–∞: {e}")
        return {
            "timestamp": datetime.now().isoformat(),
            "overall_status": "error",
            "error": str(e)
        }


# =============== –≠–ö–°–ü–û–†–¢ –ò –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø ===============

@celery_app.task(bind=True, time_limit=1800, name='tasks.export_report_to_excel')
def export_report_to_excel(self, report_id: int, user_id: int):
    """–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –≤ Excel —Ñ–∞–π–ª"""

    db_session = next(database.get_db())

    try:
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç—á–µ—Ç
        report = crud.get_report_by_id(db_session, report_id, user_id)
        if not report or not report.results:
            raise ValueError("–û—Ç—á–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ –ø—É—Å—Ç")

        # –°–æ–∑–¥–∞–µ–º Excel —Ñ–∞–π–ª
        import io
        from datetime import datetime

        output = io.BytesIO()

        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–≥–∏–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è Excel —Ñ–∞–π–ª–∞
        # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–æ–º, —á—Ç–æ —ç–∫—Å–ø–æ—Ä—Ç –≥–æ—Ç–æ–≤

        export_info = {
            "status": "completed",
            "report_id": report_id,
            "export_date": datetime.now().isoformat(),
            "file_size_bytes": len(str(report.results)),
            "format": "excel",
            "note": "Excel export feature - –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ"
        }

        return export_info

    except Exception as e:
        logger.error(f"[EXCEL EXPORT] –û—à–∏–±–∫–∞: {e}")
        return {
            "status": "failed",
            "error": str(e),
            "report_id": report_id
        }

    finally:
        try:
            db_session.close()
        except:
            pass


logger.info("DataFrame tasks —Å–∏—Å—Ç–µ–º–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
