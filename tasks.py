# tasks.py - –ø–æ–ª–Ω–∞—è DataFrame —Å–∏—Å—Ç–µ–º–∞ —Å GPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π
import logging
from typing import List, Dict, Any
import numpy as np
from celery.exceptions import Ignore
from sqlalchemy import create_engine, text
import crud
import database
from services.dataframe_manager import DataFrameManager
from services.dataframe_analyzer import DataFrameAnalyzer
from services.gpt_analyzer import GPTAnalyzer
from utils.json_serializer import convert_to_serializable
from celery_worker import celery_app
from datetime import datetime, timedelta
import json
import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ utils
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

logger = logging.getLogger(__name__)


@celery_app.task(bind=True, time_limit=7200, name='tasks.generate_dataframe_report')
def generate_dataframe_report(self, connection_id: int, user_id: int, report_id: int, max_questions: int = 15):
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–∞ –ø–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞ –æ—Å–Ω–æ–≤–µ DataFrame —Å GPT –∞–Ω–∞–ª–∏–∑–æ–º
    """
    db_session = next(database.get_db())

    try:
        logger.info(f"[DATAFRAME REPORT] –ó–∞–ø—É—Å–∫ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}, –æ—Ç—á–µ—Ç {report_id}")

        # === –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø ===
        self.update_state(
            state='INITIALIZING',
            meta={'progress': '–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è DataFrame —Å–∏—Å—Ç–µ–º—ã —Å GPT...'}
        )

        connection_string = crud.get_decrypted_connection_string(db_session, connection_id, user_id)
        if not connection_string:
            raise ValueError("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–æ")

        # –°–æ–∑–¥–∞–µ–º –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö
        engine = create_engine(connection_string, connect_args={'connect_timeout': 60})

        # === –ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í DATAFRAME ===
        self.update_state(
            state='LOADING_DATA',
            meta={'progress': '–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö —Ç–∞–±–ª–∏—Ü –≤ –ø–∞–º—è—Ç—å...', 'progress_percentage': 10}
        )

        df_manager = DataFrameManager(engine)
        tables_loaded = df_manager.load_all_tables(max_rows_per_table=50000)

        if not tables_loaded:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ –±–∞–∑—ã. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞.")

        logger.info(f"[DATAFRAME REPORT] –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(tables_loaded)} —Ç–∞–±–ª–∏—Ü")

        # === –û–ü–¢–ò–ú–ò–ó–ê–¶–ò–Ø –ü–ê–ú–Ø–¢–ò ===
        self.update_state(
            state='OPTIMIZING',
            meta={'progress': '–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏...', 'progress_percentage': 20}
        )

        df_manager.optimize_memory()

        # === –°–û–ó–î–ê–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê–¢–û–†–û–í ===
        self.update_state(
            state='INITIALIZING_ANALYZERS',
            meta={'progress': '–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è GPT –∏ DataFrame –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–æ–≤...', 'progress_percentage': 25}
        )

        analyzer = DataFrameAnalyzer(df_manager)
        gpt_analyzer = GPTAnalyzer()

        # === –°–û–ó–î–ê–ù–ò–ï –ò–ù–¢–ï–õ–õ–ï–ö–¢–£–ê–õ–¨–ù–û–ì–û –ü–õ–ê–ù–ê –ê–ù–ê–õ–ò–ó–ê ===
        self.update_state(
            state='PLANNING',
            meta={'progress': '–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –ø–ª–∞–Ω–∞ –∞–Ω–∞–ª–∏–∑–∞...', 'progress_percentage': 30}
        )

        analysis_plan = _create_enhanced_analysis_plan(df_manager, max_questions)
        logger.info(f"[DATAFRAME REPORT] –ü–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞: {len(analysis_plan)} –≤–æ–ø—Ä–æ—Å–æ–≤")

        # === –í–´–ü–û–õ–ù–ï–ù–ò–ï –ê–ù–ê–õ–ò–ó–ê ===
        session_memory = []
        successful_analyses = 0
        gpt_analyses_count = 0

        for i, question_config in enumerate(analysis_plan):
            if i >= max_questions:
                break

            progress = 30 + (i + 1) / min(len(analysis_plan), max_questions) * 60  # 30-90%
            question = question_config.get('question', str(question_config))
            analysis_type = question_config.get('type', 'general')

            self.update_state(
                state='ANALYZING',
                meta={
                    'progress': f'–ê–Ω–∞–ª–∏–∑ {i + 1}/{min(len(analysis_plan), max_questions)}: {question}',
                    'progress_percentage': progress,
                    'analysis_type': analysis_type
                }
            )

            logger.info(f"[DATAFRAME REPORT] –ê–Ω–∞–ª–∏–∑ {i + 1}: {question} (—Ç–∏–ø: {analysis_type})")

            try:
                # –í—ã–ø–æ–ª–Ω—è–µ–º DataFrame –∞–Ω–∞–ª–∏–∑
                result = analyzer.analyze_question(question)

                if not result.get('error'):
                    # –ë–µ–∑–æ–ø–∞—Å–Ω–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏
                    data_preview = result.get('data', None)
                    if data_preview:
                        if hasattr(data_preview, 'head'):
                            data_preview = convert_to_serializable(data_preview.head(10).to_dict('records'))
                        elif isinstance(data_preview, (list, dict)):
                            data_preview = convert_to_serializable(data_preview)
                    else:
                        data_preview = []

                    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤—Å–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
                    additional_info = result.get('additional_info', {})
                    if additional_info:
                        additional_info = convert_to_serializable(additional_info)

                    # === GPT –£–ì–õ–£–ë–õ–ï–ù–ù–´–ô –ê–ù–ê–õ–ò–ó ===
                    gpt_insights = {}
                    if question_config.get('enable_gpt', True) and data_preview:
                        try:
                            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø GPT –∞–Ω–∞–ª–∏–∑–∞
                            gpt_type = _determine_gpt_analysis_type(question, analysis_type)

                            if gpt_type and len(result.get('analyzed_tables', [])) > 0:
                                main_table = result['analyzed_tables'][0]
                                df_for_gpt = df_manager.tables[main_table]

                                gpt_result = gpt_analyzer.analyze_data_with_gpt(
                                    df=df_for_gpt,
                                    table_name=main_table,
                                    analysis_type=gpt_type,
                                    context={
                                        'question': question,
                                        'dataframe_results': result,
                                        'analysis_type': analysis_type
                                    }
                                )

                                gpt_insights = {
                                    'gpt_analysis': gpt_result.get('gpt_analysis', ''),
                                    'gpt_type': gpt_type,
                                    'confidence': gpt_result.get('confidence', 'medium')
                                }

                                gpt_analyses_count += 1
                                logger.info(f"[GPT ANALYSIS] –£—Å–ø–µ—à–Ω–æ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {i + 1}")

                        except Exception as gpt_error:
                            logger.error(f"[GPT ANALYSIS] –û—à–∏–±–∫–∞ –¥–ª—è –≤–æ–ø—Ä–æ—Å–∞ {i + 1}: {gpt_error}")
                            gpt_insights = {
                                'gpt_analysis': f'GPT –∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {str(gpt_error)}',
                                'confidence': 'low'
                            }

                    finding_entry = {
                        'question': str(question),
                        'summary': str(result.get('summary', '')),
                        'data_preview': data_preview,
                        'chart_data': convert_to_serializable(result.get('chart_data')),
                        'analyzed_tables': list(result.get('analyzed_tables', [])),
                        'method': 'dataframe_with_gpt',
                        'analysis_type': analysis_type,
                        'additional_info': additional_info,
                        'gpt_insights': gpt_insights,
                        'timestamp': datetime.now().isoformat(),
                        'success': True
                    }

                    session_memory.append(finding_entry)
                    successful_analyses += 1

                    logger.info(f"[DATAFRAME REPORT] ‚úÖ –ê–Ω–∞–ª–∏–∑ {i + 1} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")

                else:
                    error_msg = str(result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞'))
                    logger.error(f"[DATAFRAME REPORT] ‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {i + 1}: {error_msg}")

                    session_memory.append({
                        'question': str(question),
                        'summary': f'–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {error_msg}',
                        'data_preview': [],
                        'error': error_msg,
                        'method': 'dataframe',
                        'analysis_type': analysis_type,
                        'success': False,
                        'timestamp': datetime.now().isoformat()
                    })

            except Exception as analysis_error:
                error_msg = f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {str(analysis_error)}"
                logger.error(f"[DATAFRAME REPORT] üí• {error_msg}")

                session_memory.append({
                    'question': str(question),
                    'summary': error_msg,
                    'data_preview': [],
                    'error': error_msg,
                    'method': 'dataframe',
                    'analysis_type': analysis_type,
                    'success': False,
                    'timestamp': datetime.now().isoformat()
                })

        # === –°–û–ó–î–ê–ù–ò–ï –§–ò–ù–ê–õ–¨–ù–û–ì–û –û–¢–ß–ï–¢–ê ===
        self.update_state(
            state='FINALIZING',
            meta={'progress': '–°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞ —Å GPT —Å–≤–æ–¥–∫–æ–π...', 'progress_percentage': 90}
        )

        logger.info(
            f"[DATAFRAME REPORT] –°–æ–∑–¥–∞–Ω–∏–µ –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {successful_analyses}/{len(session_memory)} —É—Å–ø–µ—à–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤, {gpt_analyses_count} GPT –∞–Ω–∞–ª–∏–∑–æ–≤")

        # –ü–æ–ª—É—á–∞–µ–º —Å–≤–æ–¥–∫—É –ø–æ —Ç–∞–±–ª–∏—Ü–∞–º
        table_summary = df_manager.get_table_summary()
        memory_info = df_manager.get_memory_usage()

        # === GPT EXECUTIVE SUMMARY ===
        executive_summary = ""
        try:
            if successful_analyses > 0:
                gpt_insights = [f for f in session_memory if f.get('gpt_insights', {}).get('gpt_analysis')]
                if gpt_insights:
                    executive_summary = gpt_analyzer.generate_executive_summary(gpt_insights, table_summary)
                else:
                    executive_summary = _create_executive_summary(session_memory, table_summary, successful_analyses)
            else:
                executive_summary = "–ê–Ω–∞–ª–∏–∑ –Ω–µ –¥–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"
        except Exception as summary_error:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è executive summary: {summary_error}")
            executive_summary = _create_executive_summary(session_memory, table_summary, successful_analyses)

        # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç—á–µ—Ç —Å –±–µ–∑–æ–ø–∞—Å–Ω–æ–π —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π
        final_report = {
            "executive_summary": str(executive_summary),
            "detailed_findings": convert_to_serializable(session_memory),
            "method": "dataframe_with_gpt",
            "tables_info": convert_to_serializable(table_summary['tables']),
            "relations_info": convert_to_serializable(table_summary['relations']),
            "analysis_stats": {
                "questions_processed": int(len(session_memory)),
                "successful_analyses": int(successful_analyses),
                "failed_analyses": int(len(session_memory) - successful_analyses),
                "gpt_analyses_count": int(gpt_analyses_count),
                "tables_analyzed": int(table_summary['total_tables']),
                "relations_found": int(table_summary['total_relations']),
                "total_memory_mb": round(float(table_summary['total_memory_mb']), 2),
                "success_rate_percent": round(float(successful_analyses / max(len(session_memory), 1) * 100), 1),
                "gpt_integration": True
            },
            "memory_usage": convert_to_serializable(memory_info),
            "recommendations": [str(r) for r in
                                _generate_enhanced_recommendations(session_memory, table_summary, successful_analyses,
                                                                   gpt_analyses_count)],
            "report_metadata": {
                "created_at": datetime.now().isoformat(),
                "user_id": int(user_id),
                "connection_id": int(connection_id),
                "report_version": "3.0_dataframe_gpt",
                "max_questions_requested": int(max_questions),
                "gpt_enabled": True
            }
        }

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä –æ—Ç—á–µ—Ç–∞ –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
        try:
            report_json = json.dumps(final_report, ensure_ascii=False)
            report_size_mb = len(report_json.encode('utf-8')) / 1024 / 1024
            logger.info(f"[DATAFRAME REPORT] –†–∞–∑–º–µ—Ä –∏—Ç–æ–≥–æ–≤–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {report_size_mb:.2f} MB")

            # –ï—Å–ª–∏ –æ—Ç—á–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π, —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if report_size_mb > 10:  # 10MB –ª–∏–º–∏—Ç
                logger.warning(f"[DATAFRAME REPORT] –û—Ç—á–µ—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({report_size_mb:.2f} MB), —Å–æ–∫—Ä–∞—â–∞–µ–º –¥–∞–Ω–Ω—ã–µ")
                final_report = _trim_large_report(final_report)

        except Exception as json_error:
            logger.error(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ JSON —Å–µ—Ä–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {json_error}")
            final_report = convert_to_serializable(final_report)

        # === –°–û–•–†–ê–ù–ï–ù–ò–ï –û–¢–ß–ï–¢–ê ===
        self.update_state(
            state='SAVING',
            meta={'progress': '–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á–µ—Ç–∞ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö...', 'progress_percentage': 95}
        )

        try:
            crud.update_report(db_session, report_id, "COMPLETED", final_report)
            logger.info(f"[DATAFRAME REPORT] ‚úÖ –û—Ç—á–µ—Ç {report_id} —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω")
        except Exception as save_error:
            logger.error(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {save_error}")

            try:
                db_session.rollback()
            except:
                pass

            # –ü–æ–ø—ã—Ç–∫–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç
            simplified_report = {
                "executive_summary": final_report["executive_summary"],
                "method": "dataframe_with_gpt",
                "analysis_stats": final_report["analysis_stats"],
                "error": f"–ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç –Ω–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å: {str(save_error)}",
                "report_metadata": final_report["report_metadata"]
            }

            crud.update_report(db_session, report_id, "COMPLETED", simplified_report)
            logger.warning(f"[DATAFRAME REPORT] –°–æ—Ö—Ä–∞–Ω–µ–Ω —É–ø—Ä–æ—â–µ–Ω–Ω—ã–π –æ—Ç—á–µ—Ç {report_id}")

        self.update_state(
            state='SUCCESS',
            meta={'progress': 'DataFrame-–∞–Ω–∞–ª–∏–∑ —Å GPT –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!', 'progress_percentage': 100}
        )

        return {
            "status": "success",
            "report_id": report_id,
            "questions_processed": len(session_memory),
            "successful_analyses": successful_analyses,
            "gpt_analyses": gpt_analyses_count,
            "tables_loaded": len(tables_loaded)
        }

    except Exception as e:
        logger.error(f"[DATAFRAME REPORT ERROR] –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)

        try:
            db_session.rollback()
        except:
            pass

        error_report = {
            "error": str(e),
            "method": "dataframe_with_gpt",
            "stage": "critical_error",
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "connection_id": connection_id
        }

        try:
            crud.update_report(db_session, report_id, "FAILED", error_report)
            logger.info(f"[DATAFRAME REPORT] –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –æ—Ç—á–µ—Ç {report_id}")
        except Exception as save_error:
            logger.error(f"[DATAFRAME REPORT] –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –æ—à–∏–±–∫—É: {save_error}")

        self.update_state(
            state='FAILURE',
            meta={
                'progress': f'–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {str(e)}',
                'error': str(e),
                'progress_percentage': 0
            }
        )

        raise e

    finally:
        try:
            db_session.close()
        except:
            pass


def _create_enhanced_analysis_plan(df_manager: DataFrameManager, max_questions: int) -> List[Dict[str, Any]]:
    """–°–æ–∑–¥–∞–µ—Ç –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω –∞–Ω–∞–ª–∏–∑–∞ —Å GPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π"""

    plan = [
        {
            'question': '–û–±—â–∏–π –æ–±–∑–æ—Ä —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏',
            'type': 'overview',
            'enable_gpt': True,
            'priority': 1
        }
    ]

    table_names = list(df_manager.tables.keys())

    # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π –≤–∞–∂–Ω–æ–π —Ç–∞–±–ª–∏—Ü—ã
    tables_by_size = sorted(df_manager.tables.items(), key=lambda x: len(x[1]), reverse=True)

    for i, (table_name, df) in enumerate(tables_by_size[:min(4, len(tables_by_size))]):
        if i < 2:  # –ü–µ—Ä–≤—ã–µ 2 —Ç–∞–±–ª–∏—Ü—ã - –¥–µ—Ç–∞–ª—å–Ω—ã–π GPT –∞–Ω–∞–ª–∏–∑
            plan.extend([
                {
                    'question': f"–î–µ—Ç–∞–ª—å–Ω—ã–π –±–∏–∑–Ω–µ—Å-–∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü—ã '{table_name}' —Å GPT –∏–Ω—Å–∞–π—Ç–∞–º–∏",
                    'type': 'table_analysis',
                    'table_focus': table_name,
                    'enable_gpt': True,
                    'priority': 2
                },
                {
                    'question': f"–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü—ã '{table_name}' —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏",
                    'type': 'data_quality',
                    'table_focus': table_name,
                    'enable_gpt': True,
                    'priority': 2
                }
            ])
        else:  # –û—Å—Ç–∞–ª—å–Ω—ã–µ - –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
            plan.append({
                'question': f"–≠–∫—Å–ø—Ä–µ—Å—Å-–∞–Ω–∞–ª–∏–∑ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ç–∞–±–ª–∏—Ü—ã '{table_name}'",
                'type': 'table_analysis',
                'table_focus': table_name,
                'enable_gpt': False,
                'priority': 3
            })

    # –ê–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    if df_manager.relations:
        plan.append({
            'question': '–ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –∏ –∏—Ö —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏',
            'type': 'relationship_analysis',
            'enable_gpt': True,
            'priority': 2
        })

    # –°–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–µ –∞–Ω–∞–ª–∏–∑—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –¥–∞–Ω–Ω—ã—Ö
    has_numeric_data = any(
        len(df.select_dtypes(include=[np.number]).columns) > 0
        for df in df_manager.tables.values()
    )

    has_datetime_data = any(
        any('date' in col.lower() or 'time' in col.lower() or 'year' in col.lower() for col in df.columns)
        for df in df_manager.tables.values()
    )

    if has_numeric_data:
        plan.extend([
            {
                'question': '–ì–ª—É–±–æ–∫–∏–π —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å GPT –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–µ–π',
                'type': 'statistical_insights',
                'enable_gpt': True,
                'priority': 2
            },
            {
                'question': '–ê–Ω–∞–ª–∏–∑ –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Å –±–∏–∑–Ω–µ—Å-–≤—ã–≤–æ–¥–∞–º–∏',
                'type': 'correlation',
                'enable_gpt': True,
                'priority': 2
            },
            {
                'question': '–ü–æ–∏—Å–∫ –∞–Ω–æ–º–∞–ª–∏–π –∏ –≤—ã–±—Ä–æ—Å–æ–≤ —Å –∞–Ω–∞–ª–∏–∑–æ–º –ø—Ä–∏—á–∏–Ω',
                'type': 'anomalies',
                'enable_gpt': True,
                'priority': 3
            }
        ])

    if has_datetime_data:
        plan.append({
            'question': '–ü—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ç—Ä–µ–Ω–¥–æ–≤ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ',
            'type': 'predictive_analysis',
            'enable_gpt': True,
            'priority': 2
        })

    # –û–±—â–∏–µ GPT-–∞–Ω–∞–ª–∏–∑—ã
    plan.extend([
        {
            'question': '–ê–Ω–∞–ª–∏–∑ –±–∏–∑–Ω–µ—Å-–º–µ—Ç—Ä–∏–∫ –∏ KPI —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏',
            'type': 'business_insights',
            'enable_gpt': True,
            'priority': 2
        },
        {
            'question': '–°—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–∞–±–ª–∏—Ü —Å –≤—ã—è–≤–ª–µ–Ω–∏–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤',
            'type': 'comparison',
            'enable_gpt': True,
            'priority': 3
        }
    ])

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∞–Ω–∞–ª–∏–∑—ã –¥–ª—è –±–æ–ª—å—à–∏—Ö –ø–ª–∞–Ω–æ–≤
    if max_questions > 15:
        plan.extend([
            {
                'question': '–ü–æ–∏—Å–∫ —Å–∫—Ä—ã—Ç—ã—Ö –±–∏–∑–Ω–µ—Å-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –≤ –¥–∞–Ω–Ω—ã—Ö',
                'type': 'business_insights',
                'enable_gpt': True,
                'priority': 3
            },
            {
                'question': '–ê–Ω–∞–ª–∏–∑ –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª–∞ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è',
                'type': 'predictive_analysis',
                'enable_gpt': True,
                'priority': 4
            },
            {
                'question': '–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö',
                'type': 'data_quality',
                'enable_gpt': True,
                'priority': 4
            }
        ])

    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
    plan.sort(key=lambda x: x.get('priority', 5))
    return plan[:max_questions]


def _determine_gpt_analysis_type(question: str, analysis_type: str) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø GPT –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–æ–ø—Ä–æ—Å–∞"""

    question_lower = question.lower()

    if analysis_type == 'overview':
        return 'business_insights'
    elif analysis_type == 'table_analysis':
        if '–±–∏–∑–Ω–µ—Å' in question_lower or '–∏–Ω—Å–∞–π—Ç' in question_lower:
            return 'business_insights'
        elif '–∫–∞—á–µ—Å—Ç–≤–æ' in question_lower or '–ø—Ä–æ–±–ª–µ–º' in question_lower:
            return 'data_quality'
        else:
            return 'business_insights'
    elif analysis_type == 'statistical_insights':
        return 'statistical_insights'
    elif analysis_type == 'predictive_analysis':
        return 'predictive_analysis'
    elif analysis_type == 'data_quality':
        return 'data_quality'
    elif analysis_type == 'business_insights':
        return 'business_insights'
    elif analysis_type == 'correlation':
        return 'statistical_insights'
    elif analysis_type == 'anomalies':
        return 'data_quality'
    else:
        return 'business_insights'


def _generate_enhanced_recommendations(session_memory: List[dict], table_summary: dict,
                                       successful_analyses: int, gpt_analyses_count: int) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ —Å —É—á–µ—Ç–æ–º GPT –∞–Ω–∞–ª–∏–∑–∞"""

    recommendations = []

    try:
        total_tables = table_summary.get('total_tables', 0)
        total_relations = table_summary.get('total_relations', 0)
        total_memory = table_summary.get('total_memory_mb', 0)

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º GPT –∞–Ω–∞–ª–∏–∑–∞
        if gpt_analyses_count > 0:
            recommendations.append(f"ü§ñ –ü—Ä–æ–≤–µ–¥–µ–Ω–æ {gpt_analyses_count} —É–≥–ª—É–±–ª–µ–Ω–Ω—ã—Ö GPT-–∞–Ω–∞–ª–∏–∑–æ–≤ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –∏–Ω—Å–∞–π—Ç–∞–º–∏")

            # –°–æ–±–∏—Ä–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ insights –∏–∑ GPT –∞–Ω–∞–ª–∏–∑–æ–≤
            gpt_insights = []
            for finding in session_memory:
                gpt_data = finding.get('gpt_insights', {})
                if gpt_data.get('gpt_analysis'):
                    gpt_insights.append(gpt_data['gpt_analysis'])

            if len(gpt_insights) >= 3:
                recommendations.append(
                    "üìä GPT –∞–Ω–∞–ª–∏–∑ –≤—ã—è–≤–∏–ª –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã–µ –±–∏–∑–Ω–µ—Å-–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ - –¥–µ—Ç–∞–ª–∏ –≤ —Ä–∞–∑–¥–µ–ª–µ –∏–Ω—Å–∞–π—Ç–æ–≤")

        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        success_rate = (successful_analyses / max(len(session_memory), 1)) * 100

        if success_rate > 90:
            recommendations.append("‚úÖ –û—Ç–ª–∏—á–Ω–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ - –¥–∞–Ω–Ω—ã–µ –≤—ã—Å–æ–∫–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π")
        elif success_rate < 70:
            recommendations.append("‚ö†Ô∏è –ù–∏–∑–∫–∞—è —É—Å–ø–µ—à–Ω–æ—Å—Ç—å –∞–Ω–∞–ª–∏–∑–∞ - —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ –¥–∞–Ω–Ω—ã—Ö
        if total_relations > 0:
            recommendations.append(
                f"üîó –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ {total_relations} —Å–≤—è–∑–µ–π –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ - –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞—à–±–æ—Ä–¥–æ–≤")
        elif total_tables > 1:
            recommendations.append(
                "‚ùó –°–≤—è–∑–∏ –º–µ–∂–¥—É —Ç–∞–±–ª–∏—Ü–∞–º–∏ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω—ã - –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ –≤–Ω–µ—à–Ω–∏–µ –∫–ª—é—á–∏ –¥–ª—è –æ–±–µ—Å–ø–µ—á–µ–Ω–∏—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç–∏")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        if total_memory > 500:
            recommendations.append(
                "üöÄ –ë–æ–ª—å—à–æ–π –æ–±—ä–µ–º –¥–∞–Ω–Ω—ã—Ö –≤ –ø–∞–º—è—Ç–∏ - —Ä–∞—Å—Å–º–æ—Ç—Ä–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è")

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏
        recommendations.extend([
            "üîÑ –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ DataFrame-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏–∑–º–µ–Ω–µ–Ω–∏–π",
            "üìà –°–æ–∑–¥–∞–π—Ç–µ –¥–∞—à–±–æ—Ä–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—ã—è–≤–ª–µ–Ω–Ω—ã—Ö –∫–ª—é—á–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫",
            "ü§ñ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPT-–∏–Ω—Å–∞–π—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç—á–µ—Ç–æ–≤",
            "üîî –ù–∞—Å—Ç—Ä–æ–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –∞–ª–µ—Ä—Ç–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π"
        ])

        return recommendations[:10]

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        return [
            "–ó–∞–≤–µ—Ä—à–µ–Ω —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π DataFrame-–∞–Ω–∞–ª–∏–∑ —Å GPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π",
            "–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Ä–µ–≥—É–ª—è—Ä–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –∏–Ω—Å–∞–π—Ç–æ–≤"
        ]


# –û—Å—Ç–∞–ª—å–Ω—ã–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è —Ç–∞–∫–∏–º–∏ –∂–µ –∫–∞–∫ –≤ –∏—Å—Ö–æ–¥–Ω–æ–º –∫–æ–¥–µ...
def _create_executive_summary(session_memory: List[dict], table_summary: dict, successful_analyses: int) -> str:
    """–°–æ–∑–¥–∞–µ—Ç executive summary –¥–ª—è –æ—Ç—á–µ—Ç–∞"""
    try:
        total_questions = len(session_memory)
        total_tables = table_summary.get('total_tables', 0)
        total_relations = table_summary.get('total_relations', 0)
        total_memory = table_summary.get('total_memory_mb', 0)

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ç–∞–±–ª–∏—Ü—ã
        analyzed_tables = set()
        for finding in session_memory:
            analyzed_tables.update(finding.get('analyzed_tables', []))

        success_rate = (successful_analyses / max(total_questions, 1)) * 100

        # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
        summary_parts = [
            f"–ü—Ä–æ–≤–µ–¥–µ–Ω –ø–æ–ª–Ω—ã–π DataFrame-–∞–Ω–∞–ª–∏–∑ —Å GPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π.",
            f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {successful_analyses} –∏–∑ {total_questions} –∞–Ω–∞–ª–∏—Ç–∏—á–µ—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ (—É—Å–ø–µ—à–Ω–æ—Å—Ç—å: {success_rate:.1f}%).",
            f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {total_tables} —Ç–∞–±–ª–∏—Ü ({total_memory:.1f} MB) —Å {total_relations} —Å–≤—è–∑—è–º–∏.",
            f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ {len(analyzed_tables)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–∞–±–ª–∏—Ü."
        ]

        return " ".join(summary_parts)

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è executive summary: {e}")
        return f"DataFrame-–∞–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —Å {successful_analyses} —É—Å–ø–µ—à–Ω—ã–º–∏ –∞–Ω–∞–ª–∏–∑–∞–º–∏ –∏–∑ {len(session_memory)} –∑–∞–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö."


def _trim_large_report(report: dict) -> dict:
    """–°–æ–∫—Ä–∞—â–∞–µ—Ç —Ä–∞–∑–º–µ—Ä —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
    try:
        logger.info("–°–æ–∫—Ä–∞—â–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –æ—Ç—á–µ—Ç–∞ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏")

        trimmed_report = report.copy()

        # –°–æ–∫—Ä–∞—â–∞–µ–º detailed_findings
        detailed_findings = trimmed_report.get('detailed_findings', [])
        if len(detailed_findings) > 0:
            for finding in detailed_findings:
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä data_preview
                if isinstance(finding.get('data_preview'), list) and len(finding['data_preview']) > 5:
                    finding['data_preview'] = finding['data_preview'][:5]
                    finding['data_preview'].append({"note": f"... –ø–æ–∫–∞–∑–∞–Ω–æ 5 –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π"})

                # –°–æ–∫—Ä–∞—â–∞–µ–º GPT –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π
                gpt_insights = finding.get('gpt_insights', {})
                if gpt_insights.get('gpt_analysis') and len(gpt_insights['gpt_analysis']) > 1000:
                    gpt_insights['gpt_analysis'] = gpt_insights['gpt_analysis'][:1000] + "... (—Å–æ–∫—Ä–∞—â–µ–Ω–æ)"

        trimmed_report['report_metadata']['trimmed'] = True
        trimmed_report['report_metadata']['trim_reason'] = "–û—Ç—á–µ—Ç —Å–æ–∫—Ä–∞—â–µ–Ω –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏"

        logger.info("–û—Ç—á–µ—Ç —É—Å–ø–µ—à–Ω–æ —Å–æ–∫—Ä–∞—â–µ–Ω")
        return trimmed_report

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –æ—Ç—á–µ—Ç–∞: {e}")
        return report


# =============== –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –ó–ê–î–ê–ß–ò ===============

@celery_app.task(bind=True, time_limit=3600, name='tasks.quick_dataframe_analysis')
def quick_dataframe_analysis(self, connection_id: int, user_id: int, report_id: int):
    """–ë—ã—Å—Ç—Ä—ã–π DataFrame –∞–Ω–∞–ª–∏–∑ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º GPT"""
    logger.info(f"[QUICK DATAFRAME] –ó–∞–ø—É—Å–∫ –±—ã—Å—Ç—Ä–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 8}
    ).get()


@celery_app.task(bind=True, time_limit=9000, name='tasks.comprehensive_dataframe_analysis')
def comprehensive_dataframe_analysis(self, connection_id: int, user_id: int, report_id: int):
    """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π DataFrame –∞–Ω–∞–ª–∏–∑ —Å –ø–æ–ª–Ω—ã–º GPT"""
    logger.info(f"[COMPREHENSIVE DATAFRAME] –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 25}
    ).get()


# =============== LEGACY –ü–û–î–î–ï–†–ñ–ö–ê ===============

@celery_app.task(bind=True, time_limit=3600, name='tasks.generate_advanced_report')
def generate_advanced_report(self, connection_id: int, user_id: int, report_id: int):
    """Legacy —Ñ—É–Ω–∫—Ü–∏—è - –ø–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–∞ DataFrame –∞–Ω–∞–ª–∏–∑"""
    logger.warning(f"[LEGACY] –ü–µ—Ä–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ DataFrame –∞–Ω–∞–ª–∏–∑ –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}")
    return generate_dataframe_report.apply_async(
        args=[connection_id, user_id, report_id],
        kwargs={'max_questions': 12}
    ).get()


logger.info("DataFrame tasks —Å–∏—Å—Ç–µ–º–∞ —Å GPT –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–µ–π –ø–æ–ª–Ω–æ—Å—Ç—å—é –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
